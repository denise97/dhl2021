{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c24247fa39158f46a54dbb99bb8811b81cd84bf3c9aa6e8294d53a41a5837da9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ARIMA Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Read chartevents_subset from parquet file to pandas data frame\n",
    "chartevents_resampled = pd.read_parquet('./data/chartevents_resampled.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETER = 'HR'\n",
    "#CHUNKS = chartevents.CHUNK_ID_FILLED_TH.unique()\n",
    "CHUNKS = ['296490.0_220045.0_2192-09-26 23:51:00']\n",
    "\n",
    "# Sampling rate of 1 data point per hour - Test for different values in the future - e.g. longer training set\n",
    "TRAIN = 12 # 12 * 1 h = 12 hour training period\n",
    "TEST = 1 # 1 * 1 h = 2 hours testing period\n",
    "STEP = 1 # move 1 * 1 h = 1 hour per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data based on PARAMETER & CHUNKS\n",
    "arima_data = chartevents_resampled[\n",
    "    (chartevents_resampled['VITAL_PARAMETER_NAME'] == PARAMETER) & \n",
    "    (chartevents_resampled.CHUNK_ID_FILLED_TH.isin(CHUNKS))\n",
    "    ][['CHUNK_ID_FILLED_TH','CHARTTIME','VITAL_PARAMETER_NAME','VITAL_PARAMTER_VALUE_MEDIAN_RESAMPLING','VITAL_PARAMTER_VALUE_MEAN_RESAMPLING','VITAL_PARAMTER_VALUE_MAX_RESAMPLING','VITAL_PARAMTER_VALUE_MIN_RESAMPLING','THRESHOLD_VALUE_HIGH','THRESHOLD_VALUE_LOW']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for chunks that have sufficient values to be used for training and testing the model\n",
    "all_chunks_value_count = arima_data.CHUNK_ID_FILLED_TH.value_counts()\n",
    "chunkid_filter = all_chunks_value_count[all_chunks_value_count >= (TRAIN + TEST)].index\n",
    "arima_data = arima_data[arima_data.CHUNK_ID_FILLED_TH.isin(chunkid_filter)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new HOURS_SINCE_FIRST_RECORD column containing the time difference that has passed since the first timestamp of the measurement series.\n",
    "import numpy as np\n",
    "# arima_data['MINUTES_SINCE_FIRST_RECORD'] = arima_data.groupby('CHUNK_ID_FILLED_TH')#['CHARTTIME'].transform(lambda x: (x - x.min())/np.timedelta64(1,'m'))\n",
    "# Alternative for hours instead of minutes\n",
    "arima_data['HOURS_SINCE_FIRST_RECORD'] = arima_data.groupby('CHUNK_ID_FILLED_TH')['CHARTTIME'].transform(lambda x: (x - x.min())/np.timedelta64(1,'h'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_data = arima_data[:15]"
   ]
  },
  {
   "source": [
    "### First Adaption\n",
    "Create dict that holds vital parameter series, threshold high and threshold low series for each chunk id (key). The series are all indexed the same way (= dif to first measurement in hours with current sampling rate) so they relate to the same time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with chunk id as key and a dataframe as value.\n",
    "# This dataframe contains of three columns the vital parameter values, the high thresholds and the low thresholds.\n",
    "# As the index of these three list is the same and can be referenced back to the \"HOURS_SINCE_FIRST_RECORD\", we keep the time related information.\n",
    "# Example:\n",
    "# dict_of_chunk_series = {\n",
    "#     \"<chunkid_A>\" : { | vital_parameter_series | threshold_high_series | threshold_low_series\n",
    "#                     0 |                   95.0 |                   120 |                   60\n",
    "#                     1 |                   90.5 |                   120 |                   60\n",
    "#                     2 |                   91.0 |                   120 |                   60\n",
    "#                    },\n",
    "#     \"<chunkid_B>\" : { | vital_parameter_series | threshold_high_series | threshold_low_series\n",
    "#                     0 |                   88.0 |                   110 |                   50\n",
    "#                     1 |                   78.5 |                   110 |                   50\n",
    "#                     2 |                   73.0 |                   120 |                   60\n",
    "#                    }\n",
    "#  }\n",
    "#\n",
    "# Example with additional vital parameter series which can be used as an exogenous variable for ARIMAX\n",
    "# dict_of_chunk_series_with_test_and_train = {\n",
    "#     \"<chunkid_A>\" : | vital_parameter_series_median | vital_parameter_series_mean | vital_parameter_series_min |threshold_high_series | threshold_low_series (+max)\n",
    "#                   0 |                          95.0 |                        98.0 |                          80|                  120 |                   60\n",
    "#                   1 |                          90.5 |                        96.0 |                          79|                  120 |                   60\n",
    "#                   2 |                          91.0 |                        94.0 |                          83|                  120 |                   60\n",
    "#  }\n",
    "\n",
    "dict_of_chunk_series = {}\n",
    "\n",
    "for chunkid in chunkid_filter:\n",
    "    \n",
    "    chunk_data = arima_data[arima_data.CHUNK_ID_FILLED_TH == chunkid].copy()\n",
    "\n",
    "    # vital parameter series - median resampling\n",
    "    chunk_value_series_median = pd.Series(chunk_data['VITAL_PARAMTER_VALUE_MEDIAN_RESAMPLING'],name=\"vital_parameter_series_median\")\n",
    "    chunk_value_series_median = chunk_value_series_median.reset_index(drop=True)\n",
    "    chunk_value_series_median.index = list(chunk_value_series_median.index)\n",
    "\n",
    "    # vital parameter series - mean resampling\n",
    "    chunk_value_series_mean = pd.Series(chunk_data['VITAL_PARAMTER_VALUE_MEAN_RESAMPLING'],name=\"vital_parameter_series_mean\")\n",
    "    chunk_value_series_mean = chunk_value_series_mean.reset_index(drop=True)\n",
    "    chunk_value_series_mean.index = list(chunk_value_series_mean.index)\n",
    "\n",
    "    # vital parameter series - min resampling\n",
    "    chunk_value_series_min = pd.Series(chunk_data['VITAL_PARAMTER_VALUE_MIN_RESAMPLING'],name=\"vital_parameter_series_min\")\n",
    "    chunk_value_series_min = chunk_value_series_min.reset_index(drop=True)\n",
    "    chunk_value_series_min.index = list(chunk_value_series_min.index)\n",
    "\n",
    "    # vital parameter series - max resampling\n",
    "    chunk_value_series_max = pd.Series(chunk_data['VITAL_PARAMTER_VALUE_MAX_RESAMPLING'],name=\"vital_parameter_series_max\")\n",
    "    chunk_value_series_max = chunk_value_series_max.reset_index(drop=True)\n",
    "    chunk_value_series_max.index = list(chunk_value_series_max.index)  \n",
    "    \n",
    "\n",
    "    # threshold series high\n",
    "    chunk_threshold_high_series = pd.Series(chunk_data['THRESHOLD_VALUE_HIGH'],name=\"threshold_high_series\")\n",
    "    chunk_threshold_high_series = chunk_threshold_high_series.reset_index(drop=True)\n",
    "    chunk_threshold_high_series.index = list(chunk_threshold_high_series.index)\n",
    "\n",
    "    # threshold series low\n",
    "    chunk_threshold_low_series = pd.Series(chunk_data['THRESHOLD_VALUE_LOW'],name=\"threshold_low_series\")\n",
    "    chunk_threshold_low_series = chunk_threshold_low_series.reset_index(drop=True)\n",
    "    chunk_threshold_low_series.index = list(chunk_threshold_low_series.index)\n",
    "\n",
    "    # Append series with key (CHUNK_ID) into dictionary\n",
    "    vital_parameter_and_thresholds_for_chunkid = pd.concat([chunk_value_series_median,chunk_value_series_mean,chunk_value_series_min,chunk_value_series_max,chunk_threshold_high_series,chunk_threshold_low_series],axis=1)\n",
    "    dict_of_chunk_series[chunkid] = vital_parameter_and_thresholds_for_chunkid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Rausschreiben\n",
    "output_file = open('dict_of_chunk_series.pickle', 'wb')\n",
    "pickle.dump(dict_of_chunk_series, output_file)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'296490.0_220045.0_2192-09-26 23:51:00':     vital_parameter_series_median  vital_parameter_series_mean  \\\n",
       " 0                            95.0                    95.000000   \n",
       " 1                            90.5                    90.766667   \n",
       " 2                            91.0                    90.083333   \n",
       " 3                            91.0                    90.333333   \n",
       " 4                            85.0                    86.016667   \n",
       " 5                            89.0                    87.883333   \n",
       " 6                            82.0                    81.766667   \n",
       " 7                            80.0                    80.916667   \n",
       " 8                            77.0                    77.233333   \n",
       " 9                            78.5                    79.033333   \n",
       " 10                           78.0                    78.250000   \n",
       " 11                           84.0                    84.766667   \n",
       " 12                           75.0                    76.400000   \n",
       " 13                           75.5                    76.600000   \n",
       " 14                           74.0                    76.216667   \n",
       " \n",
       "     vital_parameter_series_min  vital_parameter_series_max  \\\n",
       " 0                         92.0                        98.0   \n",
       " 1                         87.0                        96.0   \n",
       " 2                         85.0                        94.0   \n",
       " 3                         83.0                        92.0   \n",
       " 4                         80.0                        94.0   \n",
       " 5                         80.0                        91.0   \n",
       " 6                         77.0                        89.0   \n",
       " 7                         74.0                        91.0   \n",
       " 8                         73.0                        82.0   \n",
       " 9                         74.0                        87.0   \n",
       " 10                        74.0                        85.0   \n",
       " 11                        77.0                        92.0   \n",
       " 12                        71.0                        88.0   \n",
       " 13                        71.0                        86.0   \n",
       " 14                        69.0                        90.0   \n",
       " \n",
       "     threshold_high_series  threshold_low_series  \n",
       " 0                     NaN                   NaN  \n",
       " 1                   120.0                  60.0  \n",
       " 2                   120.0                  60.0  \n",
       " 3                   120.0                  60.0  \n",
       " 4                   120.0                  60.0  \n",
       " 5                   120.0                  60.0  \n",
       " 6                   120.0                  60.0  \n",
       " 7                   120.0                  60.0  \n",
       " 8                   120.0                  60.0  \n",
       " 9                   120.0                  60.0  \n",
       " 10                  120.0                  60.0  \n",
       " 11                  120.0                  60.0  \n",
       " 12                  120.0                  60.0  \n",
       " 13                  120.0                  60.0  \n",
       " 14                  120.0                  60.0  }"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Einlesen\n",
    "input_file = open('dict_of_chunk_series.pickle', 'rb')\n",
    "dict_of_chunk_series = pickle.load(input_file)\n",
    "input_file.close()\n",
    "dict_of_chunk_series"
   ]
  },
  {
   "source": [
    "### Second Adaption\n",
    "Specific TRAIn and TEST Size"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple test & train sets for each chunk to iteratively predict the next x measurements\n",
    "# Create nested dictionary that holds the CHUNK_ID as first key.\n",
    "# This key holds one dictionary for each iteration over this chunk. This depends on the TEST, TRAIN, and STEP.\n",
    "# For each iteration we create another dictionary, whereby the last index of the train list acts as key.\n",
    "# This key holds again one dictionary for the train list and one for the test list.\n",
    "# Example:\n",
    "# dict_of_chunk_series_with_test_and_train = {\n",
    "#     \"<chunkid_A>\" : {\n",
    "#         \"<last_index_of_training_list_of_first_chunkid_A_iteration>\" : {\n",
    "#             \"TRAIN_LIST_MEDIAN\" : train_list_median,\n",
    "#             \"TEST_LIST_MEDIAN\" : test_list_median,\n",
    "#             \"TRAIN_LIST_MEAN\" : train_list_mean,\n",
    "#             \"TEST_LIST_MEAN\" : test_list_mean,\n",
    "#             \"TRAIN_LIST_MIN\" : train_list_min,\n",
    "#             \"TEST_LIST_MIN\" : test_list_min,\n",
    "#             \"TRAIN_LIST_MAX\" : train_list_max,\n",
    "#             \"TEST_LIST_MAX\" : test_list_max,\n",
    "#             \"THRESHOLD_HIGH_FOR_TRAIN_LIST\" : threshold_high_for_train_list ,\n",
    "#             \"THRESHOLD_LOW_FOR_TEST_LIST\" : threshold_low_for_test_list\n",
    "#         },\n",
    "#         \"<last_index_of_training_list_of_second_chunkid_A_iteration>\" : {\n",
    "#             \"TRAIN_LIST_MEDIAN\" : train_list_median,\n",
    "#             \"TEST_LIST_MEDIAN\" : test_list_median,\n",
    "#             \"TRAIN_LIST_MEAN\" : train_list_mean,\n",
    "#             \"TEST_LIST_MEAN\" : test_list_mean,\n",
    "#             \"TRAIN_LIST_MIN\" : train_list_min,\n",
    "#             \"TEST_LIST_MIN\" : test_list_min,\n",
    "#             \"TRAIN_LIST_MAX\" : train_list_max,\n",
    "#             \"TEST_LIST_MAX\" : test_list_max,\n",
    "#             \"THRESHOLD_HIGH_FOR_TRAIN_LIST\" : threshold_high_for_train_list ,\n",
    "#             \"THRESHOLD_LOW_FOR_TEST_LIST\" : threshold_low_for_test_list\n",
    "#         },\n",
    "#     }\n",
    "# }\n",
    "\n",
    "dict_of_chunk_series_with_test_and_train = {}\n",
    "\n",
    "for i, chunk in enumerate(dict_of_chunk_series):\n",
    "    # acces dataframe of current chunk\n",
    "    chunk_series_for_chunk = dict_of_chunk_series[chunk]\n",
    "\n",
    "    # access vital_parameter_series_median of current chunk\n",
    "    chunk_value_series_for_chunk_median = chunk_series_for_chunk[\"vital_parameter_series_median\"]\n",
    "    # access vital_parameter_series_mean of current chunk\n",
    "    chunk_value_series_for_chunk_mean = chunk_series_for_chunk[\"vital_parameter_series_mean\"]\n",
    "    # access vital_parameter_series_min of current chunk\n",
    "    chunk_value_series_for_chunk_min = chunk_series_for_chunk[\"vital_parameter_series_min\"]\n",
    "    # access vital_parameter_series_max of current chunk\n",
    "    chunk_value_series_for_chunk_max = chunk_series_for_chunk[\"vital_parameter_series_max\"]\n",
    "        \n",
    "    # access threshold_high_series of current chunk\n",
    "    chunk_threshold_high_series_for_chunk = chunk_series_for_chunk[\"threshold_high_series\"]\n",
    "    # access threshold_low_series of current chunk\n",
    "    chunk_threshold_low_series_for_chunk = chunk_series_for_chunk[\"threshold_low_series\"]\n",
    "\n",
    "    # create an empty dictionary for the key of the current chunk\n",
    "    dict_of_chunk_series_with_test_and_train[chunk] = {}\n",
    "\n",
    "    # create multiple test and train lists for that chunk\n",
    "    for start in range(0, len(chunk_value_series_for_chunk_median) - (TRAIN + TEST)+1, STEP):\n",
    "        \n",
    "        # vital_parameter_series_median\n",
    "        train_list_median = pd.Series(chunk_value_series_for_chunk_median[start : start+TRAIN], name=\"train_list_median\")\n",
    "        test_list_median = pd.Series(chunk_value_series_for_chunk_median[start+TRAIN : start+TRAIN+TEST], name=\"test_list_median\")\n",
    "        # vital_parameter_series_mean\n",
    "        train_list_mean = pd.Series(chunk_value_series_for_chunk_mean[start : start+TRAIN], name=\"train_list_mean\")\n",
    "        test_list_mean = pd.Series(chunk_value_series_for_chunk_mean[start+TRAIN : start+TRAIN+TEST], name=\"test_list_mean\")\n",
    "        # vital_parameter_series_min\n",
    "        train_list_min = pd.Series(chunk_value_series_for_chunk_min[start : start+TRAIN], name=\"train_list_min\")\n",
    "        test_list_min = pd.Series(chunk_value_series_for_chunk_min[start+TRAIN : start+TRAIN+TEST], name=\"test_list_min\")\n",
    "        # vital_parameter_series_max\n",
    "        train_list_max = pd.Series(chunk_value_series_for_chunk_max[start : start+TRAIN], name=\"train_list_max\")\n",
    "        test_list_max = pd.Series(chunk_value_series_for_chunk_max[start+TRAIN : start+TRAIN+TEST], name=\"test_list_max\")\n",
    "        \n",
    "        #threshold series high & low\n",
    "        threshold_high_for_test_list = pd.Series(chunk_threshold_high_series_for_chunk[start+TRAIN : start+TRAIN+TEST],name=\"threshold_high_for_test_list\")\n",
    "        threshold_low_for_test_list = pd.Series(chunk_threshold_low_series_for_chunk[start+TRAIN : start+TRAIN+TEST],name=\"threshold_low_for_test_list\")\n",
    "        \n",
    "        # For each iteration over the current chunk, we will create a dictionary that holds again the test and train list as dictionary\n",
    "        # We use the last index of the current train list (which currently refers to the difference to first measurement) as second key\n",
    "        second_key = train_list_median.index.max()\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key] = {}\n",
    "        # Assign the train and test list to the current chunk iteration      \n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"TRAIN_LIST_MEDIAN\"] = train_list_median\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"TRAIN_LIST_MEAN\"] = train_list_mean\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"TRAIN_LIST_MIN\"] = train_list_min\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"TRAIN_LIST_MAX\"] = train_list_max\n",
    "\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"TEST_LIST_MEDIAN\"] = test_list_median\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"TEST_LIST_MEAN\"] = test_list_median\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"TEST_LIST_MIN\"] = test_list_min\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"TEST_LIST_MAX\"] = test_list_max\n",
    "\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"THRESHOLD_HIGH_FOR_TEST_LIST\"] = threshold_high_for_test_list\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"THRESHOLD_LOW_FOR_TEST_LIST\"] = threshold_low_for_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Rausschreiben\n",
    "second_output_file = open('dict_of_chunk_series_with_test_and_train.pickle', 'wb')\n",
    "pickle.dump(dict_of_chunk_series_with_test_and_train, second_output_file)\n",
    "second_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'296490.0_220045.0_2192-09-26 23:51:00': {11: {'TRAIN_LIST_MEDIAN': 0     95.0\n",
       "   1     90.5\n",
       "   2     91.0\n",
       "   3     91.0\n",
       "   4     85.0\n",
       "   5     89.0\n",
       "   6     82.0\n",
       "   7     80.0\n",
       "   8     77.0\n",
       "   9     78.5\n",
       "   10    78.0\n",
       "   11    84.0\n",
       "   Name: train_list_median, dtype: float64, 'TRAIN_LIST_MEAN': 0     95.000000\n",
       "   1     90.766667\n",
       "   2     90.083333\n",
       "   3     90.333333\n",
       "   4     86.016667\n",
       "   5     87.883333\n",
       "   6     81.766667\n",
       "   7     80.916667\n",
       "   8     77.233333\n",
       "   9     79.033333\n",
       "   10    78.250000\n",
       "   11    84.766667\n",
       "   Name: train_list_mean, dtype: float64, 'TRAIN_LIST_MIN': 0     92.0\n",
       "   1     87.0\n",
       "   2     85.0\n",
       "   3     83.0\n",
       "   4     80.0\n",
       "   5     80.0\n",
       "   6     77.0\n",
       "   7     74.0\n",
       "   8     73.0\n",
       "   9     74.0\n",
       "   10    74.0\n",
       "   11    77.0\n",
       "   Name: train_list_min, dtype: float64, 'TRAIN_LIST_MAX': 0     98.0\n",
       "   1     96.0\n",
       "   2     94.0\n",
       "   3     92.0\n",
       "   4     94.0\n",
       "   5     91.0\n",
       "   6     89.0\n",
       "   7     91.0\n",
       "   8     82.0\n",
       "   9     87.0\n",
       "   10    85.0\n",
       "   11    92.0\n",
       "   Name: train_list_max, dtype: float64, 'TEST_LIST_MEDIAN': 12    75.0\n",
       "   Name: test_list_median, dtype: float64, 'TEST_LIST_MEAN': 12    75.0\n",
       "   Name: test_list_median, dtype: float64, 'TEST_LIST_MIN': 12    71.0\n",
       "   Name: test_list_min, dtype: float64, 'TEST_LIST_MAX': 12    88.0\n",
       "   Name: test_list_max, dtype: float64, 'THRESHOLD_HIGH_FOR_TEST_LIST': 12    120.0\n",
       "   Name: threshold_high_for_test_list, dtype: float64, 'THRESHOLD_LOW_FOR_TEST_LIST': 12    60.0\n",
       "   Name: threshold_low_for_test_list, dtype: float64},\n",
       "  12: {'TRAIN_LIST_MEDIAN': 1     90.5\n",
       "   2     91.0\n",
       "   3     91.0\n",
       "   4     85.0\n",
       "   5     89.0\n",
       "   6     82.0\n",
       "   7     80.0\n",
       "   8     77.0\n",
       "   9     78.5\n",
       "   10    78.0\n",
       "   11    84.0\n",
       "   12    75.0\n",
       "   Name: train_list_median, dtype: float64, 'TRAIN_LIST_MEAN': 1     90.766667\n",
       "   2     90.083333\n",
       "   3     90.333333\n",
       "   4     86.016667\n",
       "   5     87.883333\n",
       "   6     81.766667\n",
       "   7     80.916667\n",
       "   8     77.233333\n",
       "   9     79.033333\n",
       "   10    78.250000\n",
       "   11    84.766667\n",
       "   12    76.400000\n",
       "   Name: train_list_mean, dtype: float64, 'TRAIN_LIST_MIN': 1     87.0\n",
       "   2     85.0\n",
       "   3     83.0\n",
       "   4     80.0\n",
       "   5     80.0\n",
       "   6     77.0\n",
       "   7     74.0\n",
       "   8     73.0\n",
       "   9     74.0\n",
       "   10    74.0\n",
       "   11    77.0\n",
       "   12    71.0\n",
       "   Name: train_list_min, dtype: float64, 'TRAIN_LIST_MAX': 1     96.0\n",
       "   2     94.0\n",
       "   3     92.0\n",
       "   4     94.0\n",
       "   5     91.0\n",
       "   6     89.0\n",
       "   7     91.0\n",
       "   8     82.0\n",
       "   9     87.0\n",
       "   10    85.0\n",
       "   11    92.0\n",
       "   12    88.0\n",
       "   Name: train_list_max, dtype: float64, 'TEST_LIST_MEDIAN': 13    75.5\n",
       "   Name: test_list_median, dtype: float64, 'TEST_LIST_MEAN': 13    75.5\n",
       "   Name: test_list_median, dtype: float64, 'TEST_LIST_MIN': 13    71.0\n",
       "   Name: test_list_min, dtype: float64, 'TEST_LIST_MAX': 13    86.0\n",
       "   Name: test_list_max, dtype: float64, 'THRESHOLD_HIGH_FOR_TEST_LIST': 13    120.0\n",
       "   Name: threshold_high_for_test_list, dtype: float64, 'THRESHOLD_LOW_FOR_TEST_LIST': 13    60.0\n",
       "   Name: threshold_low_for_test_list, dtype: float64},\n",
       "  13: {'TRAIN_LIST_MEDIAN': 2     91.0\n",
       "   3     91.0\n",
       "   4     85.0\n",
       "   5     89.0\n",
       "   6     82.0\n",
       "   7     80.0\n",
       "   8     77.0\n",
       "   9     78.5\n",
       "   10    78.0\n",
       "   11    84.0\n",
       "   12    75.0\n",
       "   13    75.5\n",
       "   Name: train_list_median, dtype: float64, 'TRAIN_LIST_MEAN': 2     90.083333\n",
       "   3     90.333333\n",
       "   4     86.016667\n",
       "   5     87.883333\n",
       "   6     81.766667\n",
       "   7     80.916667\n",
       "   8     77.233333\n",
       "   9     79.033333\n",
       "   10    78.250000\n",
       "   11    84.766667\n",
       "   12    76.400000\n",
       "   13    76.600000\n",
       "   Name: train_list_mean, dtype: float64, 'TRAIN_LIST_MIN': 2     85.0\n",
       "   3     83.0\n",
       "   4     80.0\n",
       "   5     80.0\n",
       "   6     77.0\n",
       "   7     74.0\n",
       "   8     73.0\n",
       "   9     74.0\n",
       "   10    74.0\n",
       "   11    77.0\n",
       "   12    71.0\n",
       "   13    71.0\n",
       "   Name: train_list_min, dtype: float64, 'TRAIN_LIST_MAX': 2     94.0\n",
       "   3     92.0\n",
       "   4     94.0\n",
       "   5     91.0\n",
       "   6     89.0\n",
       "   7     91.0\n",
       "   8     82.0\n",
       "   9     87.0\n",
       "   10    85.0\n",
       "   11    92.0\n",
       "   12    88.0\n",
       "   13    86.0\n",
       "   Name: train_list_max, dtype: float64, 'TEST_LIST_MEDIAN': 14    74.0\n",
       "   Name: test_list_median, dtype: float64, 'TEST_LIST_MEAN': 14    74.0\n",
       "   Name: test_list_median, dtype: float64, 'TEST_LIST_MIN': 14    69.0\n",
       "   Name: test_list_min, dtype: float64, 'TEST_LIST_MAX': 14    90.0\n",
       "   Name: test_list_max, dtype: float64, 'THRESHOLD_HIGH_FOR_TEST_LIST': 14    120.0\n",
       "   Name: threshold_high_for_test_list, dtype: float64, 'THRESHOLD_LOW_FOR_TEST_LIST': 14    60.0\n",
       "   Name: threshold_low_for_test_list, dtype: float64}}}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Einlesen\n",
    "second_input_file = open('dict_of_chunk_series_with_test_and_train.pickle', 'rb')\n",
    "dict_of_chunk_series_with_test_and_train = pickle.load(second_input_file)\n",
    "second_input_file.close()\n",
    "dict_of_chunk_series_with_test_and_train"
   ]
  },
  {
   "source": [
    "### Second Adaption\n",
    "Expanding TRAIN  - > Train Set always includes all available past values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_chunk_series_with_expanding_test_and_steady_train = {}\n",
    "\n",
    "for i, chunk in enumerate(dict_of_chunk_series):\n",
    "    # acces dataframe of current chunk\n",
    "    chunk_series_for_chunk = dict_of_chunk_series[chunk]\n",
    "\n",
    "    # access vital_parameter_series_median of current chunk\n",
    "    chunk_value_series_for_chunk_median = chunk_series_for_chunk[\"vital_parameter_series_median\"]\n",
    "    # access vital_parameter_series_mean of current chunk\n",
    "    chunk_value_series_for_chunk_mean = chunk_series_for_chunk[\"vital_parameter_series_mean\"]\n",
    "    # access vital_parameter_series_min of current chunk\n",
    "    chunk_value_series_for_chunk_min = chunk_series_for_chunk[\"vital_parameter_series_min\"]\n",
    "    # access vital_parameter_series_max of current chunk\n",
    "    chunk_value_series_for_chunk_max = chunk_series_for_chunk[\"vital_parameter_series_max\"]\n",
    "        \n",
    "    # access threshold_high_series of current chunk\n",
    "    chunk_threshold_high_series_for_chunk = chunk_series_for_chunk[\"threshold_high_series\"]\n",
    "    # access threshold_low_series of current chunk\n",
    "    chunk_threshold_low_series_for_chunk = chunk_series_for_chunk[\"threshold_low_series\"]\n",
    "\n",
    "    # create an empty dictionary for the key of the current chunk\n",
    "    dict_of_chunk_series_with_expanding_test_and_steady_train[chunk] = {}\n",
    "\n",
    "    # create multiple test and train lists for that chunk\n",
    "    for start in range(0, len(chunk_value_series_for_chunk_median) - (TRAIN + TEST)+1, STEP):\n",
    "        \n",
    "        # vital_parameter_series_median\n",
    "        train_list_median = pd.Series(chunk_value_series_for_chunk_median[0: start+TRAIN], name=\"train_list_median\")\n",
    "        test_list_median = pd.Series(chunk_value_series_for_chunk_median[start+TRAIN : start+TRAIN+TEST], name=\"test_list_median\")\n",
    "        # vital_parameter_series_mean\n",
    "        train_list_mean = pd.Series(chunk_value_series_for_chunk_mean[0: start+TRAIN], name=\"train_list_mean\")\n",
    "        test_list_mean = pd.Series(chunk_value_series_for_chunk_mean[start+TRAIN : start+TRAIN+TEST], name=\"test_list_mean\")\n",
    "        # vital_parameter_series_min\n",
    "        train_list_min = pd.Series(chunk_value_series_for_chunk_min[0: start+TRAIN], name=\"train_list_min\")\n",
    "        test_list_min = pd.Series(chunk_value_series_for_chunk_min[start+TRAIN : start+TRAIN+TEST], name=\"test_list_min\")\n",
    "        # vital_parameter_series_max\n",
    "        train_list_max = pd.Series(chunk_value_series_for_chunk_max[0: start+TRAIN], name=\"train_list_max\")\n",
    "        test_list_max = pd.Series(chunk_value_series_for_chunk_max[start+TRAIN : start+TRAIN+TEST], name=\"test_list_max\")\n",
    "        \n",
    "        #threshold series high & low\n",
    "        threshold_high_for_test_list = pd.Series(chunk_threshold_high_series_for_chunk[start+TRAIN : start+TRAIN+TEST],name=\"threshold_high_for_test_list\")\n",
    "        threshold_low_for_test_list = pd.Series(chunk_threshold_low_series_for_chunk[start+TRAIN : start+TRAIN+TEST],name=\"threshold_low_for_test_list\")\n",
    "        \n",
    "        # For each iteration over the current chunk, we will create a dictionary that holds again the test and train list as dictionary\n",
    "        # We use the last index of the current train list (which currently refers to the difference to first measurement) as second key\n",
    "        second_key = train_list_median.index.max()\n",
    "        dict_of_chunk_series_with_expanding_test_and_steady_train[chunk][second_key] = {}\n",
    "        # Assign the train and test list to the current chunk iteration      \n",
    "        dict_of_chunk_series_with_expanding_test_and_steady_train[chunk][second_key][\"TRAIN_LIST_MEDIAN\"] = train_list_median\n",
    "        dict_of_chunk_series_with_expanding_test_and_steady_train[chunk][second_key][\"TRAIN_LIST_MEAN\"] = train_list_mean\n",
    "        dict_of_chunk_series_with_expanding_test_and_steady_train[chunk][second_key][\"TRAIN_LIST_MIN\"] = train_list_min\n",
    "        dict_of_chunk_series_with_expanding_test_and_steady_train[chunk][second_key][\"TRAIN_LIST_MAX\"] = train_list_max\n",
    "\n",
    "        dict_of_chunk_series_with_expanding_test_and_steady_train[chunk][second_key][\"TEST_LIST_MEDIAN\"] = test_list_median\n",
    "        dict_of_chunk_series_with_expanding_test_and_steady_train[chunk][second_key][\"TEST_LIST_MEAN\"] = test_list_median\n",
    "        dict_of_chunk_series_with_expanding_test_and_steady_train[chunk][second_key][\"TEST_LIST_MIN\"] = test_list_min\n",
    "        dict_of_chunk_series_with_expanding_test_and_steady_train[chunk][second_key][\"TEST_LIST_MAX\"] = test_list_max\n",
    "\n",
    "        dict_of_chunk_series_with_expanding_test_and_steady_train[chunk][second_key][\"THRESHOLD_HIGH_FOR_TEST_LIST\"] = threshold_high_for_test_list\n",
    "        dict_of_chunk_series_with_expanding_test_and_steady_train[chunk][second_key][\"THRESHOLD_LOW_FOR_TEST_LIST\"] = threshold_low_for_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Rausschreiben\n",
    "third_output_file = open('dict_of_chunk_series_with_expanding_test_and_steady_train.pickle', 'wb')\n",
    "pickle.dump(dict_of_chunk_series_with_expanding_test_and_steady_train, third_output_file)\n",
    "third_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{11: {'TRAIN_LIST_MEDIAN': 0     95.0\n",
       "  1     90.5\n",
       "  2     91.0\n",
       "  3     91.0\n",
       "  4     85.0\n",
       "  5     89.0\n",
       "  6     82.0\n",
       "  7     80.0\n",
       "  8     77.0\n",
       "  9     78.5\n",
       "  10    78.0\n",
       "  11    84.0\n",
       "  Name: train_list_median, dtype: float64, 'TRAIN_LIST_MEAN': 0     95.000000\n",
       "  1     90.766667\n",
       "  2     90.083333\n",
       "  3     90.333333\n",
       "  4     86.016667\n",
       "  5     87.883333\n",
       "  6     81.766667\n",
       "  7     80.916667\n",
       "  8     77.233333\n",
       "  9     79.033333\n",
       "  10    78.250000\n",
       "  11    84.766667\n",
       "  Name: train_list_mean, dtype: float64, 'TRAIN_LIST_MIN': 0     92.0\n",
       "  1     87.0\n",
       "  2     85.0\n",
       "  3     83.0\n",
       "  4     80.0\n",
       "  5     80.0\n",
       "  6     77.0\n",
       "  7     74.0\n",
       "  8     73.0\n",
       "  9     74.0\n",
       "  10    74.0\n",
       "  11    77.0\n",
       "  Name: train_list_min, dtype: float64, 'TRAIN_LIST_MAX': 0     98.0\n",
       "  1     96.0\n",
       "  2     94.0\n",
       "  3     92.0\n",
       "  4     94.0\n",
       "  5     91.0\n",
       "  6     89.0\n",
       "  7     91.0\n",
       "  8     82.0\n",
       "  9     87.0\n",
       "  10    85.0\n",
       "  11    92.0\n",
       "  Name: train_list_max, dtype: float64, 'TEST_LIST_MEDIAN': 12    75.0\n",
       "  Name: test_list_median, dtype: float64, 'TEST_LIST_MEAN': 12    75.0\n",
       "  Name: test_list_median, dtype: float64, 'TEST_LIST_MIN': 12    71.0\n",
       "  Name: test_list_min, dtype: float64, 'TEST_LIST_MAX': 12    88.0\n",
       "  Name: test_list_max, dtype: float64, 'THRESHOLD_HIGH_FOR_TEST_LIST': 12    120.0\n",
       "  Name: threshold_high_for_test_list, dtype: float64, 'THRESHOLD_LOW_FOR_TEST_LIST': 12    60.0\n",
       "  Name: threshold_low_for_test_list, dtype: float64},\n",
       " 12: {'TRAIN_LIST_MEDIAN': 0     95.0\n",
       "  1     90.5\n",
       "  2     91.0\n",
       "  3     91.0\n",
       "  4     85.0\n",
       "  5     89.0\n",
       "  6     82.0\n",
       "  7     80.0\n",
       "  8     77.0\n",
       "  9     78.5\n",
       "  10    78.0\n",
       "  11    84.0\n",
       "  12    75.0\n",
       "  Name: train_list_median, dtype: float64, 'TRAIN_LIST_MEAN': 0     95.000000\n",
       "  1     90.766667\n",
       "  2     90.083333\n",
       "  3     90.333333\n",
       "  4     86.016667\n",
       "  5     87.883333\n",
       "  6     81.766667\n",
       "  7     80.916667\n",
       "  8     77.233333\n",
       "  9     79.033333\n",
       "  10    78.250000\n",
       "  11    84.766667\n",
       "  12    76.400000\n",
       "  Name: train_list_mean, dtype: float64, 'TRAIN_LIST_MIN': 0     92.0\n",
       "  1     87.0\n",
       "  2     85.0\n",
       "  3     83.0\n",
       "  4     80.0\n",
       "  5     80.0\n",
       "  6     77.0\n",
       "  7     74.0\n",
       "  8     73.0\n",
       "  9     74.0\n",
       "  10    74.0\n",
       "  11    77.0\n",
       "  12    71.0\n",
       "  Name: train_list_min, dtype: float64, 'TRAIN_LIST_MAX': 0     98.0\n",
       "  1     96.0\n",
       "  2     94.0\n",
       "  3     92.0\n",
       "  4     94.0\n",
       "  5     91.0\n",
       "  6     89.0\n",
       "  7     91.0\n",
       "  8     82.0\n",
       "  9     87.0\n",
       "  10    85.0\n",
       "  11    92.0\n",
       "  12    88.0\n",
       "  Name: train_list_max, dtype: float64, 'TEST_LIST_MEDIAN': 13    75.5\n",
       "  Name: test_list_median, dtype: float64, 'TEST_LIST_MEAN': 13    75.5\n",
       "  Name: test_list_median, dtype: float64, 'TEST_LIST_MIN': 13    71.0\n",
       "  Name: test_list_min, dtype: float64, 'TEST_LIST_MAX': 13    86.0\n",
       "  Name: test_list_max, dtype: float64, 'THRESHOLD_HIGH_FOR_TEST_LIST': 13    120.0\n",
       "  Name: threshold_high_for_test_list, dtype: float64, 'THRESHOLD_LOW_FOR_TEST_LIST': 13    60.0\n",
       "  Name: threshold_low_for_test_list, dtype: float64},\n",
       " 13: {'TRAIN_LIST_MEDIAN': 0     95.0\n",
       "  1     90.5\n",
       "  2     91.0\n",
       "  3     91.0\n",
       "  4     85.0\n",
       "  5     89.0\n",
       "  6     82.0\n",
       "  7     80.0\n",
       "  8     77.0\n",
       "  9     78.5\n",
       "  10    78.0\n",
       "  11    84.0\n",
       "  12    75.0\n",
       "  13    75.5\n",
       "  Name: train_list_median, dtype: float64, 'TRAIN_LIST_MEAN': 0     95.000000\n",
       "  1     90.766667\n",
       "  2     90.083333\n",
       "  3     90.333333\n",
       "  4     86.016667\n",
       "  5     87.883333\n",
       "  6     81.766667\n",
       "  7     80.916667\n",
       "  8     77.233333\n",
       "  9     79.033333\n",
       "  10    78.250000\n",
       "  11    84.766667\n",
       "  12    76.400000\n",
       "  13    76.600000\n",
       "  Name: train_list_mean, dtype: float64, 'TRAIN_LIST_MIN': 0     92.0\n",
       "  1     87.0\n",
       "  2     85.0\n",
       "  3     83.0\n",
       "  4     80.0\n",
       "  5     80.0\n",
       "  6     77.0\n",
       "  7     74.0\n",
       "  8     73.0\n",
       "  9     74.0\n",
       "  10    74.0\n",
       "  11    77.0\n",
       "  12    71.0\n",
       "  13    71.0\n",
       "  Name: train_list_min, dtype: float64, 'TRAIN_LIST_MAX': 0     98.0\n",
       "  1     96.0\n",
       "  2     94.0\n",
       "  3     92.0\n",
       "  4     94.0\n",
       "  5     91.0\n",
       "  6     89.0\n",
       "  7     91.0\n",
       "  8     82.0\n",
       "  9     87.0\n",
       "  10    85.0\n",
       "  11    92.0\n",
       "  12    88.0\n",
       "  13    86.0\n",
       "  Name: train_list_max, dtype: float64, 'TEST_LIST_MEDIAN': 14    74.0\n",
       "  Name: test_list_median, dtype: float64, 'TEST_LIST_MEAN': 14    74.0\n",
       "  Name: test_list_median, dtype: float64, 'TEST_LIST_MIN': 14    69.0\n",
       "  Name: test_list_min, dtype: float64, 'TEST_LIST_MAX': 14    90.0\n",
       "  Name: test_list_max, dtype: float64, 'THRESHOLD_HIGH_FOR_TEST_LIST': 14    120.0\n",
       "  Name: threshold_high_for_test_list, dtype: float64, 'THRESHOLD_LOW_FOR_TEST_LIST': 14    60.0\n",
       "  Name: threshold_low_for_test_list, dtype: float64}}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "dict_of_chunk_series_with_expanding_test_and_steady_train[chunk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
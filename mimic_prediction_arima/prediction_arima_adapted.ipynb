{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd0c24247fa39158f46a54dbb99bb8811b81cd84bf3c9aa6e8294d53a41a5837da9",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ARIMA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Read chartevents_subset from parquet file to pandas data frame\n",
    "chartevents_subset = pd.read_parquet('./data/chartevents_clean_values_and_thresholds_with_chunkid_65_resampled.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETER = 220045\n",
    "CHUNKS = ['296490.0_220045.0_2192-09-26 23:51:00']\n",
    "\n",
    "# Sampling rate of 1 data point per hour - Test for different values in the future - e.g. longer training set\n",
    "TRAIN = 12 # 12 * 1 h = 12 hour training period\n",
    "TEST = 2 # 2 * 1 h = 2 hours testing period\n",
    "STEP = 1 # move 1 * 1 h = 1 hour per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data based on PARAMETER & CHUNKS\n",
    "arima_data = chartevents_subset[\n",
    "    (chartevents_subset[\"ITEMID\"] == PARAMETER) & \n",
    "    (chartevents_subset.CHUNK_ID_FILLED_TH.isin(CHUNKS))\n",
    "    ][['CHUNK_ID_FILLED_TH','CHARTTIME','ITEMID','VALUENUM_CLEAN']]\n",
    "display(arima_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for chunks that have sufficient values to be used for training and testing the model\n",
    "all_chunks_value_count = arima_data.CHUNK_ID_FILLED_TH.value_counts()\n",
    "chunkid_filter = all_chunks_value_count[all_chunks_value_count >= (TRAIN + TEST)].index\n",
    "arima_data = arima_data[arima_data.CHUNK_ID_FILLED_TH.isin(chunkid_filter)]\n",
    "display(arima_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new MINUTES_SINCE_FIRST_RECORD column containing the time difference that has passed since the first timestamp of the measurement series.\n",
    "import numpy as np\n",
    "#arima_data['MINUTES_SINCE_FIRST_RECORD'] = arima_data.groupby('CHUNK_ID_FILLED_TH')#['CHARTTIME'].transform(lambda x: (x - x.min())/np.timedelta64(1,'m'))\n",
    "# Alternative for hours instead of minutes\n",
    "arima_data['HOURS_SINCE_FIRST_RECORD'] = arima_data.groupby('CHUNK_ID_FILLED_TH')['CHARTTIME'].transform(lambda x: (x - x.min())/np.timedelta64(1,'h'))\n",
    "display(arima_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dataset to small amount in order to first test script\n",
    "# Now we have 15 measurements for that chunk; With a TRAIN of 12, a TEST of 2 and a STEP of 1 we expect to receive two training sets and two test sets - looking at row ids they would look like the following:\n",
    "# first train = 0:11 ; first test= 12:13\n",
    "# second train = 1:12 ; second test= 13:14\n",
    "arima_data = arima_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "** Old Coding** - Change cell type to Code when needed\n",
    "### Change data structure\n",
    "### Create a list containing one element for each chunk, which are of type pandas series.\n",
    "### Each of these series includes the measured values of the chunk with the MINUTES_SINCE_FIRST_RECORD as index.\n",
    "### The data structure is transposed, so to speak, so that the MINUTES_SINCE_FIRST_RECORD that were previously in rows now serve as 'columns' (not literally; they are in the index of the series).\n",
    "\n",
    "### MINUTES_SINCE_FIRST_RECORD  |     0 |    60 |   120 | ...\n",
    "### ----------------------------------------------------- ...\n",
    "### firstChunk                  |  95.0 |  90.5 |  91.0 | ...\n",
    "### secondChunk                 | 110.5 | 108.0 | 110.0 | ...\n",
    "### ...\n",
    "\n",
    "### Set up list that will contain the chunk value series transformed as described above.\n",
    "list_of_chunk_value_series = []\n",
    "\n",
    "for chunkid in chunkid_filter:\n",
    "\n",
    "    chunk_value_series = arima_data[arima_data.CHUNK_ID_FILLED_TH == chunkid].copy()\n",
    "    chunk_value_series.set_index('MINUTES_SINCE_FIRST_RECORD', inplace=True)\n",
    "    chunk_value_series.sort_index(inplace=True)    \n",
    "    list_of_chunk_value_series.append(chunk_value_series['VALUENUM_CLEAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Adaption for following cell:\n",
    "# Change list_of_chunk_value_series from List to Dictionary\n",
    "# The CHUNK_ID is used as key and in this step one key holds three series: the vital parameter series, the low threshold series and the high threshold series. They need the same \"sampling rate\" - so that the high threshold with index 0 is the high threshold that applies at the time of the vital parameter with index 0 \n",
    "\n",
    "# Vital parameter Series:\n",
    "# index                       |     0 |    1  |   2   | ...\n",
    "# ----------------------------------------------------- ...\n",
    "# firstChunk - Vital Parameter|  95.0 |  90.5 |  91.0 | ...\n",
    "\n",
    "# Threshold High Series:\n",
    "# index                       |     0 |    1  |   2   | ...\n",
    "# ----------------------------------------------------- ...\n",
    "# firstChunk - Th. High       |  120.0 |  120.0 |  110.0 | ...\n",
    "\n",
    "# Threshold Low Series:\n",
    "# index                       |     0 |    1  |   2   | ...\n",
    "# ----------------------------------------------------- ...\n",
    "# firstChunk - Th. High       |  70.0 |  70.0 |  60.0 | ..."
   ]
  },
  {
   "source": [
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Adaption for following cell:\n",
    "# Change chunk_value_series_with_test_and_train from List to Dictionary\n",
    "# The CHUNK_ID is used as key and in this step one key holds two series: The train_list and the test_list. As long as the index in train and test list is kept, we can still refer to the difference to the first measurement (as long as sampling rate is one hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple test & train sets for each chunk to iteratively predict the next x measurements\n",
    "chunk_value_series_with_test_and_train = pd.DataFrame(columns=[\"SUB_CHUNK_ID\", \"TRAIN_LIST\",\"TEST_LIST\"])\n",
    "for i, chunk_value_series in enumerate(list_of_chunk_value_series):\n",
    "    for start in range(0, len(chunk_value_series) - (TRAIN + TEST)+1, STEP):\n",
    "\n",
    "        sub_chunk_id = str(i)+str(start)\n",
    "        train_list = chunk_value_series[start : start+TRAIN]\n",
    "        test_list = chunk_value_series[start+TRAIN : start+TRAIN+TEST]\n",
    "        a_new_row= {\"SUB_CHUNK_ID\":sub_chunk_id,\"TRAIN_LIST\":train_list,\"TEST_LIST\":test_list}\n",
    "        a_new_row_series = pd.Series(a_new_row, name=sub_chunk_id)\n",
    "        chunk_value_series_with_test_and_train = chunk_value_series_with_test_and_train.append(a_new_row_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Adaption for following cell:\n",
    "# Currently we only have a true values list and a predictions list. But we are not interested in whether the prediction is exactly the true value. We want to see if the prediction value also triggers an alarm if the true value does. Therefore we need the threshold values that apply at the time of the respective prediction/true value.\n",
    "# A Final version should hold the following informations that can be traced back to a specific Chunk ID:\n",
    "# * List of true values (vital parameters in test list)\n",
    "# * List of Threshold High (for the time at which the predictions take place)\n",
    "# * List of Threshold Low (for the time at which the predictions take place)\n",
    "# * Arima Predictions (the predictions for the true values based on the train values)\n",
    "\n",
    "# Our thoughts:\n",
    "# Currently prediction looks as follows (two colums as TRAIN is 2; two rows as two chunk_value_series are created for our chunk (containing 15 values)):\n",
    "\n",
    "#   | 0                                         | 1\n",
    "# 0 | first prediction for chunk_value_series 1 | second prediction for chunk_value_series 1\n",
    "# 1 | first prediction for chunk_value_series 2 | second prediction for chunk_value_series 2\n",
    "\n",
    "# We wanted to add the last index of the train_list and the CHUNK_ID in a nested way to these predictions so that we can trace them back to the thresholds that apply at the time of the prediction\n",
    "\n",
    "#   | CHUNK_ID | Time ref. | 0                                 | 1\n",
    "# 0 |  xxxx    | 11        | 1st pred. for chunk_value_series 1| 2nd pred. for chunk_value_series 1 \n",
    "# 1 |  xxxx    | 12        | 1st pred. for chunk_value_series 2| 2nd pred. for chunk_value_series 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct Arima\n",
    "from progressbar import progressbar\n",
    "import pmdarima as pm\n",
    "\n",
    "true_values = []\n",
    "prediction = []\n",
    "all_sub_chunk_ids = chunk_value_series_with_test_and_train.SUB_CHUNK_ID.value_counts()\n",
    "\n",
    "for i, sub_chunk_id in enumerate(all_sub_chunk_ids):\n",
    "    arima = pm.auto_arima(chunk_value_series_with_test_and_train['TRAIN_LIST'][i])\n",
    "    forecast = arima.predict(TEST)\n",
    "    \n",
    "    test_list = chunk_value_series_with_test_and_train[\"TEST_LIST\"][i]\n",
    "    test_list = test_list.reset_index(drop=True)\n",
    "    test_list = test_list.to_numpy()\n",
    "\n",
    "    true_values.append(test_list)\n",
    "    prediction.append(forecast)"
   ]
  },
  {
   "source": [
    "Arima from previous version:\n",
    "auto_arima_model = auto_arima(data, start_p=1, start_q=1,\n",
    "                            max_p=3, max_q=3, m=1,\n",
    "                            start_P=0, seasonal=False,\n",
    "                            d=1, D=1, trace=True,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
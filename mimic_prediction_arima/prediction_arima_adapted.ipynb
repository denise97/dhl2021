{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd0c24247fa39158f46a54dbb99bb8811b81cd84bf3c9aa6e8294d53a41a5837da9",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ARIMA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Read chartevents_subset from parquet file to pandas data frame\n",
    "chartevents_subset = pd.read_parquet('./data/chartevents_clean_values_and_thresholds_with_chunkid_65_resampled.parquet', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETER = 220045\n",
    "CHUNKS = ['296490.0_220045.0_2192-09-26 23:51:00']\n",
    "\n",
    "# Sampling rate of 1 data point per hour - Test for different values in the future - e.g. longer training set\n",
    "TRAIN = 12 # 12 * 1 h = 12 hour training period\n",
    "TEST = 2 # 2 * 1 h = 2 hours testing period\n",
    "STEP = 1 # move 1 * 1 h = 1 hour per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data based on PARAMETER & CHUNKS\n",
    "arima_data = chartevents_subset[\n",
    "    (chartevents_subset[\"ITEMID\"] == PARAMETER) & \n",
    "    (chartevents_subset.CHUNK_ID_FILLED_TH.isin(CHUNKS))\n",
    "    ][['CHUNK_ID_FILLED_TH','CHARTTIME','ITEMID','VALUENUM_CLEAN']]\n",
    "display(arima_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for chunks that have sufficient values to be used for training and testing the model\n",
    "all_chunks_value_count = arima_data.CHUNK_ID_FILLED_TH.value_counts()\n",
    "chunkid_filter = all_chunks_value_count[all_chunks_value_count >= (TRAIN + TEST)].index\n",
    "arima_data = arima_data[arima_data.CHUNK_ID_FILLED_TH.isin(chunkid_filter)]\n",
    "#display(arima_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new HOURS_SINCE_FIRST_RECORD column containing the time difference that has passed since the first timestamp of the measurement series.\n",
    "import numpy as np\n",
    "#arima_data['MINUTES_SINCE_FIRST_RECORD'] = arima_data.groupby('CHUNK_ID_FILLED_TH')#['CHARTTIME'].transform(lambda x: (x - x.min())/np.timedelta64(1,'m'))\n",
    "# Alternative for hours instead of minutes\n",
    "arima_data['HOURS_SINCE_FIRST_RECORD'] = arima_data.groupby('CHUNK_ID_FILLED_TH')['CHARTTIME'].transform(lambda x: (x - x.min())/np.timedelta64(1,'h'))\n",
    "display(arima_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dataset to small amount in order to first test script\n",
    "# Now we have 15 measurements for that chunk; With a TRAIN of 12, a TEST of 2 and a STEP of 1 we expect to receive two training sets and two test sets - looking at row ids they would look like the following:\n",
    "# first train = 0:11 ; first test= 12:13\n",
    "# second train = 1:12 ; second test= 13:14\n",
    "arima_data = arima_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Adaption for following cell:\n",
    "# Change list_of_chunk_value_series from List to Dictionary\n",
    "# The CHUNK_ID is used as key and in this step one key holds three series: the vital parameter series, the low threshold series and the high threshold series. They need the same \"sampling rate\" - so that the high threshold with index 0 is the high threshold that applies at the time of the vital parameter with index 0 \n",
    "\n",
    "# Vital parameter Series:\n",
    "# index                       |     0 |    1  |   2   | ...\n",
    "# ----------------------------------------------------- ...\n",
    "# firstChunk - Vital Parameter|  95.0 |  90.5 |  91.0 | ...\n",
    "\n",
    "# Threshold High Series:\n",
    "# index                       |     0 |    1  |   2   | ...\n",
    "# ----------------------------------------------------- ...\n",
    "# firstChunk - Th. High       |  120.0 |  120.0 |  110.0 | ...\n",
    "\n",
    "# Threshold Low Series:\n",
    "# index                       |     0 |    1  |   2   | ...\n",
    "# ----------------------------------------------------- ...\n",
    "# firstChunk - Th. Low        |  70.0 |  70.0 |  60.0 | ..."
   ]
  },
  {
   "source": [
    "### First Adaption\n",
    "Create dict that holds vital parameter series, threshold high and threshold low series for each chunk id (key). The series are all indexed the same way (= dif to first measurement in hours with current sampling rate) so they relate to the same time\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test threshold data\n",
    "arima_threshold_high_data = pd.read_csv('./data/threshold_high.csv') \n",
    "arima_threshold_low_data = pd.read_csv('./data/threshold_low.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Adaption\n",
    "# Create dictionary with chunk id as key and a dataframe as value.\n",
    "# This dataframe contains of three columns the vital parameter values, the high thresholds and the low thresholds. As the index of these three list is the same and can be referenced back to the \"HOURS_SINCE_FIRST_RECORD\", we keep the time related information.\n",
    "#Example:\n",
    "# dict_of_chunk_series_with_test_and_train = {\n",
    "#    \"<chunkid_A>\" :      |vital_parameter_series  | threshold_high_series | threshold_low_series\n",
    "#                     0          95.0                     120                      60\n",
    "#                     1          90.5                     120                      60\n",
    "#                     2          91.0                     120                      60\n",
    "# }\n",
    "        \n",
    "\n",
    "dict_of_chunk_series = {}\n",
    "\n",
    "for chunkid in chunkid_filter:\n",
    "\n",
    "    #vital parameter series\n",
    "    chunk_value_data = arima_data[arima_data.CHUNK_ID_FILLED_TH == chunkid].copy()\n",
    "    chunk_value_series = pd.Series(chunk_value_data['VALUENUM_CLEAN'],name=\"vital_parameter_series\")\n",
    "    chunk_value_series = chunk_value_series.reset_index(drop=True)\n",
    "    chunk_value_series.index = list(chunk_value_series.index)\n",
    "    \n",
    "\n",
    "    #threshold series high\n",
    "    chunk_threshold_high_data = arima_threshold_high_data[arima_threshold_high_data.CHUNK_ID_FILLED_TH ==chunkid].copy()\n",
    "    chunk_threshold_high_series = pd.Series(chunk_threshold_high_data['VALUENUM_CLEAN'],name=\"threshold_high_series\")\n",
    "    chunk_threshold_high_series = chunk_threshold_high_series.reset_index(drop=True)\n",
    "    chunk_threshold_high_series.index = list(chunk_threshold_high_series.index)\n",
    "\n",
    "     #threshold series low\n",
    "    chunk_threshold_low_data = arima_threshold_low_data[arima_threshold_low_data.CHUNK_ID_FILLED_TH ==chunkid].copy()\n",
    "    chunk_threshold_low_series = pd.Series(chunk_threshold_low_data['VALUENUM_CLEAN'],name=\"threshold_low_series\")\n",
    "    chunk_threshold_low_series = chunk_threshold_low_series.reset_index(drop=True)\n",
    "    chunk_threshold_low_series.index = list(chunk_threshold_low_series.index)\n",
    "    \n",
    "\n",
    "    # Append series with key(CHUNK_ID) into dictionary\n",
    "    vital_parameter_and_thresholds_for_chunkid = pd.concat([chunk_value_series,chunk_threshold_high_series,chunk_threshold_low_series],axis=1)\n",
    "    dict_of_chunk_series[chunkid] = vital_parameter_and_thresholds_for_chunkid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of accessing dataframe in value of specific chunk\n",
    "dict_of_chunk_series['296490.0_220045.0_2192-09-26 23:51:00']"
   ]
  },
  {
   "source": [
    "### Second Adaption"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple test & train sets for each chunk to iteratively predict the next x measurements\n",
    "# Create nested dictionary that holds the CHUNK_ID as first key. This key holds one dictionary for each iteration over this chunk. This depends on the TEST,TRAIN,and STEP.\n",
    "# For each iteration we create another dictionary, whereby the last index of the train list acts as key. This key holds again one dictionary for the train list and one for the test list.\n",
    "#Example:\n",
    "# dict_of_chunk_series_with_test_and_train = {\n",
    "#    \"<chunkid_A>\" : {\n",
    "#         \"<last_index_of_training_list_of_first_chunkid_A_iteration>\" : {\n",
    "#                 \"TRAIN_LIST\" : train_list,\n",
    "#                 \"TEST_LIST\" : test_list,\n",
    "#                 \"THRESHOLD_HIGH_FOR_TEST_LIST\" : threshold_high_for_test_list ,\n",
    "#                 \"THRESHOLD_LOW_FOR_TEST_LIST\" : threshold_low_for_test_list\n",
    "#          },\n",
    "#         \"<last_index_of_training_list_of_second_chunkid_A_iteration>\" : {\n",
    "#                 \"TRAIN_LIST\" : train_list,\n",
    "#                 \"TEST_LIST\" : test_list,\n",
    "#                 \"THRESHOLD_HIGH_FOR_TEST_LIST\" : threshold_high_for_test_list ,\n",
    "#                 \"THRESHOLD_LOW_FOR_TEST_LIST\" : threshold_low_for_test_list\n",
    "#          },\n",
    "# }}\n",
    "\n",
    "dict_of_chunk_series_with_test_and_train = {}\n",
    "\n",
    "for i, chunk in enumerate(dict_of_chunk_series):\n",
    "    #acces dataframe of current chunk\n",
    "    chunk_series_for_chunk = dict_of_chunk_series[chunk]\n",
    "    # access vital_parameter_series of current chunk\n",
    "    chunk_value_series_for_chunk = chunk_series_for_chunk[\"vital_parameter_series\"]\n",
    "\n",
    "    # access threshold_high_series of current chunk\n",
    "    chunk_threshold_high_series_for_chunk = chunk_series_for_chunk[\"threshold_high_series\"]\n",
    "\n",
    "    # access threshold_low_series of current chunk\n",
    "    chunk_threshold_low_series_for_chunk = chunk_series_for_chunk[\"threshold_low_series\"]\n",
    "\n",
    "    # create an empty dictionary for the key of the current chunk\n",
    "    dict_of_chunk_series_with_test_and_train[chunk] = {}\n",
    "\n",
    "    # create multiple test and train lists for that chunk\n",
    "    for start in range(0, len(chunk_value_series_for_chunk) - (TRAIN + TEST)+1, STEP):\n",
    "        \n",
    "        train_list = pd.Series(chunk_value_series_for_chunk[start : start+TRAIN],name=\"train_list\")\n",
    "        test_list = pd.Series(chunk_value_series_for_chunk[start+TRAIN : start+TRAIN+TEST],name=\"test_list\")\n",
    "        threshold_high_for_test_list = pd.Series(chunk_threshold_high_series_for_chunk[start+TRAIN : start+TRAIN+TEST],name=\"threshold_high_for_test_list\")\n",
    "        threshold_low_for_test_list = pd.Series(chunk_threshold_low_series_for_chunk[start+TRAIN : start+TRAIN+TEST],name=\"threshold_low_for_test_list\")\n",
    "        #For each iteration over the current chunk, we will create a dictionary that holds again the test and train list as dictionary\n",
    "        #We use the last index of the current train list (which currently refers to the difference to first measurement) as second key\n",
    "        second_key = train_list.index.max() \n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key] = {}\n",
    "        #Assign the train and test list to the current chunk iteration      \n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"TRAIN_LIST\"] = train_list\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"TEST_LIST\"] = test_list\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"THRESHOLD_HIGH_FOR_TEST_LIST\"] = threshold_high_for_test_list\n",
    "        dict_of_chunk_series_with_test_and_train[chunk][second_key][\"THRESHOLD_LOW_FOR_TEST_LIST\"] = threshold_low_for_test_list\n",
    "\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of accessing first train list of a specific chunkid\n",
    "dict_of_chunk_series_with_test_and_train[chunkid][TRAIN-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of accessing  lists of a specific chunk iteration\n",
    "dict_of_chunk_series_with_test_and_train[chunkid][TRAIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of accessing second test list of a specific chunkid\n",
    "dict_of_chunk_series_with_test_and_train[chunkid][TRAIN][\"TEST_LIST\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Adaption for following cell:\n",
    "# Currently we only have a true values list and a predictions list. But we are not interested in whether the prediction is exactly the true value. We want to see if the prediction value also triggers an alarm if the true value does. Therefore we need the threshold values that apply at the time of the respective prediction/true value.\n",
    "# A Final version should hold the following informations that can be traced back to a specific Chunk ID:\n",
    "# * List of true values (vital parameters in test list)\n",
    "# * List of Threshold High (for the time at which the predictions take place)\n",
    "# * List of Threshold Low (for the time at which the predictions take place)\n",
    "# * Arima Predictions (the predictions for the true values based on the train values)\n",
    "\n",
    "# Our thoughts:\n",
    "# Currently prediction looks as follows (two colums as TRAIN is 2; two rows as two chunk_value_series are created for our chunk (containing 15 values)):\n",
    "\n",
    "#   | 0                                         | 1\n",
    "# 0 | first prediction for chunk_value_series 1 | second prediction for chunk_value_series 1\n",
    "# 1 | first prediction for chunk_value_series 2 | second prediction for chunk_value_series 2\n",
    "\n",
    "# We wanted to add the last index of the train_list and the CHUNK_ID in a nested way to these predictions so that we can trace them back to the thresholds that apply at the time of the prediction\n",
    "\n",
    "#   | CHUNK_ID | Time ref. | 0                                 | 1\n",
    "# 0 |  xxxx    | 11        | 1st pred. for chunk_value_series 1| 2nd pred. for chunk_value_series 1 \n",
    "# 1 |  xxxx    | 12        | 1st pred. for chunk_value_series 2| 2nd pred. for chunk_value_series 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "\n",
    "# Expand the previously created dictionary (dict_of_chunk_series_with_test_and_train) to also hold the prediction series next to the train and the test series (and threshold values for test)\n",
    "dict_of_chunk_series_with_test_and_train_and_forecast = dict_of_chunk_series_with_test_and_train.copy()\n",
    "\n",
    "for i,chunk in enumerate(dict_of_chunk_series_with_test_and_train_and_forecast):\n",
    "    for i,chunk_iteration in enumerate(dict_of_chunk_series_with_test_and_train_and_forecast[chunk]):\n",
    "        #train the arima model on the train list of the current chunk iteration\n",
    "        current_train_list = dict_of_chunk_series_with_test_and_train_and_forecast[chunk][chunk_iteration][\"TRAIN_LIST\"]\n",
    "        arima = pm.auto_arima(current_train_list)\n",
    "\n",
    "        #make sure that test_list and prediction_list share the same indizes to make alarm prediction easier later on - working for current step size and sampling rate\n",
    "        #if index is alway the auto index and does not relate to \"Hours:Since_First_Measurement\" directly then \"1\" as step size here is working\n",
    "        forecast = pd.Series(arima.predict(TEST),index= [*range(i+TRAIN,i+TRAIN+TEST,1)],name=\"forecast_list\")\n",
    "        \n",
    "        #add prediction to dictionary\n",
    "        dict_of_chunk_series_with_test_and_train_and_forecast[chunk][chunk_iteration][\"FORECAST_LIST\"] = forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of accessing the dictionary (containingo of train list, test list, prediction list) of a specific chunk iteration\n",
    "dict_of_chunk_series_with_test_and_train_and_forecast[chunk].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to data frame - check if that makes alarm prediction easier\n",
    "pd.concat([chunk_value_series,chunk_threshold_high_series,chunk_threshold_low_series],axis=1)\n",
    "dict_of_chunk_series_with_forecast = {}\n",
    "\n",
    "for i,chunk in enumerate(dict_of_chunk_series_with_test_and_train_and_forecast):\n",
    "    dict_of_chunk_series_with_forecast[chunk] = {}\n",
    "\n",
    "    for i,chunk_iteration in enumerate(dict_of_chunk_series_with_test_and_train_and_forecast[chunk]):\n",
    "        \n",
    "        train_list = dict_of_chunk_series_with_test_and_train_and_forecast[chunk][chunk_iteration][\"TRAIN_LIST\"]\n",
    "        test_list = dict_of_chunk_series_with_test_and_train_and_forecast[chunk][chunk_iteration][\"TEST_LIST\"]\n",
    "        threshold_high_for_test_list = dict_of_chunk_series_with_test_and_train_and_forecast[chunk][chunk_iteration][\"THRESHOLD_HIGH_FOR_TEST_LIST\"]\n",
    "        threshold_low_for_test_list = dict_of_chunk_series_with_test_and_train_and_forecast[chunk][chunk_iteration][\"THRESHOLD_LOW_FOR_TEST_LIST\"]\n",
    "        forecast_list = dict_of_chunk_series_with_test_and_train_and_forecast[chunk][chunk_iteration][\"FORECAST_LIST\"]\n",
    "        all_dict_lists_as_df = pd.concat([train_list,test_list,threshold_high_for_test_list,threshold_low_for_test_list,forecast_list],axis=1)\n",
    "        dict_of_chunk_series_with_forecast[chunkid][chunk_iteration] = all_dict_lists_as_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of accessing the dictionary (contains of train list, test list, prediction list in a !dataframe!) of a specific chunk iteration\n",
    "dict_of_chunk_series_with_forecast[chunk][11]"
   ]
  },
  {
   "source": [
    "Arima from previous version:\n",
    "auto_arima_model = auto_arima(data, start_p=1, start_q=1,\n",
    "                            max_p=3, max_q=3, m=1,\n",
    "                            start_P=0, seasonal=False,\n",
    "                            d=1, D=1, trace=True,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add information whether alarm was triggered\n",
    "dict_of_chunk_series_with_forecast_and_alarms = dict_of_chunk_series_with_forecast.copy() \n",
    "for i,chunk in enumerate(dict_of_chunk_series_with_forecast_and_alarms):\n",
    "    #dict_of_chunk_series_with_forecast_and_alarms[chunk] = {}\n",
    "\n",
    "    for i,chunk_iteration in enumerate(dict_of_chunk_series_with_forecast_and_alarms[chunk]):\n",
    "        \n",
    "        df_for_chunk_iteration = dict_of_chunk_series_with_forecast_and_alarms[chunk][chunk_iteration]\n",
    "        df_for_chunk_iteration['high_alarm_triggered'] = np.where(df_for_chunk_iteration['test_list'] > df_for_chunk_iteration['threshold_high_for_test_list'] ,1,0)\n",
    "        df_for_chunk_iteration['high_alarm_triggered_forecast'] = np.where(df_for_chunk_iteration['forecast_list'] > df_for_chunk_iteration['threshold_high_for_test_list'] ,1,0)\n",
    "        df_for_chunk_iteration['low_alarm_triggered'] = np.where(df_for_chunk_iteration['test_list'] < df_for_chunk_iteration['threshold_low_for_test_list'] ,1,0)\n",
    "        df_for_chunk_iteration['low_alarm_triggered_forecast'] = np.where(df_for_chunk_iteration['forecast_list'] < df_for_chunk_iteration['threshold_low_for_test_list'] ,1,0)\n",
    "        dict_of_chunk_series_with_forecast_and_alarms[chunk][chunk_iteration] = df_for_chunk_iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of accessing result for a specific chunk iteration (the one where the training set ends at index 12)\n",
    "dict_of_chunk_series_with_forecast_and_alarms[chunk][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy_dict_for_chunk_iterations = {}\n",
    "\n",
    "for i,chunk in enumerate(dict_of_chunk_series_with_forecast_and_alarms):\n",
    "    accuracy_dict_for_chunk_iterations[chunk] = {}\n",
    "       \n",
    "    for i,chunk_iteration in enumerate(dict_of_chunk_series_with_forecast_and_alarms[chunk]):\n",
    "        \n",
    "        tp, tn, fp, fn = 0, 0, 0, 0\n",
    "        accurracy_matrix_df_for_chunk_iteration = pd.DataFrame(columns=[\"TP\",\"FN\",\"FP\",\"TN\"]) \n",
    "        \n",
    "        #select column of dataframe but only where relevant (starting after train size)\n",
    "        df_for_chunk_iteration = dict_of_chunk_series_with_forecast_and_alarms[chunk][chunk_iteration]\n",
    "        \n",
    "        #select true high alarms triggered\n",
    "        column_index_of_high_alarm_triggered = df_for_chunk_iteration.columns.get_loc(\"high_alarm_triggered\")\n",
    "        #select predicted high alarms\n",
    "        column_index_of_high_alarm_triggered_forecast = df_for_chunk_iteration.columns.get_loc(\"high_alarm_triggered_forecast\")\n",
    "        \n",
    "        #create df with bot as column - only needed rows (test only)\n",
    "        high_alarms = df_for_chunk_iteration.iloc[TRAIN:,[column_index_of_high_alarm_triggered,column_index_of_high_alarm_triggered_forecast]]\n",
    "        \n",
    "        for row_in_high_alarms in high_alarms.iterrows():\n",
    "\n",
    "            if row_in_high_alarms[1][0] and row_in_high_alarms[1][1]:\n",
    "                tp +=1\n",
    "                print(\"tp\", tp)\n",
    "            if row_in_high_alarms[1][0] and not row_in_high_alarms[1][1]:\n",
    "                fn +=1\n",
    "                print(\"fn\", fn)\n",
    "            if not row_in_high_alarms[1][0] and row_in_high_alarms[1][1]:\n",
    "                fp +=1\n",
    "                print(\"fp\", fp)\n",
    "            if not row_in_high_alarms[1][0] and not row_in_high_alarms[1][1]:\n",
    "                tn +=1\n",
    "                print(\"tn\",tn)\n",
    "        \n",
    "        a_new_row = {\"TP\":tp,\"FN\":fn,\"FP\":fp,\"TN\":tn}\n",
    "        a_new_row_series = pd.Series(a_new_row,name=\"accuracy_high_alarms\")\n",
    "\n",
    "        accurracy_matrix_df_for_chunk_iteration = accurracy_matrix_df_for_chunk_iteration.append(a_new_row_series)\n",
    "        accuracy_dict_for_chunk_iterations[chunk][chunk_iteration] = accurracy_matrix_df_for_chunk_iteration\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example to check for all iterations of one chunk\n",
    "accuracy_dict_for_chunk_iterations[chunk]"
   ]
  }
 ]
}

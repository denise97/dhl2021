{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "interpreter": {
   "hash": "e2bfb1b1dd0bcdebdb315279aa118b1f834444d4ba3ba6d660e9f6ce7703f6a2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Load Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_data = '/hpi/fs00/share/MPSS2021BA1/data/'\n",
    "path_to_data = '../data/arima_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 - ARIMA - Steady Train Size 12\n",
    "import pickle\n",
    "# Heart Rate - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arima_12_hr_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arima_12_hr_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# Blood Pressure - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arima_12_bp_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arima_12_bp_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# O2 Saturation -  First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arima_12_o2_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arima_12_o2_first1000 = pickle.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - ARIMA - Expanding Train Size 12\n",
    "import pickle\n",
    "# Heart Rate - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arima_expanding_12_hr_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arima_expanding_12_hr_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# Blood Pressure - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arima_expanding_12_bp_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arima_expanding_12_bp_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# O2 Saturation -  First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arima_expanding_12_o2_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arima_expanding_12_o2_first1000 = pickle.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - ARIMAX - Steady Train Size 12\n",
    "import pickle\n",
    "# Heart Rate - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arimax_12_hr_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arimax_12_hr_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# Blood Pressure - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arimax_12_bp_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arimax_12_bp_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# O2 Saturation -  First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arimax_12_o2_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arimax_12_o2_first1000 = pickle.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - ARIMAX - Expanding Train Size 12\n",
    "import pickle\n",
    "# Heart Rate - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arimax_expanding_12_hr_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arimax_expanding_12_hr_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# Blood Pressure - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arimax_expanding_12_bp_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arimax_expanding_12_bp_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# O2 Saturation -  First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arimax_expanding_12_o2_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arimax_expanding_12_o2_first1000 = pickle.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "source": [
    "## Prepare Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Transform nested dictionary into data frame using pd.concat()\n",
    "def dict_to_df(selected_dict):\n",
    "\n",
    "    starttime = time.time()\n",
    "    print('START transforming dictionary.')\n",
    "    chunkno = 0\n",
    "\n",
    "    selected_dict_as_df = pd.DataFrame({})\n",
    "    confusion_matrices_per_chunk_in_selected_dict = {}\n",
    "    for chunkid in selected_dict:\n",
    "        confusion_matrices_per_chunk_in_selected_dict[chunkid] = pd.concat(selected_dict[chunkid], axis=0).reset_index().rename(columns={'level_0':'ITERATION', 'level_1':'ACCURACY_TYPE'})\n",
    "\n",
    "        chunkno = chunkno+1\n",
    "        # runningtime = round(((time.time() - starttime) / 60), 5)\n",
    "        # print('Completed chunk '+str(chunkid)+', running time in minutes: '+str(runningtime))\n",
    "\n",
    "    selected_dict_as_df = pd.concat(confusion_matrices_per_chunk_in_selected_dict, axis=0).reset_index(level=0).rename(columns={'level_0':'CHUNK_ID_FILLED_TH'})\n",
    "\n",
    "    endtime = round(((time.time() - starttime) / 60), 5)\n",
    "    print('DONE transforming dictionary.')\n",
    "    print('Completed '+str(chunkno)+' chunks in '+str(endtime)+' minutes')\n",
    "    print('Number of chunk-iteration combinations:',sum(selected_dict_as_df.groupby(['CHUNK_ID_FILLED_TH'])['ITERATION'].nunique()))\n",
    "    print('--------------------')\n",
    "\n",
    "    return selected_dict_as_df\n",
    "\n",
    "# Create confusion matrix per chunk by aggregating so that TP, FN, FP, and TN of the several chunk iterations are summed up\n",
    "def aggregate_to_confusion_matrix_per_chunk(df):\n",
    "    confusion_matrix_per_chunk = pd.DataFrame({})\n",
    "    confusion_matrix_per_chunk = df.groupby(['CHUNK_ID_FILLED_TH', 'ACCURACY_TYPE'])[['TP', 'FN', 'FP', 'TN']].agg('sum').reset_index()\n",
    "    return confusion_matrix_per_chunk\n",
    "\n",
    "# Create confusion matrix by aggregating so that TP, FN, FP, and TN of all chunks are summed up\n",
    "def aggregate_to_confusion_matrix_per_model(df):\n",
    "    confusion_matrix_per_model = df.groupby(['ACCURACY_TYPE'])[['TP', 'FN', 'FP', 'TN']].agg('sum')\n",
    "    return confusion_matrix_per_model\n",
    "\n",
    "# Calculate statistics about the number of chunks as well as chunk iteration combinations \n",
    "def calc_stats(df):\n",
    "    chunks = len(df.CHUNK_ID_FILLED_TH.unique())\n",
    "    iterations = sum(df.groupby(['CHUNK_ID_FILLED_TH'])['ITERATION'].nunique())\n",
    "    stats = pd.DataFrame({\n",
    "        'CHUNKS':       [chunks],\n",
    "        'ITERATIONS':   [iterations]\n",
    "        })\n",
    "    return stats\n",
    "\n",
    "# Create confusion matrix per chunk by aggregating so that TP, FN, FP, and TN of the several chunk iterations are summed up\n",
    "def aggregate_to_confusion_matrix_per_chunk(df):\n",
    "    confusion_matrix_per_chunk = df.groupby(['CHUNK_ID_FILLED_TH', 'ACCURACY_TYPE'])[['TP', 'FN', 'FP', 'TN']].agg('sum').reset_index()\n",
    "    return confusion_matrix_per_chunk\n",
    "\n",
    "# Create confusion matrix by aggregating so that TP, FN, FP, and TN of all chunks are summed up\n",
    "def aggregate_to_confusion_matrix_per_model(df):\n",
    "    confusion_matrix_per_model = df.groupby(['ACCURACY_TYPE'])[['TP', 'FN', 'FP', 'TN']].agg('sum')\n",
    "    return confusion_matrix_per_model\n",
    "\n",
    "# Calculate statistical measures for evaluation of the model\n",
    "def calc_metrics(df):\n",
    "    TP_high = df.iloc[0,0]\n",
    "    FN_high = df.iloc[0,1]\n",
    "    FP_high = df.iloc[0,2]\n",
    "    TN_high = df.iloc[0,3]\n",
    "\n",
    "    TP_low = df.iloc[1,0]\n",
    "    FN_low = df.iloc[1,1]\n",
    "    FP_low = df.iloc[1,2]\n",
    "    TN_low = df.iloc[1,3]\n",
    "\n",
    "    fpr_high = FP_high/(FP_high+TN_high)\n",
    "    tpr_high = TP_high/(TP_high+FN_high)\n",
    "    fnr_high = FN_high/(TP_high+FN_high)\n",
    "    tnr_high = TN_high/(FP_high+TN_high)\n",
    "    acc_high = (TP_high+TN_high)/(TP_high+FN_high+FP_high+TN_high)\n",
    "    f1s_high = TP_high/(TP_high+0.5*(FP_high+FN_high))\n",
    "\n",
    "    fpr_low = FP_low/(FP_low+TN_low)\n",
    "    tpr_low = TP_low/(TP_low+FN_low)\n",
    "    fnr_low = FN_low/(TP_low+FN_low)\n",
    "    tnr_low = TN_low/(FP_low+TN_low)\n",
    "    acc_low = (TP_low+TN_low)/(TP_low+FN_low+FP_low+TN_low)\n",
    "    f1s_low = TP_low/(TP_low+0.5*(FP_low+FN_low))\n",
    "\n",
    "    metrics = pd.DataFrame({\n",
    "        'ALARMTYPE':    ['High',        'Low'],\n",
    "        'FPR':          [fpr_high,      fpr_low],\n",
    "        'TPR':          [tpr_high,      tpr_low],\n",
    "        'FNR':          [fnr_high,      fnr_low],\n",
    "        'TNR':          [tnr_high,      tnr_low],\n",
    "        'ACC':          [acc_high,      acc_low],\n",
    "        'F1S':          [f1s_high,      f1s_low]\n",
    "        }).round(4)\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 01 - ARIMA - Steady Train Size 12\n",
    "\n",
    "# Transform nested dictionary into data frame\n",
    "\n",
    "confusion_matrices_per_chunk_01_arima_steady_12_hr_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arima_12_hr_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_01_arima_steady_12_bp_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arima_12_bp_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_01_arima_steady_12_o2_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arima_12_o2_first1000)\n",
    "\n",
    "# Calculate statistics about the number of chunks as well as chunk iteration combinations\n",
    "\n",
    "stats_01_arima_steady_12_hr_first1000 = calc_stats(confusion_matrices_per_chunk_01_arima_steady_12_hr_first1000)\n",
    "\n",
    "stats_01_arima_steady_12_bp_first1000 = calc_stats(confusion_matrices_per_chunk_01_arima_steady_12_bp_first1000)\n",
    "\n",
    "stats_01_arima_steady_12_o2_first1000 = calc_stats(confusion_matrices_per_chunk_01_arima_steady_12_o2_first1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - ARIMA - Expanding Train Size 12\n",
    "# Transform nested dictionary into data frame\n",
    "\n",
    "confusion_matrices_per_chunk_02_arima_expanding_12_hr_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arima_expanding_12_hr_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_02_arima_expanding_12_bp_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arima_expanding_12_bp_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_02_arima_expanding_12_o2_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arima_expanding_12_o2_first1000)\n",
    "\n",
    "# Calculate statistics about the number of chunks as well as chunk iteration combinations\n",
    "\n",
    "stats_02_arima_expanding_12_hr_first1000 = calc_stats(confusion_matrices_per_chunk_02_arima_expanding_12_hr_first1000)\n",
    "\n",
    "stats_02_arima_expanding_12_bp_first1000 = calc_stats(confusion_matrices_per_chunk_02_arima_expanding_12_bp_first1000)\n",
    "\n",
    "stats_02_arima_expanding_12_o2_first1000 = calc_stats(confusion_matrices_per_chunk_02_arima_expanding_12_o2_first1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 03 - ARIMAX - Steady Train Size 12\n",
    "# Transform nested dictionary into data frame\n",
    "\n",
    "confusion_matrices_per_chunk_03_arimax_steady_12_hr_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arimax_12_hr_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_03_arimax_steady_12_bp_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arimax_12_bp_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_03_arimax_steady_12_o2_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arimax_12_o2_first1000)\n",
    "\n",
    "# Calculate statistics about the number of chunks as well as chunk iteration combinations\n",
    "\n",
    "stats_03_arimax_steady_12_hr_first1000 = calc_stats(confusion_matrices_per_chunk_03_arimax_steady_12_hr_first1000)\n",
    "\n",
    "stats_03_arimax_steady_12_bp_first1000 = calc_stats(confusion_matrices_per_chunk_03_arimax_steady_12_bp_first1000)\n",
    "\n",
    "stats_03_arimax_steady_12_o2_first1000 = calc_stats(confusion_matrices_per_chunk_03_arimax_steady_12_o2_first1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - ARIMAX - Expanding Train Size 12\n",
    "# Transform nested dictionary into data frame\n",
    "\n",
    "confusion_matrices_per_chunk_04_arimax_expanding_12_hr_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arimax_expanding_12_hr_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_04_arimax_expanding_12_bp_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arimax_expanding_12_bp_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_04_arimax_expanding_12_o2_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arimax_expanding_12_o2_first1000)\n",
    "\n",
    "# Calculate statistics about the number of chunks as well as chunk iteration combinations\n",
    "\n",
    "stats_04_arimax_expanding_12_hr_first1000 = calc_stats(confusion_matrices_per_chunk_04_arimax_expanding_12_hr_first1000)\n",
    "\n",
    "stats_04_arimax_expanding_12_bp_first1000 = calc_stats(confusion_matrices_per_chunk_04_arimax_expanding_12_bp_first1000)\n",
    "\n",
    "stats_04_arimax_expanding_12_o2_first1000 = calc_stats(confusion_matrices_per_chunk_04_arimax_expanding_12_o2_first1000)"
   ]
  },
  {
   "source": [
    "## Analyze Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 - ARIMA - Steady TRAIN Size 12\n",
    "\n",
    "# Create confusion matrix per chunk by aggregating so that TP, FN, FP, and TN of the several chunk iterations are summed up\n",
    "\n",
    "confusion_matrix_per_chunk_01_arima_steady_12_hr_first1000 = aggregate_to_confusion_matrix_per_chunk(confusion_matrices_per_chunk_01_arima_steady_12_hr_first1000)\n",
    "\n",
    "confusion_matrix_per_chunk_01_arima_steady_12_bp_first1000 = aggregate_to_confusion_matrix_per_chunk(confusion_matrices_per_chunk_01_arima_steady_12_bp_first1000)\n",
    "\n",
    "confusion_matrix_per_chunk_01_arima_steady_12_o2_first1000 = aggregate_to_confusion_matrix_per_chunk(confusion_matrices_per_chunk_01_arima_steady_12_o2_first1000)\n",
    "\n",
    "# Create confusion matrix by aggregating so that TP, FN, FP, and TN of all chunks are summed up\n",
    "\n",
    "confusion_matrix_01_arima_steady_12_hr_first1000 = aggregate_to_confusion_matrix_per_model(confusion_matrices_per_chunk_01_arima_steady_12_hr_first1000)\n",
    "\n",
    "confusion_matrix_01_arima_steady_12_bp_first1000 = aggregate_to_confusion_matrix_per_model(confusion_matrices_per_chunk_01_arima_steady_12_bp_first1000)\n",
    "\n",
    "confusion_matrix_01_arima_steady_12_o2_first1000 = aggregate_to_confusion_matrix_per_model(confusion_matrices_per_chunk_01_arima_steady_12_o2_first1000)\n",
    "\n",
    "# Calculate statistical measures for evaluation of the model\n",
    "\n",
    "metrics_01_arima_steady_12_hr_first1000 = calc_metrics(confusion_matrix_01_arima_steady_12_hr_first1000)\n",
    "\n",
    "metrics_01_arima_steady_12_bp_first1000 = calc_metrics(confusion_matrix_01_arima_steady_12_bp_first1000)\n",
    "\n",
    "metrics_01_arima_steady_12_o2_first1000 = calc_metrics(confusion_matrix_01_arima_steady_12_o2_first1000)\n",
    "\n",
    "# Create overview table\n",
    "\n",
    "model_01_arima_steady_12 = pd.DataFrame({\n",
    "    'ID':           ['01',          '01'],\n",
    "    'MODEL':        ['ARIMA',       'ARIMA'],\n",
    "    'RESAMPLING':   ['Median',      'Median'],\n",
    "    'EXOGENOUS':    ['n/a',         'n/a'],\n",
    "    'TRAINSIZE':    ['Steady 12',   'Steady 12']\n",
    "    })\n",
    "\n",
    "model_01_arima_steady_12_hr_first1000 = pd.DataFrame({'PARAMETER': ['HR']})\n",
    "overview_01_arima_steady_12_hr_first1000 = pd.concat([\n",
    "    model_01_arima_steady_12_hr_first1000,\n",
    "    model_01_arima_steady_12, \n",
    "    metrics_01_arima_steady_12_hr_first1000,\n",
    "    stats_01_arima_steady_12_hr_first1000], axis=1).pad()\n",
    "\n",
    "model_01_arima_steady_12_bp_first1000 = pd.DataFrame({'PARAMETER': ['BP']})\n",
    "overview_01_arima_steady_12_bp_first1000 = pd.concat([\n",
    "    model_01_arima_steady_12_bp_first1000,\n",
    "    model_01_arima_steady_12, \n",
    "    metrics_01_arima_steady_12_bp_first1000,\n",
    "    stats_01_arima_steady_12_bp_first1000], axis=1).pad()\n",
    "\n",
    "model_01_arima_steady_12_o2_first1000 = pd.DataFrame({'PARAMETER': ['O2']})\n",
    "overview_01_arima_steady_12_o2_first1000 = pd.concat([\n",
    "    model_01_arima_steady_12_o2_first1000,\n",
    "    model_01_arima_steady_12, \n",
    "    metrics_01_arima_steady_12_o2_first1000,\n",
    "    stats_01_arima_steady_12_o2_first1000], axis=1).pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - ARIMA - Expanding TRAIN Size\n",
    "\n",
    "# Create confusion matrix per chunk by aggregating so that TP, FN, FP, and TN of the several chunk iterations are summed up\n",
    "\n",
    "confusion_matrix_per_chunk_02_arima_expanding_12_hr_first1000 = aggregate_to_confusion_matrix_per_chunk(confusion_matrices_per_chunk_02_arima_expanding_12_hr_first1000)\n",
    "\n",
    "confusion_matrix_per_chunk_02_arima_expanding_12_bp_first1000 = aggregate_to_confusion_matrix_per_chunk(confusion_matrices_per_chunk_02_arima_expanding_12_bp_first1000)\n",
    "\n",
    "confusion_matrix_per_chunk_02_arima_expanding_12_o2_first1000 = aggregate_to_confusion_matrix_per_chunk(confusion_matrices_per_chunk_02_arima_expanding_12_o2_first1000)\n",
    "\n",
    "# Create confusion matrix by aggregating so that TP, FN, FP, and TN of all chunks are summed up\n",
    "\n",
    "confusion_matrix_02_arima_expanding_12_hr_first1000 = aggregate_to_confusion_matrix_per_model(confusion_matrices_per_chunk_02_arima_expanding_12_hr_first1000)\n",
    "\n",
    "confusion_matrix_02_arima_expanding_12_bp_first1000 = aggregate_to_confusion_matrix_per_model(confusion_matrices_per_chunk_02_arima_expanding_12_bp_first1000)\n",
    "\n",
    "confusion_matrix_02_arima_expanding_12_o2_first1000 = aggregate_to_confusion_matrix_per_model(confusion_matrices_per_chunk_02_arima_expanding_12_o2_first1000)\n",
    "\n",
    "# Calculate statistical measures for evaluation of the model\n",
    "\n",
    "metrics_02_arima_expanding_12_hr_first1000 = calc_metrics(confusion_matrix_02_arima_expanding_12_hr_first1000)\n",
    "\n",
    "metrics_02_arima_expanding_12_bp_first1000 = calc_metrics(confusion_matrix_02_arima_expanding_12_bp_first1000)\n",
    "\n",
    "metrics_02_arima_expanding_12_o2_first1000 = calc_metrics(confusion_matrix_02_arima_expanding_12_o2_first1000)\n",
    "\n",
    "# Create overview table\n",
    "\n",
    "model_02_arima_expanding_12 = pd.DataFrame({\n",
    "    'ID':           ['02',              '02'],\n",
    "    'MODEL':        ['ARIMA',           'ARIMA'],\n",
    "    'RESAMPLING':   ['Median',          'Median'],\n",
    "    'EXOGENOUS':    ['n/a',             'n/a'],\n",
    "    'TRAINSIZE':    ['Expanding 12',    'Expanding 12']\n",
    "    })\n",
    "\n",
    "model_02_arima_expanding_12_hr_first1000 = pd.DataFrame({'PARAMETER': ['HR']})\n",
    "overview_02_arima_expanding_12_hr_first1000 = pd.concat([\n",
    "    model_02_arima_expanding_12_hr_first1000,\n",
    "    model_02_arima_expanding_12, \n",
    "    metrics_02_arima_expanding_12_hr_first1000,\n",
    "    stats_02_arima_expanding_12_hr_first1000], axis=1).pad()\n",
    "\n",
    "model_02_arima_expanding_12_bp_first1000 = pd.DataFrame({'PARAMETER': ['BP']})\n",
    "overview_02_arima_expanding_12_bp_first1000 = pd.concat([\n",
    "    model_02_arima_expanding_12_bp_first1000,\n",
    "    model_02_arima_expanding_12, \n",
    "    metrics_02_arima_expanding_12_bp_first1000,\n",
    "    stats_02_arima_expanding_12_bp_first1000], axis=1).pad()\n",
    "\n",
    "model_02_arima_expanding_12_o2_first1000 = pd.DataFrame({'PARAMETER': ['O2']})\n",
    "overview_02_arima_expanding_12_o2_first1000 = pd.concat([\n",
    "    model_02_arima_expanding_12_o2_first1000,\n",
    "    model_02_arima_expanding_12, \n",
    "    metrics_02_arima_expanding_12_o2_first1000,\n",
    "    stats_02_arima_expanding_12_o2_first1000], axis=1).pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - ARIMAX - Steady TRAIN Size 12\n",
    "\n",
    "# Create confusion matrix per chunk by aggregating so that TP, FN, FP, and TN of the several chunk iterations are summed up\n",
    "\n",
    "confusion_matrix_per_chunk_03_arimax_steady_12_hr_first1000 = aggregate_to_confusion_matrix_per_chunk(confusion_matrices_per_chunk_03_arimax_steady_12_hr_first1000)\n",
    "\n",
    "confusion_matrix_per_chunk_03_arimax_steady_12_bp_first1000 = aggregate_to_confusion_matrix_per_chunk(confusion_matrices_per_chunk_03_arimax_steady_12_bp_first1000)\n",
    "\n",
    "confusion_matrix_per_chunk_03_arimax_steady_12_o2_first1000 = aggregate_to_confusion_matrix_per_chunk(confusion_matrices_per_chunk_03_arimax_steady_12_o2_first1000)\n",
    "\n",
    "# Create confusion matrix by aggregating so that TP, FN, FP, and TN of all chunks are summed up\n",
    "\n",
    "confusion_matrix_03_arimax_steady_12_hr_first1000 = aggregate_to_confusion_matrix_per_model(confusion_matrices_per_chunk_03_arimax_steady_12_hr_first1000)\n",
    "\n",
    "confusion_matrix_03_arimax_steady_12_bp_first1000 = aggregate_to_confusion_matrix_per_model(confusion_matrices_per_chunk_03_arimax_steady_12_bp_first1000)\n",
    "\n",
    "confusion_matrix_03_arimax_steady_12_o2_first1000 = aggregate_to_confusion_matrix_per_model(confusion_matrices_per_chunk_03_arimax_steady_12_o2_first1000)\n",
    "\n",
    "# Calculate statistical measures for evaluation of the model\n",
    "\n",
    "metrics_03_arimax_steady_12_hr_first1000 = calc_metrics(confusion_matrix_03_arimax_steady_12_hr_first1000)\n",
    "\n",
    "metrics_03_arimax_steady_12_bp_first1000 = calc_metrics(confusion_matrix_03_arimax_steady_12_bp_first1000)\n",
    "\n",
    "metrics_03_arimax_steady_12_o2_first1000 = calc_metrics(confusion_matrix_03_arimax_steady_12_o2_first1000)\n",
    "\n",
    "# Create overview table\n",
    "\n",
    "model_03_arimax_steady_12 = pd.DataFrame({\n",
    "    'ID':           ['03',          '03'],\n",
    "    'MODEL':        ['ARIMAX',      'ARIMAX'],\n",
    "    'RESAMPLING':   ['Max',         'Min'],\n",
    "    'EXOGENOUS':    ['Median',      'Median'],\n",
    "    'TRAINSIZE':    ['Steady 12',   'Steady 12']\n",
    "    })\n",
    "\n",
    "model_03_arimax_steady_12_hr_first1000 = pd.DataFrame({'PARAMETER': ['HR']})\n",
    "overview_03_arimax_steady_12_hr_first1000 = pd.concat([\n",
    "    model_03_arimax_steady_12_hr_first1000,\n",
    "    model_03_arimax_steady_12, \n",
    "    metrics_03_arimax_steady_12_hr_first1000,\n",
    "    stats_03_arimax_steady_12_hr_first1000], axis=1).pad()\n",
    "\n",
    "model_03_arimax_steady_12_bp_first1000 = pd.DataFrame({'PARAMETER': ['BP']})\n",
    "overview_03_arimax_steady_12_bp_first1000 = pd.concat([\n",
    "    model_03_arimax_steady_12_bp_first1000,\n",
    "    model_03_arimax_steady_12, \n",
    "    metrics_03_arimax_steady_12_bp_first1000,\n",
    "    stats_03_arimax_steady_12_bp_first1000], axis=1).pad()\n",
    "\n",
    "model_03_arimax_steady_12_o2_first1000 = pd.DataFrame({'PARAMETER': ['O2']})\n",
    "overview_03_arimax_steady_12_o2_first1000 = pd.concat([\n",
    "    model_03_arimax_steady_12_o2_first1000,\n",
    "    model_03_arimax_steady_12, \n",
    "    metrics_03_arimax_steady_12_o2_first1000,\n",
    "    stats_03_arimax_steady_12_o2_first1000], axis=1).pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - ARIMAX - Expanding TRAIN Size\n",
    "\n",
    "# Create confusion matrix per chunk by aggregating so that TP, FN, FP, and TN of the several chunk iterations are summed up\n",
    "\n",
    "confusion_matrix_per_chunk_04_arimax_expanding_12_hr_first1000 = aggregate_to_confusion_matrix_per_chunk(confusion_matrices_per_chunk_04_arimax_expanding_12_hr_first1000)\n",
    "\n",
    "confusion_matrix_per_chunk_04_arimax_expanding_12_bp_first1000 = aggregate_to_confusion_matrix_per_chunk(confusion_matrices_per_chunk_04_arimax_expanding_12_bp_first1000)\n",
    "\n",
    "confusion_matrix_per_chunk_04_arimax_expanding_12_o2_first1000 = aggregate_to_confusion_matrix_per_chunk(confusion_matrices_per_chunk_04_arimax_expanding_12_o2_first1000)\n",
    "\n",
    "# Create confusion matrix by aggregating so that TP, FN, FP, and TN of all chunks are summed up\n",
    "\n",
    "confusion_matrix_04_arimax_expanding_12_hr_first1000 = aggregate_to_confusion_matrix_per_model(confusion_matrices_per_chunk_04_arimax_expanding_12_hr_first1000)\n",
    "\n",
    "confusion_matrix_04_arimax_expanding_12_bp_first1000 = aggregate_to_confusion_matrix_per_model(confusion_matrices_per_chunk_04_arimax_expanding_12_bp_first1000)\n",
    "\n",
    "confusion_matrix_04_arimax_expanding_12_o2_first1000 = aggregate_to_confusion_matrix_per_model(confusion_matrices_per_chunk_04_arimax_expanding_12_o2_first1000)\n",
    "\n",
    "# Calculate statistical measures for evaluation of the model\n",
    "\n",
    "metrics_04_arimax_expanding_12_hr_first1000 = calc_metrics(confusion_matrix_04_arimax_expanding_12_hr_first1000)\n",
    "\n",
    "metrics_04_arimax_expanding_12_bp_first1000 = calc_metrics(confusion_matrix_04_arimax_expanding_12_bp_first1000)\n",
    "\n",
    "metrics_04_arimax_expanding_12_o2_first1000 = calc_metrics(confusion_matrix_04_arimax_expanding_12_o2_first1000)\n",
    "\n",
    "# Create overview table\n",
    "\n",
    "model_04_arimax_expanding_12 = pd.DataFrame({\n",
    "    'ID':           ['04',              '04'],\n",
    "    'MODEL':        ['ARIMAX',          'ARIMAX'],\n",
    "    'RESAMPLING':   ['Max',             'Min'],\n",
    "    'EXOGENOUS':    ['Median',          'Median'],\n",
    "    'TRAINSIZE':    ['Expanding 12',    'Expanding 12']\n",
    "    })\n",
    "\n",
    "model_04_arimax_expanding_12_hr_first1000 = pd.DataFrame({'PARAMETER': ['HR']})\n",
    "overview_04_arimax_expanding_12_hr_first1000 = pd.concat([\n",
    "    model_04_arimax_expanding_12_hr_first1000,\n",
    "    model_04_arimax_expanding_12, \n",
    "    metrics_04_arimax_expanding_12_hr_first1000,\n",
    "    stats_04_arimax_expanding_12_hr_first1000], axis=1).pad()\n",
    "\n",
    "model_04_arimax_expanding_12_bp_first1000 = pd.DataFrame({'PARAMETER': ['BP']})\n",
    "overview_04_arimax_expanding_12_bp_first1000 = pd.concat([\n",
    "    model_04_arimax_expanding_12_bp_first1000,\n",
    "    model_04_arimax_expanding_12, \n",
    "    metrics_04_arimax_expanding_12_bp_first1000,\n",
    "    stats_04_arimax_expanding_12_bp_first1000], axis=1).pad()\n",
    "\n",
    "model_04_arimax_expanding_12_o2_first1000 = pd.DataFrame({'PARAMETER': ['O2']})\n",
    "overview_04_arimax_expanding_12_o2_first1000 = pd.concat([\n",
    "    model_04_arimax_expanding_12_o2_first1000,\n",
    "    model_04_arimax_expanding_12, \n",
    "    metrics_04_arimax_expanding_12_o2_first1000,\n",
    "    stats_04_arimax_expanding_12_o2_first1000], axis=1).pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overview tables per parameter\n",
    "\n",
    "overview_hr_first1000 = pd.concat([\n",
    "    overview_01_arima_steady_12_hr_first1000,\n",
    "    overview_02_arima_expanding_12_hr_first1000,\n",
    "    overview_03_arimax_steady_12_hr_first1000,\n",
    "    overview_04_arimax_expanding_12_hr_first1000], axis=0).reset_index(drop=True)\n",
    "\n",
    "overview_bp_first1000 = pd.concat([\n",
    "    overview_01_arima_steady_12_bp_first1000,\n",
    "    overview_02_arima_expanding_12_bp_first1000,\n",
    "    overview_03_arimax_steady_12_bp_first1000,\n",
    "    overview_04_arimax_expanding_12_bp_first1000], axis=0).reset_index(drop=True)\n",
    "\n",
    "overview_o2_first1000 = pd.concat([\n",
    "    overview_01_arima_steady_12_o2_first1000,\n",
    "    overview_02_arima_expanding_12_o2_first1000,\n",
    "    overview_03_arimax_steady_12_o2_first1000,\n",
    "    overview_04_arimax_expanding_12_o2_first1000], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  PARAMETER  ID   MODEL RESAMPLING EXOGENOUS     TRAINSIZE ALARMTYPE     FPR  \\\n0        HR  01   ARIMA     Median       n/a     Steady 12      High  0.0063   \n1        HR  01   ARIMA     Median       n/a     Steady 12       Low  0.0088   \n2        HR  02   ARIMA     Median       n/a  Expanding 12      High  0.0045   \n3        HR  02   ARIMA     Median       n/a  Expanding 12       Low  0.0035   \n4        HR  03  ARIMAX        Max    Median     Steady 12      High  0.0019   \n5        HR  03  ARIMAX        Min    Median     Steady 12       Low  0.0010   \n6        HR  04  ARIMAX        Max    Median  Expanding 12      High  0.0034   \n7        HR  04  ARIMAX        Min    Median  Expanding 12       Low  0.0014   \n\n      TPR     FNR     TNR     ACC     F1S  CHUNKS  ITERATIONS  \n0  0.3541  0.6459  0.9937  0.9842  0.3982   671.0     25111.0  \n1  0.4873  0.5127  0.9912  0.9872  0.3743   671.0     25111.0  \n2  0.3811  0.6189  0.9955  0.9865  0.4534   671.0     25111.0  \n3  0.4822  0.5178  0.9965  0.9924  0.5000   671.0     25111.0  \n4  0.9011  0.0989  0.9981  0.9964  0.8970   671.0     25112.0  \n5  0.9372  0.0628  0.9990  0.9985  0.9108   671.0     25112.0  \n6  0.8966  0.1034  0.9966  0.9949  0.8581   671.0     25112.0  \n7  0.9324  0.0676  0.9986  0.9980  0.8853   671.0     25112.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PARAMETER</th>\n      <th>ID</th>\n      <th>MODEL</th>\n      <th>RESAMPLING</th>\n      <th>EXOGENOUS</th>\n      <th>TRAINSIZE</th>\n      <th>ALARMTYPE</th>\n      <th>FPR</th>\n      <th>TPR</th>\n      <th>FNR</th>\n      <th>TNR</th>\n      <th>ACC</th>\n      <th>F1S</th>\n      <th>CHUNKS</th>\n      <th>ITERATIONS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HR</td>\n      <td>01</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Steady 12</td>\n      <td>High</td>\n      <td>0.0063</td>\n      <td>0.3541</td>\n      <td>0.6459</td>\n      <td>0.9937</td>\n      <td>0.9842</td>\n      <td>0.3982</td>\n      <td>671.0</td>\n      <td>25111.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HR</td>\n      <td>01</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Steady 12</td>\n      <td>Low</td>\n      <td>0.0088</td>\n      <td>0.4873</td>\n      <td>0.5127</td>\n      <td>0.9912</td>\n      <td>0.9872</td>\n      <td>0.3743</td>\n      <td>671.0</td>\n      <td>25111.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HR</td>\n      <td>02</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Expanding 12</td>\n      <td>High</td>\n      <td>0.0045</td>\n      <td>0.3811</td>\n      <td>0.6189</td>\n      <td>0.9955</td>\n      <td>0.9865</td>\n      <td>0.4534</td>\n      <td>671.0</td>\n      <td>25111.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HR</td>\n      <td>02</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Expanding 12</td>\n      <td>Low</td>\n      <td>0.0035</td>\n      <td>0.4822</td>\n      <td>0.5178</td>\n      <td>0.9965</td>\n      <td>0.9924</td>\n      <td>0.5000</td>\n      <td>671.0</td>\n      <td>25111.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HR</td>\n      <td>03</td>\n      <td>ARIMAX</td>\n      <td>Max</td>\n      <td>Median</td>\n      <td>Steady 12</td>\n      <td>High</td>\n      <td>0.0019</td>\n      <td>0.9011</td>\n      <td>0.0989</td>\n      <td>0.9981</td>\n      <td>0.9964</td>\n      <td>0.8970</td>\n      <td>671.0</td>\n      <td>25112.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>HR</td>\n      <td>03</td>\n      <td>ARIMAX</td>\n      <td>Min</td>\n      <td>Median</td>\n      <td>Steady 12</td>\n      <td>Low</td>\n      <td>0.0010</td>\n      <td>0.9372</td>\n      <td>0.0628</td>\n      <td>0.9990</td>\n      <td>0.9985</td>\n      <td>0.9108</td>\n      <td>671.0</td>\n      <td>25112.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>HR</td>\n      <td>04</td>\n      <td>ARIMAX</td>\n      <td>Max</td>\n      <td>Median</td>\n      <td>Expanding 12</td>\n      <td>High</td>\n      <td>0.0034</td>\n      <td>0.8966</td>\n      <td>0.1034</td>\n      <td>0.9966</td>\n      <td>0.9949</td>\n      <td>0.8581</td>\n      <td>671.0</td>\n      <td>25112.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>HR</td>\n      <td>04</td>\n      <td>ARIMAX</td>\n      <td>Min</td>\n      <td>Median</td>\n      <td>Expanding 12</td>\n      <td>Low</td>\n      <td>0.0014</td>\n      <td>0.9324</td>\n      <td>0.0676</td>\n      <td>0.9986</td>\n      <td>0.9980</td>\n      <td>0.8853</td>\n      <td>671.0</td>\n      <td>25112.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(overview_hr_first1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  PARAMETER  ID   MODEL RESAMPLING EXOGENOUS     TRAINSIZE ALARMTYPE     FPR  \\\n0        BP  01   ARIMA     Median       n/a     Steady 12      High  0.0231   \n1        BP  01   ARIMA     Median       n/a     Steady 12       Low  0.0111   \n2        BP  02   ARIMA     Median       n/a  Expanding 12      High  0.0196   \n3        BP  02   ARIMA     Median       n/a  Expanding 12       Low  0.0099   \n4        BP  03  ARIMAX        Max    Median     Steady 12      High  0.0044   \n5        BP  03  ARIMAX        Min    Median     Steady 12       Low  0.0052   \n6        BP  04  ARIMAX        Max    Median  Expanding 12      High  0.0074   \n7        BP  04  ARIMAX        Min    Median  Expanding 12       Low  0.0052   \n\n      TPR     FNR     TNR     ACC     F1S  CHUNKS  ITERATIONS  \n0  0.3545  0.6455  0.9769  0.9485  0.3861   207.0      2407.0  \n1  0.2603  0.7397  0.9889  0.9668  0.3220   207.0      2407.0  \n2  0.3545  0.6455  0.9804  0.9518  0.4021   207.0      2406.0  \n3  0.1781  0.8219  0.9901  0.9655  0.2385   207.0      2406.0  \n4  0.9310  0.0690  0.9956  0.9925  0.9231   207.0      2407.0  \n5  0.9487  0.0513  0.9948  0.9934  0.9024   207.0      2407.0  \n6  0.9310  0.0690  0.9926  0.9896  0.8963   207.0      2407.0  \n7  0.9487  0.0513  0.9948  0.9934  0.9024   207.0      2407.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PARAMETER</th>\n      <th>ID</th>\n      <th>MODEL</th>\n      <th>RESAMPLING</th>\n      <th>EXOGENOUS</th>\n      <th>TRAINSIZE</th>\n      <th>ALARMTYPE</th>\n      <th>FPR</th>\n      <th>TPR</th>\n      <th>FNR</th>\n      <th>TNR</th>\n      <th>ACC</th>\n      <th>F1S</th>\n      <th>CHUNKS</th>\n      <th>ITERATIONS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BP</td>\n      <td>01</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Steady 12</td>\n      <td>High</td>\n      <td>0.0231</td>\n      <td>0.3545</td>\n      <td>0.6455</td>\n      <td>0.9769</td>\n      <td>0.9485</td>\n      <td>0.3861</td>\n      <td>207.0</td>\n      <td>2407.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BP</td>\n      <td>01</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Steady 12</td>\n      <td>Low</td>\n      <td>0.0111</td>\n      <td>0.2603</td>\n      <td>0.7397</td>\n      <td>0.9889</td>\n      <td>0.9668</td>\n      <td>0.3220</td>\n      <td>207.0</td>\n      <td>2407.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BP</td>\n      <td>02</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Expanding 12</td>\n      <td>High</td>\n      <td>0.0196</td>\n      <td>0.3545</td>\n      <td>0.6455</td>\n      <td>0.9804</td>\n      <td>0.9518</td>\n      <td>0.4021</td>\n      <td>207.0</td>\n      <td>2406.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>BP</td>\n      <td>02</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Expanding 12</td>\n      <td>Low</td>\n      <td>0.0099</td>\n      <td>0.1781</td>\n      <td>0.8219</td>\n      <td>0.9901</td>\n      <td>0.9655</td>\n      <td>0.2385</td>\n      <td>207.0</td>\n      <td>2406.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BP</td>\n      <td>03</td>\n      <td>ARIMAX</td>\n      <td>Max</td>\n      <td>Median</td>\n      <td>Steady 12</td>\n      <td>High</td>\n      <td>0.0044</td>\n      <td>0.9310</td>\n      <td>0.0690</td>\n      <td>0.9956</td>\n      <td>0.9925</td>\n      <td>0.9231</td>\n      <td>207.0</td>\n      <td>2407.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>BP</td>\n      <td>03</td>\n      <td>ARIMAX</td>\n      <td>Min</td>\n      <td>Median</td>\n      <td>Steady 12</td>\n      <td>Low</td>\n      <td>0.0052</td>\n      <td>0.9487</td>\n      <td>0.0513</td>\n      <td>0.9948</td>\n      <td>0.9934</td>\n      <td>0.9024</td>\n      <td>207.0</td>\n      <td>2407.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>BP</td>\n      <td>04</td>\n      <td>ARIMAX</td>\n      <td>Max</td>\n      <td>Median</td>\n      <td>Expanding 12</td>\n      <td>High</td>\n      <td>0.0074</td>\n      <td>0.9310</td>\n      <td>0.0690</td>\n      <td>0.9926</td>\n      <td>0.9896</td>\n      <td>0.8963</td>\n      <td>207.0</td>\n      <td>2407.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>BP</td>\n      <td>04</td>\n      <td>ARIMAX</td>\n      <td>Min</td>\n      <td>Median</td>\n      <td>Expanding 12</td>\n      <td>Low</td>\n      <td>0.0052</td>\n      <td>0.9487</td>\n      <td>0.0513</td>\n      <td>0.9948</td>\n      <td>0.9934</td>\n      <td>0.9024</td>\n      <td>207.0</td>\n      <td>2407.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(overview_bp_first1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  PARAMETER  ID   MODEL RESAMPLING EXOGENOUS     TRAINSIZE ALARMTYPE     FPR  \\\n0        O2  01   ARIMA     Median       n/a     Steady 12      High  0.0087   \n1        O2  01   ARIMA     Median       n/a     Steady 12       Low  0.0388   \n2        O2  02   ARIMA     Median       n/a  Expanding 12      High  0.0074   \n3        O2  02   ARIMA     Median       n/a  Expanding 12       Low  0.0171   \n4        O2  03  ARIMAX        Max    Median     Steady 12      High  0.0303   \n5        O2  03  ARIMAX        Min    Median     Steady 12       Low  0.0073   \n6        O2  04  ARIMAX        Max    Median  Expanding 12      High  0.0652   \n7        O2  04  ARIMAX        Min    Median  Expanding 12       Low  0.0123   \n\n      TPR     FNR     TNR     ACC     F1S  CHUNKS  ITERATIONS  \n0  1.0000  0.0000  0.9913  0.9913  0.2609   438.0     11715.0  \n1  0.2851  0.7149  0.9612  0.9480  0.1759   438.0     11715.0  \n2  1.0000  0.0000  0.9926  0.9927  0.2951   438.0     11716.0  \n3  0.2632  0.7368  0.9829  0.9689  0.2479   438.0     11716.0  \n4  1.0000  0.0000  0.9697  0.9698  0.0923   438.0     11717.0  \n5  0.8893  0.1107  0.9927  0.9903  0.8087   438.0     11717.0  \n6  1.0000  0.0000  0.9348  0.9349  0.0451   438.0     11717.0  \n7  0.9004  0.0996  0.9877  0.9857  0.7439   438.0     11717.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PARAMETER</th>\n      <th>ID</th>\n      <th>MODEL</th>\n      <th>RESAMPLING</th>\n      <th>EXOGENOUS</th>\n      <th>TRAINSIZE</th>\n      <th>ALARMTYPE</th>\n      <th>FPR</th>\n      <th>TPR</th>\n      <th>FNR</th>\n      <th>TNR</th>\n      <th>ACC</th>\n      <th>F1S</th>\n      <th>CHUNKS</th>\n      <th>ITERATIONS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>O2</td>\n      <td>01</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Steady 12</td>\n      <td>High</td>\n      <td>0.0087</td>\n      <td>1.0000</td>\n      <td>0.0000</td>\n      <td>0.9913</td>\n      <td>0.9913</td>\n      <td>0.2609</td>\n      <td>438.0</td>\n      <td>11715.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>O2</td>\n      <td>01</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Steady 12</td>\n      <td>Low</td>\n      <td>0.0388</td>\n      <td>0.2851</td>\n      <td>0.7149</td>\n      <td>0.9612</td>\n      <td>0.9480</td>\n      <td>0.1759</td>\n      <td>438.0</td>\n      <td>11715.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>O2</td>\n      <td>02</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Expanding 12</td>\n      <td>High</td>\n      <td>0.0074</td>\n      <td>1.0000</td>\n      <td>0.0000</td>\n      <td>0.9926</td>\n      <td>0.9927</td>\n      <td>0.2951</td>\n      <td>438.0</td>\n      <td>11716.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>O2</td>\n      <td>02</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Expanding 12</td>\n      <td>Low</td>\n      <td>0.0171</td>\n      <td>0.2632</td>\n      <td>0.7368</td>\n      <td>0.9829</td>\n      <td>0.9689</td>\n      <td>0.2479</td>\n      <td>438.0</td>\n      <td>11716.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>O2</td>\n      <td>03</td>\n      <td>ARIMAX</td>\n      <td>Max</td>\n      <td>Median</td>\n      <td>Steady 12</td>\n      <td>High</td>\n      <td>0.0303</td>\n      <td>1.0000</td>\n      <td>0.0000</td>\n      <td>0.9697</td>\n      <td>0.9698</td>\n      <td>0.0923</td>\n      <td>438.0</td>\n      <td>11717.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>O2</td>\n      <td>03</td>\n      <td>ARIMAX</td>\n      <td>Min</td>\n      <td>Median</td>\n      <td>Steady 12</td>\n      <td>Low</td>\n      <td>0.0073</td>\n      <td>0.8893</td>\n      <td>0.1107</td>\n      <td>0.9927</td>\n      <td>0.9903</td>\n      <td>0.8087</td>\n      <td>438.0</td>\n      <td>11717.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>O2</td>\n      <td>04</td>\n      <td>ARIMAX</td>\n      <td>Max</td>\n      <td>Median</td>\n      <td>Expanding 12</td>\n      <td>High</td>\n      <td>0.0652</td>\n      <td>1.0000</td>\n      <td>0.0000</td>\n      <td>0.9348</td>\n      <td>0.9349</td>\n      <td>0.0451</td>\n      <td>438.0</td>\n      <td>11717.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>O2</td>\n      <td>04</td>\n      <td>ARIMAX</td>\n      <td>Min</td>\n      <td>Median</td>\n      <td>Expanding 12</td>\n      <td>Low</td>\n      <td>0.0123</td>\n      <td>0.9004</td>\n      <td>0.0996</td>\n      <td>0.9877</td>\n      <td>0.9857</td>\n      <td>0.7439</td>\n      <td>438.0</td>\n      <td>11717.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(overview_o2_first1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   PARAMETER  ID   MODEL RESAMPLING EXOGENOUS     TRAINSIZE ALARMTYPE     FPR  \\\n0         HR  01   ARIMA     Median       n/a     Steady 12      High  0.0063   \n1         HR  01   ARIMA     Median       n/a     Steady 12       Low  0.0088   \n2         HR  02   ARIMA     Median       n/a  Expanding 12      High  0.0045   \n3         HR  02   ARIMA     Median       n/a  Expanding 12       Low  0.0035   \n4         HR  03  ARIMAX        Max    Median     Steady 12      High  0.0019   \n5         HR  03  ARIMAX        Min    Median     Steady 12       Low  0.0010   \n6         HR  04  ARIMAX        Max    Median  Expanding 12      High  0.0034   \n7         HR  04  ARIMAX        Min    Median  Expanding 12       Low  0.0014   \n8         BP  01   ARIMA     Median       n/a     Steady 12      High  0.0231   \n9         BP  01   ARIMA     Median       n/a     Steady 12       Low  0.0111   \n10        BP  02   ARIMA     Median       n/a  Expanding 12      High  0.0196   \n11        BP  02   ARIMA     Median       n/a  Expanding 12       Low  0.0099   \n12        BP  03  ARIMAX        Max    Median     Steady 12      High  0.0044   \n13        BP  03  ARIMAX        Min    Median     Steady 12       Low  0.0052   \n14        BP  04  ARIMAX        Max    Median  Expanding 12      High  0.0074   \n15        BP  04  ARIMAX        Min    Median  Expanding 12       Low  0.0052   \n16        O2  01   ARIMA     Median       n/a     Steady 12      High  0.0087   \n17        O2  01   ARIMA     Median       n/a     Steady 12       Low  0.0388   \n18        O2  02   ARIMA     Median       n/a  Expanding 12      High  0.0074   \n19        O2  02   ARIMA     Median       n/a  Expanding 12       Low  0.0171   \n20        O2  03  ARIMAX        Max    Median     Steady 12      High  0.0303   \n21        O2  03  ARIMAX        Min    Median     Steady 12       Low  0.0073   \n22        O2  04  ARIMAX        Max    Median  Expanding 12      High  0.0652   \n23        O2  04  ARIMAX        Min    Median  Expanding 12       Low  0.0123   \n\n       TPR     FNR     TNR     ACC     F1S  CHUNKS  ITERATIONS  \n0   0.3541  0.6459  0.9937  0.9842  0.3982   671.0     25111.0  \n1   0.4873  0.5127  0.9912  0.9872  0.3743   671.0     25111.0  \n2   0.3811  0.6189  0.9955  0.9865  0.4534   671.0     25111.0  \n3   0.4822  0.5178  0.9965  0.9924  0.5000   671.0     25111.0  \n4   0.9011  0.0989  0.9981  0.9964  0.8970   671.0     25112.0  \n5   0.9372  0.0628  0.9990  0.9985  0.9108   671.0     25112.0  \n6   0.8966  0.1034  0.9966  0.9949  0.8581   671.0     25112.0  \n7   0.9324  0.0676  0.9986  0.9980  0.8853   671.0     25112.0  \n8   0.3545  0.6455  0.9769  0.9485  0.3861   207.0      2407.0  \n9   0.2603  0.7397  0.9889  0.9668  0.3220   207.0      2407.0  \n10  0.3545  0.6455  0.9804  0.9518  0.4021   207.0      2406.0  \n11  0.1781  0.8219  0.9901  0.9655  0.2385   207.0      2406.0  \n12  0.9310  0.0690  0.9956  0.9925  0.9231   207.0      2407.0  \n13  0.9487  0.0513  0.9948  0.9934  0.9024   207.0      2407.0  \n14  0.9310  0.0690  0.9926  0.9896  0.8963   207.0      2407.0  \n15  0.9487  0.0513  0.9948  0.9934  0.9024   207.0      2407.0  \n16  1.0000  0.0000  0.9913  0.9913  0.2609   438.0     11715.0  \n17  0.2851  0.7149  0.9612  0.9480  0.1759   438.0     11715.0  \n18  1.0000  0.0000  0.9926  0.9927  0.2951   438.0     11716.0  \n19  0.2632  0.7368  0.9829  0.9689  0.2479   438.0     11716.0  \n20  1.0000  0.0000  0.9697  0.9698  0.0923   438.0     11717.0  \n21  0.8893  0.1107  0.9927  0.9903  0.8087   438.0     11717.0  \n22  1.0000  0.0000  0.9348  0.9349  0.0451   438.0     11717.0  \n23  0.9004  0.0996  0.9877  0.9857  0.7439   438.0     11717.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PARAMETER</th>\n      <th>ID</th>\n      <th>MODEL</th>\n      <th>RESAMPLING</th>\n      <th>EXOGENOUS</th>\n      <th>TRAINSIZE</th>\n      <th>ALARMTYPE</th>\n      <th>FPR</th>\n      <th>TPR</th>\n      <th>FNR</th>\n      <th>TNR</th>\n      <th>ACC</th>\n      <th>F1S</th>\n      <th>CHUNKS</th>\n      <th>ITERATIONS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HR</td>\n      <td>01</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Steady 12</td>\n      <td>High</td>\n      <td>0.0063</td>\n      <td>0.3541</td>\n      <td>0.6459</td>\n      <td>0.9937</td>\n      <td>0.9842</td>\n      <td>0.3982</td>\n      <td>671.0</td>\n      <td>25111.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HR</td>\n      <td>01</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Steady 12</td>\n      <td>Low</td>\n      <td>0.0088</td>\n      <td>0.4873</td>\n      <td>0.5127</td>\n      <td>0.9912</td>\n      <td>0.9872</td>\n      <td>0.3743</td>\n      <td>671.0</td>\n      <td>25111.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HR</td>\n      <td>02</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Expanding 12</td>\n      <td>High</td>\n      <td>0.0045</td>\n      <td>0.3811</td>\n      <td>0.6189</td>\n      <td>0.9955</td>\n      <td>0.9865</td>\n      <td>0.4534</td>\n      <td>671.0</td>\n      <td>25111.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HR</td>\n      <td>02</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Expanding 12</td>\n      <td>Low</td>\n      <td>0.0035</td>\n      <td>0.4822</td>\n      <td>0.5178</td>\n      <td>0.9965</td>\n      <td>0.9924</td>\n      <td>0.5000</td>\n      <td>671.0</td>\n      <td>25111.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HR</td>\n      <td>03</td>\n      <td>ARIMAX</td>\n      <td>Max</td>\n      <td>Median</td>\n      <td>Steady 12</td>\n      <td>High</td>\n      <td>0.0019</td>\n      <td>0.9011</td>\n      <td>0.0989</td>\n      <td>0.9981</td>\n      <td>0.9964</td>\n      <td>0.8970</td>\n      <td>671.0</td>\n      <td>25112.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>HR</td>\n      <td>03</td>\n      <td>ARIMAX</td>\n      <td>Min</td>\n      <td>Median</td>\n      <td>Steady 12</td>\n      <td>Low</td>\n      <td>0.0010</td>\n      <td>0.9372</td>\n      <td>0.0628</td>\n      <td>0.9990</td>\n      <td>0.9985</td>\n      <td>0.9108</td>\n      <td>671.0</td>\n      <td>25112.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>HR</td>\n      <td>04</td>\n      <td>ARIMAX</td>\n      <td>Max</td>\n      <td>Median</td>\n      <td>Expanding 12</td>\n      <td>High</td>\n      <td>0.0034</td>\n      <td>0.8966</td>\n      <td>0.1034</td>\n      <td>0.9966</td>\n      <td>0.9949</td>\n      <td>0.8581</td>\n      <td>671.0</td>\n      <td>25112.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>HR</td>\n      <td>04</td>\n      <td>ARIMAX</td>\n      <td>Min</td>\n      <td>Median</td>\n      <td>Expanding 12</td>\n      <td>Low</td>\n      <td>0.0014</td>\n      <td>0.9324</td>\n      <td>0.0676</td>\n      <td>0.9986</td>\n      <td>0.9980</td>\n      <td>0.8853</td>\n      <td>671.0</td>\n      <td>25112.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>BP</td>\n      <td>01</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Steady 12</td>\n      <td>High</td>\n      <td>0.0231</td>\n      <td>0.3545</td>\n      <td>0.6455</td>\n      <td>0.9769</td>\n      <td>0.9485</td>\n      <td>0.3861</td>\n      <td>207.0</td>\n      <td>2407.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>BP</td>\n      <td>01</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Steady 12</td>\n      <td>Low</td>\n      <td>0.0111</td>\n      <td>0.2603</td>\n      <td>0.7397</td>\n      <td>0.9889</td>\n      <td>0.9668</td>\n      <td>0.3220</td>\n      <td>207.0</td>\n      <td>2407.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>BP</td>\n      <td>02</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Expanding 12</td>\n      <td>High</td>\n      <td>0.0196</td>\n      <td>0.3545</td>\n      <td>0.6455</td>\n      <td>0.9804</td>\n      <td>0.9518</td>\n      <td>0.4021</td>\n      <td>207.0</td>\n      <td>2406.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>BP</td>\n      <td>02</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Expanding 12</td>\n      <td>Low</td>\n      <td>0.0099</td>\n      <td>0.1781</td>\n      <td>0.8219</td>\n      <td>0.9901</td>\n      <td>0.9655</td>\n      <td>0.2385</td>\n      <td>207.0</td>\n      <td>2406.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>BP</td>\n      <td>03</td>\n      <td>ARIMAX</td>\n      <td>Max</td>\n      <td>Median</td>\n      <td>Steady 12</td>\n      <td>High</td>\n      <td>0.0044</td>\n      <td>0.9310</td>\n      <td>0.0690</td>\n      <td>0.9956</td>\n      <td>0.9925</td>\n      <td>0.9231</td>\n      <td>207.0</td>\n      <td>2407.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>BP</td>\n      <td>03</td>\n      <td>ARIMAX</td>\n      <td>Min</td>\n      <td>Median</td>\n      <td>Steady 12</td>\n      <td>Low</td>\n      <td>0.0052</td>\n      <td>0.9487</td>\n      <td>0.0513</td>\n      <td>0.9948</td>\n      <td>0.9934</td>\n      <td>0.9024</td>\n      <td>207.0</td>\n      <td>2407.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>BP</td>\n      <td>04</td>\n      <td>ARIMAX</td>\n      <td>Max</td>\n      <td>Median</td>\n      <td>Expanding 12</td>\n      <td>High</td>\n      <td>0.0074</td>\n      <td>0.9310</td>\n      <td>0.0690</td>\n      <td>0.9926</td>\n      <td>0.9896</td>\n      <td>0.8963</td>\n      <td>207.0</td>\n      <td>2407.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>BP</td>\n      <td>04</td>\n      <td>ARIMAX</td>\n      <td>Min</td>\n      <td>Median</td>\n      <td>Expanding 12</td>\n      <td>Low</td>\n      <td>0.0052</td>\n      <td>0.9487</td>\n      <td>0.0513</td>\n      <td>0.9948</td>\n      <td>0.9934</td>\n      <td>0.9024</td>\n      <td>207.0</td>\n      <td>2407.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>O2</td>\n      <td>01</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Steady 12</td>\n      <td>High</td>\n      <td>0.0087</td>\n      <td>1.0000</td>\n      <td>0.0000</td>\n      <td>0.9913</td>\n      <td>0.9913</td>\n      <td>0.2609</td>\n      <td>438.0</td>\n      <td>11715.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>O2</td>\n      <td>01</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Steady 12</td>\n      <td>Low</td>\n      <td>0.0388</td>\n      <td>0.2851</td>\n      <td>0.7149</td>\n      <td>0.9612</td>\n      <td>0.9480</td>\n      <td>0.1759</td>\n      <td>438.0</td>\n      <td>11715.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>O2</td>\n      <td>02</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Expanding 12</td>\n      <td>High</td>\n      <td>0.0074</td>\n      <td>1.0000</td>\n      <td>0.0000</td>\n      <td>0.9926</td>\n      <td>0.9927</td>\n      <td>0.2951</td>\n      <td>438.0</td>\n      <td>11716.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>O2</td>\n      <td>02</td>\n      <td>ARIMA</td>\n      <td>Median</td>\n      <td>n/a</td>\n      <td>Expanding 12</td>\n      <td>Low</td>\n      <td>0.0171</td>\n      <td>0.2632</td>\n      <td>0.7368</td>\n      <td>0.9829</td>\n      <td>0.9689</td>\n      <td>0.2479</td>\n      <td>438.0</td>\n      <td>11716.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>O2</td>\n      <td>03</td>\n      <td>ARIMAX</td>\n      <td>Max</td>\n      <td>Median</td>\n      <td>Steady 12</td>\n      <td>High</td>\n      <td>0.0303</td>\n      <td>1.0000</td>\n      <td>0.0000</td>\n      <td>0.9697</td>\n      <td>0.9698</td>\n      <td>0.0923</td>\n      <td>438.0</td>\n      <td>11717.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>O2</td>\n      <td>03</td>\n      <td>ARIMAX</td>\n      <td>Min</td>\n      <td>Median</td>\n      <td>Steady 12</td>\n      <td>Low</td>\n      <td>0.0073</td>\n      <td>0.8893</td>\n      <td>0.1107</td>\n      <td>0.9927</td>\n      <td>0.9903</td>\n      <td>0.8087</td>\n      <td>438.0</td>\n      <td>11717.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>O2</td>\n      <td>04</td>\n      <td>ARIMAX</td>\n      <td>Max</td>\n      <td>Median</td>\n      <td>Expanding 12</td>\n      <td>High</td>\n      <td>0.0652</td>\n      <td>1.0000</td>\n      <td>0.0000</td>\n      <td>0.9348</td>\n      <td>0.9349</td>\n      <td>0.0451</td>\n      <td>438.0</td>\n      <td>11717.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>O2</td>\n      <td>04</td>\n      <td>ARIMAX</td>\n      <td>Min</td>\n      <td>Median</td>\n      <td>Expanding 12</td>\n      <td>Low</td>\n      <td>0.0123</td>\n      <td>0.9004</td>\n      <td>0.0996</td>\n      <td>0.9877</td>\n      <td>0.9857</td>\n      <td>0.7439</td>\n      <td>438.0</td>\n      <td>11717.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "overview_first1000 = pd.concat([\n",
    "    overview_hr_first1000,\n",
    "    overview_bp_first1000,\n",
    "    overview_o2_first1000], axis=0).reset_index(drop=True)\n",
    "\n",
    "display(overview_first1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "overview_first1000.to_parquet(path_to_data+'arima-x_forecast_overview_first1000.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "overview_first1000 = pd.read_parquet(path_to_data+'arima-x_forecast_overview_first1000.parquet', engine='pyarrow')"
   ]
  },
  {
   "source": [
    "## Comparison for selected Chunk"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_chunkid = '200347.0_220045.0_2116-06-05 15:02:00'\n",
    "selected_iteration = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - ARIMA - Expanding TRAIN Size\n",
    "# Confusion matrix for selected chunk\n",
    "confusion_matrix_per_chunk_02_arima_expanding_df[\n",
    "    confusion_matrix_per_chunk_02_arima_expanding_df.CHUNK_ID_FILLED_TH == selected_chunkid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - ARIMA - Expanding TRAIN Size\n",
    "# Confusion matrix for selected iteration for selected chunk\n",
    "confusion_matrices_per_chunk_02_arima_expanding_df[\n",
    "    (confusion_matrices_per_chunk_02_arima_expanding_df.CHUNK_ID_FILLED_TH == selected_chunkid) &\n",
    "    (confusion_matrices_per_chunk_02_arima_expanding_df.ITERATION == selected_iteration)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - ARIMAX - Steady TRAIN Size 12\n",
    "# Confusion matrix for selected chunk\n",
    "confusion_matrix_per_chunk_03_arimax_steady_12_df[\n",
    "    confusion_matrix_per_chunk_03_arimax_steady_12_df.CHUNK_ID_FILLED_TH == selected_chunkid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - ARIMAX - Steady TRAIN Size 12\n",
    "# Confusion matrix for selected iteration for selected chunk\n",
    "confusion_matrices_per_chunk_03_arimax_steady_12_df[\n",
    "    (confusion_matrices_per_chunk_03_arimax_steady_12_df.CHUNK_ID_FILLED_TH == selected_chunkid) &\n",
    "    (confusion_matrices_per_chunk_03_arimax_steady_12_df.ITERATION == selected_iteration)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
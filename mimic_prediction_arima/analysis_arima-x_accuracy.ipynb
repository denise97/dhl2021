{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "interpreter": {
   "hash": "e2bfb1b1dd0bcdebdb315279aa118b1f834444d4ba3ba6d660e9f6ce7703f6a2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Load Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_data = '/hpi/fs00/share/MPSS2021BA1/data/'\n",
    "path_to_data = '../data/arima_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 - ARIMA - Steady Train Size 12\n",
    "import pickle\n",
    "# Heart Rate - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arima_12_hr_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arima_12_hr_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# Blood Pressure - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arima_12_bp_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arima_12_bp_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# O2 Saturation -  First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arima_12_o2_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arima_12_o2_first1000 = pickle.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - ARIMA - Expanding Train Size 12\n",
    "import pickle\n",
    "# Heart Rate - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arima_expanding_12_hr_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arima_expanding_12_hr_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# Blood Pressure - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arima_expanding_12_bp_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arima_expanding_12_bp_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# O2 Saturation -  First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arima_expanding_12_o2_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arima_expanding_12_o2_first1000 = pickle.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - ARIMAX - Steady Train Size 12\n",
    "import pickle\n",
    "# Heart Rate - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arimax_12_hr_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arimax_12_hr_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# Blood Pressure - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arimax_12_bp_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arimax_12_bp_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# O2 Saturation -  First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arimax_12_o2_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arimax_12_o2_first1000 = pickle.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - ARIMAX - Expanding Train Size 12\n",
    "import pickle\n",
    "# Heart Rate - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arimax_expanding_12_hr_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arimax_expanding_12_hr_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# Blood Pressure - First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arimax_expanding_12_bp_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arimax_expanding_12_bp_first1000 = pickle.load(input_file)\n",
    "input_file.close()\n",
    "# O2 Saturation -  First 1000\n",
    "input_file = open(path_to_data+'accuracy_dict_for_chunk_iterations_arimax_expanding_12_o2_first1000.pickle', 'rb')\n",
    "accuracy_dict_for_chunk_iterations_arimax_expanding_12_o2_first1000 = pickle.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "source": [
    "## Prepare Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Transform nested dictionary into data frame using pd.concat()\n",
    "\n",
    "def dict_to_df(selected_dict):\n",
    "\n",
    "    starttime = time.time()\n",
    "    print('START transforming dictionary.')\n",
    "    chunkno = 0\n",
    "\n",
    "    selected_dict_as_df = pd.DataFrame({})\n",
    "    confusion_matrices_per_chunk_in_selected_dict = {}\n",
    "    for chunkid in selected_dict:\n",
    "        confusion_matrices_per_chunk_in_selected_dict[chunkid] = pd.concat(selected_dict[chunkid], axis=0).reset_index().rename(columns={'level_0':'ITERATION', 'level_1':'ACCURACY_TYPE'})\n",
    "\n",
    "        chunkno = chunkno+1\n",
    "        runningtime = round(((time.time() - starttime) / 60), 5)\n",
    "        print('Completed chunk '+str(chunkid)+', running time in minutes: '+str(runningtime))\n",
    "\n",
    "    selected_dict_as_df = pd.concat(confusion_matrices_per_chunk_in_selected_dict, axis=0).reset_index(level=0).rename(columns={'level_0':'CHUNK_ID_FILLED_TH'})\n",
    "\n",
    "    endtime = round(((time.time() - starttime) / 60), 5)\n",
    "    print('DONE transforming dictionary.')\n",
    "    print('Completed '+str(chunkno)+' chunks in '+str(endtime)+' minutes')\n",
    "\n",
    "    return selected_dict_as_df#.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 - ARIMA - Steady Train Size 12\n",
    "# Transform nested dictionary into data frame\n",
    "\n",
    "confusion_matrices_per_chunk_01_arima_steady_12_hr_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arima_12_hr_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_01_arima_steady_12_bp_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arima_12_bp_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_01_arima_steady_12_o2_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arima_12_o2_first1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - ARIMA - Expanding Train Size 12\n",
    "# Transform nested dictionary into data frame\n",
    "\n",
    "confusion_matrices_per_chunk_02_arima_expanding_12_hr_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arima_expanding_12_hr_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_02_arima_expanding_12_bp_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arima_expanding_12_bp_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_02_arima_expanding_12_o2_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arima_expanding_12_o2_first1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 03 - ARIMAX - Steady Train Size 12\n",
    "# Transform nested dictionary into data frame\n",
    "\n",
    "confusion_matrices_per_chunk_03_arimax_steady_12_hr_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arimax_12_hr_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_03_arimax_steady_12_bp_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arimax_12_bp_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_03_arimax_steady_12_o2_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arimax_12_o2_first1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - ARIMAX - Expanding Train Size 12\n",
    "# Transform nested dictionary into data frame\n",
    "\n",
    "confusion_matrices_per_chunk_04_arimax_expanding_12_hr_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arimax_expanding_12_hr_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_04_arimax_expanding_12_bp_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arimax_expanding_12_bp_first1000)\n",
    "\n",
    "confusion_matrices_per_chunk_04_arimax_expanding_12_o2_first1000 = dict_to_df(accuracy_dict_for_chunk_iterations_arimax_expanding_12_o2_first1000)"
   ]
  },
  {
   "source": [
    "## Analyze Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This section is not updated yet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 - ARIMA - Steady TRAIN Size 12\n",
    "\n",
    "# Create confusion matrix per chunk by aggregating so that TP, FN, FP, and TN of the several chunk iterations are summed up\n",
    "\n",
    "# Create confusion matrix by aggregating so that TP, FN, FP, and TN of all chunks are summed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - ARIMA - Expanding TRAIN Size\n",
    "\n",
    "# Create confusion matrix per chunk by aggregating so that TP, FN, FP, and TN of the several chunk iterations are summed up\n",
    "confusion_matrix_per_chunk_02_arima_expanding_df = confusion_matrices_per_chunk_02_arima_expanding_df.groupby(['CHUNK_ID_FILLED_TH', 'ACCURACY_TYPE'])[['TP', 'FN', 'FP', 'TN']].agg('sum')\n",
    "confusion_matrix_per_chunk_02_arima_expanding_df = confusion_matrix_per_chunk_02_arima_expanding_df.reset_index()\n",
    "\n",
    "# Create confusion matrix by aggregating so that TP, FN, FP, and TN of all chunks are summed up\n",
    "confusion_matrix_02_arima_expanding_df = confusion_matrix_per_chunk_02_arima_expanding_df.groupby(['ACCURACY_TYPE'])[['TP', 'FN', 'FP', 'TN']].agg('sum')\n",
    "\n",
    "display(confusion_matrix_02_arima_expanding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = confusion_matrix_02_arima_expanding_df\n",
    "TP_high = df.iloc[0,0]\n",
    "FN_high = df.iloc[0,1]\n",
    "FP_high = df.iloc[0,2]\n",
    "TN_high = df.iloc[0,3]\n",
    "TP_low = df.iloc[1,0]\n",
    "FN_low = df.iloc[1,1]\n",
    "FP_low = df.iloc[1,2]\n",
    "TN_low = df.iloc[1,3]\n",
    "\n",
    "fpr_high_02_arima_expanding = round( (FP_high/(FP_high+TN_high)) ,4)\n",
    "tpr_high_02_arima_expanding = round( (TP_high/(TP_high+FN_high)) ,4) \n",
    "fnr_high_02_arima_expanding = round( (FN_high/(TP_high+FN_high)) ,4)\n",
    "tnr_high_02_arima_expanding = round( (TN_high/(FP_high+TN_high)) ,4)\n",
    "acc_high_02_arima_expanding = round( ((TP_high+TN_high)/(TP_high+FN_high+FP_high+TN_high)) ,4)\n",
    "f1s_high_02_arima_expanding = round( (TP_high/(TP_high+0.5*(FP_high+FN_high))) ,4)\n",
    "\n",
    "fpr_low_02_arima_expanding = round( (FP_low/(FP_low+TN_low)) ,4)\n",
    "tpr_low_02_arima_expanding = round( (TP_low/(TP_low+FN_low)) ,4) \n",
    "fnr_low_02_arima_expanding = round( (FN_low/(TP_low+FN_low)) ,4)\n",
    "tnr_low_02_arima_expanding = round( (TN_low/(FP_low+TN_low)) ,4)\n",
    "acc_low_02_arima_expanding = round( ((TP_low+TN_low)/(TP_low+FN_low+FP_low+TN_low)) ,4)\n",
    "f1s_low_02_arima_expanding = round( (TP_low/(TP_low+0.5*(FP_low+FN_low))) ,4)\n",
    "\n",
    "print('fpr_high_02_arima_expanding:',fpr_high_02_arima_expanding)\n",
    "print('tpr_high_02_arima_expanding:',tpr_high_02_arima_expanding)\n",
    "print('fnr_high_02_arima_expanding:',fnr_high_02_arima_expanding)\n",
    "print('tnr_high_02_arima_expanding:',tnr_high_02_arima_expanding)\n",
    "print('acc_high_02_arima_expanding:',acc_high_02_arima_expanding)\n",
    "print('f1s_high_02_arima_expanding:',f1s_high_02_arima_expanding)\n",
    "print()\n",
    "print('fpr_low_02_arima_expanding:',fpr_low_02_arima_expanding)\n",
    "print('tpr_low_02_arima_expanding:',tpr_low_02_arima_expanding)\n",
    "print('fnr_low_02_arima_expanding:',fnr_low_02_arima_expanding)\n",
    "print('tnr_low_02_arima_expanding:',tnr_low_02_arima_expanding)\n",
    "print('acc_low_02_arima_expanding:',acc_low_02_arima_expanding)\n",
    "print('f1s_low_02_arima_expanding:',f1s_low_02_arima_expanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - ARIMAX - Steady TRAIN Size 12\n",
    "\n",
    "# Create confusion matrix per chunk by aggregating so that TP, FN, FP, and TN of the several chunk iterations are summed up\n",
    "confusion_matrix_per_chunk_03_arimax_steady_12_df = confusion_matrices_per_chunk_03_arimax_steady_12_df.groupby(['CHUNK_ID_FILLED_TH', 'ACCURACY_TYPE'])[['TP', 'FN', 'FP', 'TN']].agg('sum')\n",
    "confusion_matrix_per_chunk_03_arimax_steady_12_df = confusion_matrix_per_chunk_03_arimax_steady_12_df.reset_index()\n",
    "\n",
    "# Create confusion matrix by aggregating so that TP, FN, FP, and TN of all chunks are summed up\n",
    "confusion_matrix_03_arimax_steady_12_df = confusion_matrix_per_chunk_03_arimax_steady_12_df.groupby(['ACCURACY_TYPE'])[['TP', 'FN', 'FP', 'TN']].agg('sum')\n",
    "\n",
    "display(confusion_matrix_03_arimax_steady_12_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = confusion_matrix_03_arimax_steady_12_df\n",
    "TP_high = df.iloc[0,0]\n",
    "FN_high = df.iloc[0,1]\n",
    "FP_high = df.iloc[0,2]\n",
    "TN_high = df.iloc[0,3]\n",
    "TP_low = df.iloc[1,0]\n",
    "FN_low = df.iloc[1,1]\n",
    "FP_low = df.iloc[1,2]\n",
    "TN_low = df.iloc[1,3]\n",
    "\n",
    "fpr_high_03_arimax_steady_12 = round( (FP_high/(FP_high+TN_high)) ,4)\n",
    "tpr_high_03_arimax_steady_12 = round( (TP_high/(TP_high+FN_high)) ,4) \n",
    "fnr_high_03_arimax_steady_12 = round( (FN_high/(TP_high+FN_high)) ,4)\n",
    "tnr_high_03_arimax_steady_12 = round( (TN_high/(FP_high+TN_high)) ,4)\n",
    "acc_high_03_arimax_steady_12 = round( ((TP_high+TN_high)/(TP_high+FN_high+FP_high+TN_high)) ,4)\n",
    "f1s_high_03_arimax_steady_12 = round( (TP_high/(TP_high+0.5*(FP_high+FN_high))) ,4)\n",
    "\n",
    "fpr_low_03_arimax_steady_12 = round( (FP_low/(FP_low+TN_low)) ,4)\n",
    "tpr_low_03_arimax_steady_12 = round( (TP_low/(TP_low+FN_low)) ,4) \n",
    "fnr_low_03_arimax_steady_12 = round( (FN_low/(TP_low+FN_low)) ,4)\n",
    "tnr_low_03_arimax_steady_12 = round( (TN_low/(FP_low+TN_low)) ,4)\n",
    "acc_low_03_arimax_steady_12 = round( ((TP_low+TN_low)/(TP_low+FN_low+FP_low+TN_low)) ,4)\n",
    "f1s_low_03_arimax_steady_12 = round( (TP_low/(TP_low+0.5*(FP_low+FN_low))) ,4)\n",
    "\n",
    "print('fpr_high_03_arimax_steady_12:',fpr_high_03_arimax_steady_12)\n",
    "print('tpr_high_03_arimax_steady_12:',tpr_high_03_arimax_steady_12)\n",
    "print('fnr_high_03_arimax_steady_12:',fnr_high_03_arimax_steady_12)\n",
    "print('tnr_high_03_arimax_steady_12:',tnr_high_03_arimax_steady_12)\n",
    "print('acc_high_03_arimax_steady_12:',acc_high_03_arimax_steady_12)\n",
    "print('f1s_high_03_arimax_steady_12:',f1s_high_03_arimax_steady_12)\n",
    "print()\n",
    "print('fpr_low_03_arimax_steady_12:',fpr_low_03_arimax_steady_12)\n",
    "print('tpr_low_03_arimax_steady_12:',tpr_low_03_arimax_steady_12)\n",
    "print('fnr_low_03_arimax_steady_12:',fnr_low_03_arimax_steady_12)\n",
    "print('tnr_low_03_arimax_steady_12:',tnr_low_03_arimax_steady_12)\n",
    "print('acc_low_03_arimax_steady_12:',acc_low_03_arimax_steady_12)\n",
    "print('f1s_low_03_arimax_steady_12:',f1s_low_03_arimax_steady_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 - ARIMAX - Expanding TRAIN Size\n",
    "\n",
    "# Create confusion matrix per chunk by aggregating so that TP, FN, FP, and TN of the several chunk iterations are summed up\n",
    "\n",
    "# Create confusion matrix by aggregating so that TP, FN, FP, and TN of all chunks are summed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'ID': [\n",
    "        '02', '02',\n",
    "        '03', '03'\n",
    "        ],\n",
    "    'PARAMETER': [\n",
    "        'HR', 'HR',\n",
    "        'HR', 'HR'\n",
    "        ],\n",
    "    'MODEL': [\n",
    "        'ARIMA', 'ARIMA',\n",
    "        'ARIMAX', 'ARIMAX'\n",
    "        ],\n",
    "    'RESAMPLING': [\n",
    "        'Median', 'Median',\n",
    "        'Max', 'Min'\n",
    "        ],\n",
    "    'EXOGENOUS': [\n",
    "        '', '',\n",
    "        'Median', 'Median'\n",
    "        ],\n",
    "    'TRAINSIZE': [\n",
    "        'Expanding', 'Expanding',\n",
    "        'Steady 12h', 'Steady 12h'\n",
    "        ],\n",
    "    'ALARMTYPE': [\n",
    "        'High', 'Low',\n",
    "        'High', 'Low'\n",
    "        ],\n",
    "    'FPR': [\n",
    "        fpr_high_02_arima_expanding, fpr_low_02_arima_expanding,\n",
    "        fpr_high_03_arimax_steady_12, fpr_low_03_arimax_steady_12\n",
    "        ],\n",
    "    'TPR': [\n",
    "        tpr_high_02_arima_expanding, tpr_low_02_arima_expanding,\n",
    "        tpr_high_03_arimax_steady_12, tpr_low_03_arimax_steady_12\n",
    "        ],\n",
    "    'FNR': [\n",
    "        fnr_high_02_arima_expanding, fnr_low_02_arima_expanding,\n",
    "        fnr_high_03_arimax_steady_12, fnr_low_03_arimax_steady_12\n",
    "        ],\n",
    "    'TNR': [\n",
    "        tnr_high_02_arima_expanding, tnr_low_02_arima_expanding,\n",
    "        tnr_high_03_arimax_steady_12, tnr_low_03_arimax_steady_12\n",
    "        ],\n",
    "    'ACC': [\n",
    "        acc_high_02_arima_expanding, acc_low_02_arima_expanding,\n",
    "        acc_high_03_arimax_steady_12, acc_low_03_arimax_steady_12\n",
    "        ],\n",
    "    'F1S': [\n",
    "        f1s_high_02_arima_expanding, f1s_low_02_arima_expanding,\n",
    "        f1s_high_03_arimax_steady_12, f1s_low_03_arimax_steady_12\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(comparison_df)"
   ]
  },
  {
   "source": [
    "## Comparison for selected Chunk"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_chunkid = '200347.0_220045.0_2116-06-05 15:02:00'\n",
    "selected_iteration = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - ARIMA - Expanding TRAIN Size\n",
    "# Confusion matrix for selected chunk\n",
    "confusion_matrix_per_chunk_02_arima_expanding_df[\n",
    "    confusion_matrix_per_chunk_02_arima_expanding_df.CHUNK_ID_FILLED_TH == selected_chunkid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 - ARIMA - Expanding TRAIN Size\n",
    "# Confusion matrix for selected iteration for selected chunk\n",
    "confusion_matrices_per_chunk_02_arima_expanding_df[\n",
    "    (confusion_matrices_per_chunk_02_arima_expanding_df.CHUNK_ID_FILLED_TH == selected_chunkid) &\n",
    "    (confusion_matrices_per_chunk_02_arima_expanding_df.ITERATION == selected_iteration)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - ARIMAX - Steady TRAIN Size 12\n",
    "# Confusion matrix for selected chunk\n",
    "confusion_matrix_per_chunk_03_arimax_steady_12_df[\n",
    "    confusion_matrix_per_chunk_03_arimax_steady_12_df.CHUNK_ID_FILLED_TH == selected_chunkid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03 - ARIMAX - Steady TRAIN Size 12\n",
    "# Confusion matrix for selected iteration for selected chunk\n",
    "confusion_matrices_per_chunk_03_arimax_steady_12_df[\n",
    "    (confusion_matrices_per_chunk_03_arimax_steady_12_df.CHUNK_ID_FILLED_TH == selected_chunkid) &\n",
    "    (confusion_matrices_per_chunk_03_arimax_steady_12_df.ITERATION == selected_iteration)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
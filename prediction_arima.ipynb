{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd0e2bfb1b1dd0bcdebdb315279aa118b1f834444d4ba3ba6d660e9f6ce7703f6a2",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "e2bfb1b1dd0bcdebdb315279aa118b1f834444d4ba3ba6d660e9f6ce7703f6a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Read chartevents_subset from parquet file to pandas data frame\n",
    "chartevents_subset = pd.read_parquet('../data/chartevents_clean_values_and_thresholds_with_chunkid_65_resampled.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETER = 220045\n",
    "CHUNKS = ['296490.0_220045.0_2192-09-26 23:51:00','260223.0_220045.0_2156-07-22 06:49:00']\n",
    "\n",
    "TRAIN = 60 # 60 * 5 min = 5 hours of training\n",
    "TEST = 12 # 12 * 5 min = 1 hour of testing\n",
    "STEP = 6 # move 6 * 5 min = 0.5 hours per step"
   ]
  },
  {
   "source": [
    "# subset data based on PARAMETER & CHUNKS\n",
    "arima_data = chartevents_subset.loc[(chartevents_subset[\"ITEMID\"] == PARAMETER) & (chartevents_subset.CHUNK_ID_FILLED_TH.isin(CHUNKS)) ,['CHUNK_ID_FILLED_TH','CHARTTIME','ITEMID','VALUENUM_CLEAN']]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = arima_data.CHUNK_ID_FILLED_TH.value_counts()\n",
    "relevant_chunks = all_chunks[all_chunks >= (TRAIN + TEST)].index\n",
    "arima_data = arima_data.loc[arima_data.CHUNK_ID_FILLED_TH.isin(relevant_chunks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Column that holds difference to first measurement\n",
    "import numpy as np\n",
    "arima_data['HOURS_SINCE_FIRST'] = arima_data.groupby('CHUNK_ID_FILLED_TH')['CHARTTIME'].transform(lambda x: (x - x.min())/np.timedelta64(1,'h'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one row for each chunk; each column is a 'HOURS_SINCE_FIRST' value \n",
    "# index     | 1 | 2 | 3...\n",
    "# firstChunk|89 | 93| 102...\n",
    "#secondChunk| 77| 81|90...\n",
    "measurements = []\n",
    "\n",
    "\n",
    "for chunk in relevant_chunks:\n",
    "\n",
    "    chunk_data = arima_data[arima_data.CHUNK_ID_FILLED_TH == chunk].copy()\n",
    "    chunk_data.set_index('HOURS_SINCE_FIRST', inplace=True)\n",
    "    chunk_data.sort_index(inplace=True)    \n",
    "    measurements.append(chunk_data['VALUENUM_CLEAN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple test & train sets for each chunk\n",
    "chunk_with_test_train = pd.DataFrame(columns=[\"SUB_CHUNK_ID\", \"TRAIN_LIST\",\"TEST_LIST\"])\n",
    "\n",
    "#merged_test_train = dict()\n",
    "\n",
    "#single_test_train = dict()\n",
    "\n",
    "for i,measurement in enumerate(measurements):\n",
    "    #für jeden startpunkt eines neuen train/test-abschnittes diese chunks (von 0 bis (Gesamtlänge dieser Patientenmessreihe - (Train+Test)) gehe STEPS weiter )\n",
    "    for start in range(0, len(measurement) - (TRAIN + TEST), STEP):\n",
    "        sub_chunk_id = str(i)+str(start)\n",
    "        train_list = measurement[start : start+TRAIN]\n",
    "        test_list = measurement[start+TRAIN : start+TRAIN+TEST]\n",
    "        a_new_row= {\"SUB_CHUNK_ID\":sub_chunk_id,\"TRAIN_LIST\":train_list,\"TEST_LIST\":test_list}\n",
    "        a_new_row_series = pd.Series(a_new_row, name=sub_chunk_id)\n",
    "        chunk_with_test_train = chunk_with_test_train.append(a_new_row_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct arima\n",
    "from progressbar import progressbar\n",
    "import pmdarima as pm\n",
    "\n",
    "condition = []\n",
    "prediction = []\n",
    "all_sub_chunk_ids = chunk_with_test_train.SUB_CHUNK_ID.value_counts()\n",
    "\n",
    "for i, sub_chunk_id in enumerate(all_sub_chunk_ids):\n",
    "    arima = pm.auto_arima(chunk_with_test_train['TRAIN_LIST'][i])\n",
    "    forecast = arima.predict(TEST)\n",
    "\n",
    "    condition.append(min(chunk_with_test_train[\"TEST_LIST\"][i]) > 120)\n",
    "    prediction.append(min(forecast) > 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "\n",
    "for cond, pred in zip(condition, prediction):\n",
    "    if cond and pred:\n",
    "        tp += 1\n",
    "    if cond and not pred:\n",
    "        fn += 1\n",
    "    if not cond and pred:\n",
    "        fp += 1\n",
    "    if not cond and not pred:\n",
    "        tn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TP = {tp}\")\n",
    "print(f\"TN = {tn}\")\n",
    "print(f\"FP = {fp}\")\n",
    "print(f\"FN = {fn}\")\n",
    "print()\n",
    "print(f\"Sens = {tp/(tp+fn)} (recall)\")\n",
    "print(f\"Spec = {tn/(tn+fp)}\")\n",
    "print(f\"PPV  = {tp/(tp+fp)} (precision)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## ARIMA Forecasting for Single Time Series\n",
    "\n",
    "Perform ARIMA analysis and prediction for a single manually selected time series, i.e. a single chunk and thus a single parameter.\n",
    "\n",
    "The following is an updated and annotated version of the steps performed above. I did not want to overwrite these. In the next step, we should consolidate so that we only use one version."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Read chartevents_subset from parquet file to pandas data frame\n",
    "chartevents_subset = pd.read_parquet('../data/chartevents_clean_values_and_thresholds_with_chunkid_65_resampled.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETER = 220045\n",
    "CHUNKS = ['296490.0_220045.0_2192-09-26 23:51:00','260223.0_220045.0_2156-08-06 17:46:00'] # ['296490.0_220045.0_2192-09-26 23:51:00'] # Only a single chunk is selected\n",
    "\n",
    "# Sampling rate of 1 data point per hour \n",
    "TRAIN = 60 # 60 * 1 h = 60 hour training period\n",
    "TEST = 12 # 12 * 1 h = 12 hour testing period\n",
    "STEP = 6 # move 6 * 1 h = 6 hours per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data based on PARAMETER & CHUNKS\n",
    "arima_data = chartevents_subset[\n",
    "    (chartevents_subset[\"ITEMID\"] == PARAMETER) & \n",
    "    (chartevents_subset.CHUNK_ID_FILLED_TH.isin(CHUNKS))\n",
    "    ][['CHUNK_ID_FILLED_TH','CHARTTIME','ITEMID','VALUENUM_CLEAN']]\n",
    "display(arima_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for chunks that have sufficient values to be used for training and testing the model\n",
    "all_chunks_value_count = arima_data.CHUNK_ID_FILLED_TH.value_counts()\n",
    "chunkid_filter = all_chunks_value_count[all_chunks_value_count >= (TRAIN + TEST)].index\n",
    "arima_data = arima_data[arima_data.CHUNK_ID_FILLED_TH.isin(chunkid_filter)]\n",
    "display(arima_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new MINUTES_SINCE_FIRST_RECORD column containing the time difference that has passed since the first timestamp of the measurement series.\n",
    "import numpy as np\n",
    "arima_data['MINUTES_SINCE_FIRST_RECORD'] = arima_data.groupby('CHUNK_ID_FILLED_TH')['CHARTTIME'].transform(lambda x: (x - x.min())/np.timedelta64(1,'m'))\n",
    "# Alternative for hours instead of minutes\n",
    "# arima_data['HOURS_SINCE_FIRST_RECORD'] = arima_data.groupby('CHUNK_ID_FILLED_TH')['CHARTTIME'].transform(lambda x: (x - x.min())/np.timedelta64(1,'h'))\n",
    "display(arima_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data structure\n",
    "# Create a list containing one element for each chunk, which are of type pandas series.\n",
    "# Each of these series includes the measured values of the chunk with the MINUTES_SINCE_FIRST_RECORD as index.\n",
    "# The data structure is transposed, so to speak, so that the MINUTES_SINCE_FIRST_RECORD that were previously in rows now serve as 'columns' (not literally; they are in the index of the series).\n",
    "\n",
    "# MINUTES_SINCE_FIRST_RECORD  |     0 |    60 |   120 | ...\n",
    "# ----------------------------------------------------- ...\n",
    "# firstChunk                  |  95.0 |  90.5 |  91.0 | ...\n",
    "# secondChunk                 | 110.5 | 108.0 | 110.0 | ...\n",
    "# ...\n",
    "\n",
    "# Set up list that will contain the chunk value series transformed as described above.\n",
    "list_of_chunk_value_series = []\n",
    "\n",
    "for chunkid in chunkid_filter:\n",
    "\n",
    "    chunk_value_series = arima_data[arima_data.CHUNK_ID_FILLED_TH == chunkid].copy()\n",
    "    chunk_value_series.set_index('MINUTES_SINCE_FIRST_RECORD', inplace=True)\n",
    "    chunk_value_series.sort_index(inplace=True)    \n",
    "    list_of_chunk_value_series.append(chunk_value_series['VALUENUM_CLEAN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The step of creating multiple test & training sets per measurement series may be  skipped, as we currently see no use for it in the context of ARIMA.\n",
    "\n",
    "# Create multiple test & training sets per chunk value series\n",
    "chunk_value_series_with_test_and_train = pd.DataFrame(columns=[\"SUB_CHUNK_ID\", \"TRAIN_LIST\",\"TEST_LIST\"])\n",
    "\n",
    "for i, chunk_value_series in enumerate(list_of_chunk_value_series):\n",
    "\n",
    "    # For each starting point of a new train/test section of this chunk (from 0 to total length of this chunk value series - (TRAIN + TEST)) move STEPS \n",
    "    for start in range(0, len(chunk_value_series) - (TRAIN + TEST), STEP):\n",
    "\n",
    "        sub_chunk_id = str(i)+str(start)\n",
    "        train_list = chunk_value_series[start : start+TRAIN]\n",
    "        test_list = chunk_value_series[start+TRAIN : start+TRAIN+TEST]\n",
    "        a_new_row= {\"SUB_CHUNK_ID\":sub_chunk_id,\"TRAIN_LIST\":train_list,\"TEST_LIST\":test_list}\n",
    "        a_new_row_series = pd.Series(a_new_row, name=sub_chunk_id)\n",
    "        chunk_value_series_with_test_and_train = chunk_value_series_with_test_and_train.append(a_new_row_series)\n",
    "\n",
    "display(chunk_value_series_with_test_and_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct ARIMA for single times series\n",
    "\n",
    "# Used resources:\n",
    "# https://kanoki.org/2020/04/30/time-series-analysis-and-forecasting-with-arima-python/\n",
    "# https://medium.com/@josemarcialportilla/using-python-and-auto-arima-to-forecast-seasonal-time-series-90877adff03c\n",
    "\n",
    "# Select single times series\n",
    "data = pd.DataFrame(list_of_chunk_value_series[0])\n",
    "\n",
    "# Find the best fit ARIMA model for the univariate time series data using auto_arima\n",
    "\n",
    "# I assume that our value series are not seasonal (to be checked)\n",
    "# Therefore, I set seasonal=False and m=1\n",
    "# See also https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "auto_arima_model = auto_arima(data, start_p=1, start_q=1,\n",
    "                            max_p=3, max_q=3, m=1,\n",
    "                            start_P=0, seasonal=False,\n",
    "                            d=1, D=1, trace=True,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)\n",
    "\n",
    "print(auto_arima_model.aic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "split_position = round((len(data)*80)/100) # Position of the row which is positioned at 80% of the total number of rows\n",
    "train = data.iloc[:split_position]\n",
    "test = data.iloc[split_position:]\n",
    "\n",
    "print(\"Total data length:\",len(data))\n",
    "print(\"Train data length:\",len(train))\n",
    "print(\"Test data length:\",len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "auto_arima_model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = auto_arima_model.predict(len(test))\n",
    "# This returns an array of predictions:\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize set of predictions by creating a dataframe that contains forecast and then concatenating that with the original data.\n",
    "forecast = pd.DataFrame(forecast,index = test.index,columns=['VALUENUM_PREDICTION'])\n",
    "\n",
    "evaluation_data = pd.concat([test,forecast],axis=1)\n",
    "display(evaluation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick and dirty plot for testing purposes\n",
    "import seaborn as sns\n",
    "sns.set(rc={\"figure.figsize\":(15, 5)})\n",
    "sns.lineplot(\n",
    "    data=pd.melt(evaluation_data.reset_index(),'MINUTES_SINCE_FIRST_RECORD'), # Reshape data frame for seaborn\n",
    "    x=\"MINUTES_SINCE_FIRST_RECORD\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    marker=\"o\",\n",
    "    markersize = 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick and dirty plot with training period\n",
    "evaluation_data_with_train = pd.concat([train,test,forecast],axis=1)\n",
    "import seaborn as sns\n",
    "sns.set(rc={\"figure.figsize\":(15, 5)})\n",
    "sns.lineplot(\n",
    "    data=pd.melt(evaluation_data_with_train.reset_index(),'MINUTES_SINCE_FIRST_RECORD'), # Reshape data frame for seaborn\n",
    "    x=\"MINUTES_SINCE_FIRST_RECORD\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    marker=\"o\",\n",
    "    markersize = 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation: prediction is pretty crappy\n",
    "# No idea what the most likely reason is. I'll try it with ARIMA default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best fit ARIMA model for the univariate time series data using auto_arima\n",
    "from pmdarima.arima import auto_arima\n",
    "auto_arima_model = auto_arima(data)\n",
    "print(auto_arima_model.aic())\n",
    "\n",
    "# Train the Model\n",
    "auto_arima_model.fit(train)\n",
    "\n",
    "# Evaluate\n",
    "forecast = auto_arima_model.predict(len(test))\n",
    "forecast = pd.DataFrame(forecast,index = test.index,columns=['VALUENUM_PREDICTION'])\n",
    "evaluation_data = pd.concat([test,forecast],axis=1)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={\"figure.figsize\":(15, 5)})\n",
    "sns.lineplot(\n",
    "    data=pd.melt(evaluation_data.reset_index(),'MINUTES_SINCE_FIRST_RECORD'), # Reshape data frame for seaborn\n",
    "    x=\"MINUTES_SINCE_FIRST_RECORD\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    marker=\"o\",\n",
    "    markersize = 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation: prediction is still crappy"
   ]
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd0c24247fa39158f46a54dbb99bb8811b81cd84bf3c9aa6e8294d53a41a5837da9",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Read chartevents_subset from parquet file to pandas data frame\n",
    "chartevents_subset = pd.read_parquet('./data/chartevents_clean_values_and_thresholds_with_chunkid_65_resampled.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETER = 220045\n",
    "CHUNKS = ['296490.0_220045.0_2192-09-26 23:51:00','260223.0_220045.0_2156-07-22 06:49:00']\n",
    "\n",
    "TRAIN = 60 # 60 * 5 min = 5 hours of training\n",
    "TEST = 12 # 12 * 5 min = 1 hour of testing\n",
    "STEP = 6 # move 6 * 5 min = 0.5 hours per step"
   ]
  },
  {
   "source": [
    "# subset data based on PARAMETER & CHUNKS\n",
    "arima_data = chartevents_subset.loc[(chartevents_subset[\"ITEMID\"] == PARAMETER) & (chartevents_subset.CHUNK_ID_FILLED_TH.isin(CHUNKS)) ,['CHUNK_ID_FILLED_TH','CHARTTIME','ITEMID','VALUENUM_CLEAN']]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = arima_data.CHUNK_ID_FILLED_TH.value_counts()\n",
    "relevant_chunks = all_chunks[all_chunks >= (TRAIN + TEST)].index\n",
    "arima_data = arima_data.loc[arima_data.CHUNK_ID_FILLED_TH.isin(relevant_chunks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Column that holds difference to first measurement\n",
    "import numpy as np\n",
    "arima_data['HOURS_SINCE_FIRST'] = arima_data.groupby('CHUNK_ID_FILLED_TH')['CHARTTIME'].transform(lambda x: (x - x.min())/np.timedelta64(1,'h'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one row for each chunk; each column is a 'HOURS_SINCE_FIRST' value \n",
    "# index     | 1 | 2 | 3...\n",
    "# firstChunk|89 | 93| 102...\n",
    "#secondChunk| 77| 81|90...\n",
    "measurements = []\n",
    "\n",
    "\n",
    "for chunk in relevant_chunks:\n",
    "\n",
    "    chunk_data = arima_data[arima_data.CHUNK_ID_FILLED_TH == chunk].copy()\n",
    "    chunk_data.set_index('HOURS_SINCE_FIRST', inplace=True)\n",
    "    chunk_data.sort_index(inplace=True)    \n",
    "    measurements.append(chunk_data['VALUENUM_CLEAN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple test & train sets for each chunk\n",
    "chunk_with_test_train = pd.DataFrame(columns=[\"SUB_CHUNK_ID\", \"TRAIN_LIST\",\"TEST_LIST\"])\n",
    "\n",
    "#merged_test_train = dict()\n",
    "\n",
    "#single_test_train = dict()\n",
    "\n",
    "for i,measurement in enumerate(measurements):\n",
    "    #für jeden startpunkt eines neuen train/test-abschnittes diese chunks (von 0 bis (Gesamtlänge dieser Patientenmessreihe - (Train+Test)) gehe STEPS weiter )\n",
    "    for start in range(0, len(measurement) - (TRAIN + TEST), STEP):\n",
    "        sub_chunk_id = str(i)+str(start)\n",
    "        train_list = measurement[start : start+TRAIN]\n",
    "        test_list = measurement[start+TRAIN : start+TRAIN+TEST]\n",
    "        a_new_row= {\"SUB_CHUNK_ID\":sub_chunk_id,\"TRAIN_LIST\":train_list,\"TEST_LIST\":test_list}\n",
    "        a_new_row_series = pd.Series(a_new_row, name=sub_chunk_id)\n",
    "        chunk_with_test_train = chunk_with_test_train.append(a_new_row_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct arima\n",
    "from progressbar import progressbar\n",
    "import pmdarima as pm\n",
    "\n",
    "condition = []\n",
    "prediction = []\n",
    "all_sub_chunk_ids = chunk_with_test_train.SUB_CHUNK_ID.value_counts()\n",
    "\n",
    "for i, sub_chunk_id in enumerate(all_sub_chunk_ids):\n",
    "    arima = pm.auto_arima(chunk_with_test_train['TRAIN_LIST'][i])\n",
    "    forecast = arima.predict(TEST)\n",
    "\n",
    "    condition.append(min(chunk_with_test_train[\"TEST_LIST\"][i]) > 120)\n",
    "    prediction.append(min(forecast) > 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "\n",
    "for cond, pred in zip(condition, prediction):\n",
    "    if cond and pred:\n",
    "        tp += 1\n",
    "    if cond and not pred:\n",
    "        fn += 1\n",
    "    if not cond and pred:\n",
    "        fp += 1\n",
    "    if not cond and not pred:\n",
    "        tn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TP = {tp}\")\n",
    "print(f\"TN = {tn}\")\n",
    "print(f\"FP = {fp}\")\n",
    "print(f\"FN = {fn}\")\n",
    "print()\n",
    "print(f\"Sens = {tp/(tp+fn)} (recall)\")\n",
    "print(f\"Spec = {tn/(tn+fp)}\")\n",
    "print(f\"PPV  = {tp/(tp+fp)} (precision)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
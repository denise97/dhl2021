{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Apply Standard Score On all Series with Same Mean and Standard Deviation\n",
    "\n",
    "This pre-processing scales all train series and all prediction series with same mean and standard deviation using a manually written standard score-transformation. The [StandardScaler by scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) can perform the same calculations, but fails to process series of different lengths. The standard score-transformed series allow for the execution of the prediction scripts ending with \"s1\". These runs should be compared with the individual scaling of each series performed with the default Scaler of Darts (aka [MinMaxScaler by scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range)) that can be found in the prediction scripts ending with \"s2\". Data for all windows (train and prediction data for each needed resampling method) of all parameters are exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_standard_score(series, mean, std):\n",
    "    scaled_series = list()\n",
    "\n",
    "    for value in series:\n",
    "        scaled_series.append((value - mean) / std)\n",
    "\n",
    "    return scaled_series"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import MissingValuesFiller\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "\n",
    "\n",
    "n_chunks = 2000\n",
    "filler = MissingValuesFiller()\n",
    "means, stds = dict(), dict()\n",
    "\n",
    "for parameter in ['bp', 'hr', 'o2']:\n",
    "\n",
    "    resampled = pd.read_parquet(f'../../data/resampling/resample_output_{parameter}_first{n_chunks}.parquet',\n",
    "                                engine='pyarrow')\n",
    "\n",
    "    # Collect values of all series with minimal length\n",
    "    relevant_series_median, relevant_series_min, relevant_series_max = dict(), dict(), dict()\n",
    "\n",
    "    for chunk_id in pd.unique(resampled.CHUNK_ID_FILLED_TH):\n",
    "        current_series = resampled[resampled['CHUNK_ID_FILLED_TH'] == chunk_id]\n",
    "\n",
    "        if len(current_series) > 12:\n",
    "            relevant_series_median[chunk_id] = current_series['VITAL_PARAMTER_VALUE_MEDIAN_RESAMPLING'].tolist()\n",
    "            relevant_series_min[chunk_id] = current_series['VITAL_PARAMTER_VALUE_MIN_RESAMPLING'].tolist()\n",
    "            relevant_series_max[chunk_id] = current_series['VITAL_PARAMTER_VALUE_MAX_RESAMPLING'].tolist()\n",
    "\n",
    "    # Calculate number of chunks corresponding to 20% of chunks\n",
    "    relevant_chunk_ids = list(relevant_series_median.keys())\n",
    "    twenty_percent = int((20 * len(relevant_chunk_ids)) / 100)\n",
    "\n",
    "    # Iterate five times different 20% of the chunks (= 5 windows)\n",
    "    for window_idx in range(5):\n",
    "\n",
    "        # Extract 20% of series for prediction (and catch last window to avoid ignoring chunks)\n",
    "        if window_idx == 4:\n",
    "            pred_median = {chunk_id: relevant_series_median[chunk_id]\n",
    "                           for chunk_id in relevant_chunk_ids[twenty_percent * window_idx:]}\n",
    "            pred_min = {chunk_id: relevant_series_min[chunk_id]\n",
    "                        for chunk_id in relevant_chunk_ids[twenty_percent * window_idx:]}\n",
    "            pred_max = {chunk_id: relevant_series_max[chunk_id]\n",
    "                        for chunk_id in relevant_chunk_ids[twenty_percent * window_idx:]}\n",
    "        else:\n",
    "            pred_median = {chunk_id: relevant_series_median[chunk_id]\n",
    "                           for chunk_id in relevant_chunk_ids[\n",
    "                                           twenty_percent * window_idx:twenty_percent * (window_idx + 1)]}\n",
    "            pred_min = {chunk_id: relevant_series_min[chunk_id]\n",
    "                        for chunk_id in relevant_chunk_ids[\n",
    "                                        twenty_percent * window_idx:twenty_percent * (window_idx + 1)]}\n",
    "            pred_max = {chunk_id: relevant_series_max[chunk_id]\n",
    "                        for chunk_id in relevant_chunk_ids[\n",
    "                                        twenty_percent * window_idx:twenty_percent * (window_idx + 1)]}\n",
    "\n",
    "        # Extract 80% of series for training\n",
    "        train_median = {chunk_id: relevant_series_median[chunk_id] for chunk_id in relevant_chunk_ids\n",
    "                        if chunk_id not in list(pred_median.keys())}\n",
    "        train_min = {chunk_id: relevant_series_min[chunk_id] for chunk_id in relevant_chunk_ids\n",
    "                     if chunk_id not in list(pred_min.keys())}\n",
    "        train_max = {chunk_id: relevant_series_max[chunk_id] for chunk_id in relevant_chunk_ids\n",
    "                     if chunk_id not in list(pred_max.keys())}\n",
    "\n",
    "        # Collect all values to calculate overall mean and standard deviation\n",
    "        train_values = pd.DataFrame(list(itertools.chain.from_iterable(list(train_median.values()) +\n",
    "                                                                       list(train_min.values()) +\n",
    "                                                                       list(train_max.values()))))\n",
    "        pred_values = pd.DataFrame(list(itertools.chain.from_iterable(list(pred_median.values()) +\n",
    "                                                                      list(pred_min.values()) +\n",
    "                                                                      list(pred_max.values()))))\n",
    "\n",
    "        # Scale values and merge with timestamps\n",
    "        train_median_scaled, train_min_scaled, train_max_scaled = dict(), dict(), dict()\n",
    "        pred_median_scaled, pred_min_scaled, pred_max_scaled = dict(), dict(), dict()\n",
    "\n",
    "        for chunk_id in train_median.keys():\n",
    "            train_mean, train_std = train_values.mean(), train_values.std()\n",
    "            means[f'{parameter}_{window_idx}_train'] = train_mean\n",
    "            stds[f'{parameter}_{window_idx}_train'] = train_std\n",
    "\n",
    "            original_series = resampled[resampled['CHUNK_ID_FILLED_TH'] == chunk_id]\n",
    "            original_series['SCALED_MEDIAN'] = np.array(\n",
    "                apply_standard_score(train_median[chunk_id], train_mean, train_std))\n",
    "            original_series['SCALED_MIN'] = np.array(apply_standard_score(train_min[chunk_id], train_mean, train_std))\n",
    "            original_series['SCALED_MAX'] = np.array(apply_standard_score(train_max[chunk_id], train_mean, train_std))\n",
    "\n",
    "            train_median_scaled[chunk_id] = filler.transform(TimeSeries.from_dataframe(\n",
    "                df=original_series,\n",
    "                time_col='CHARTTIME',\n",
    "                value_cols=['SCALED_MEDIAN'],\n",
    "                freq='H'))\n",
    "\n",
    "            train_min_scaled[chunk_id] = filler.transform(TimeSeries.from_dataframe(\n",
    "                df=original_series,\n",
    "                time_col='CHARTTIME',\n",
    "                value_cols=['SCALED_MIN'],\n",
    "                freq='H'))\n",
    "\n",
    "            train_max_scaled[chunk_id] = filler.transform(TimeSeries.from_dataframe(\n",
    "                df=original_series,\n",
    "                time_col='CHARTTIME',\n",
    "                value_cols=['SCALED_MAX'],\n",
    "                freq='H'))\n",
    "\n",
    "        for chunk_id in pred_median.keys():\n",
    "            pred_mean, pred_std = pred_values.mean(), pred_values.std()\n",
    "            means[f'{parameter}_{window_idx}_pred'] = pred_mean\n",
    "            stds[f'{parameter}_{window_idx}_pred'] = pred_std\n",
    "\n",
    "            original_series = resampled[resampled['CHUNK_ID_FILLED_TH'] == chunk_id]\n",
    "            original_series['SCALED_MEDIAN'] = np.array(z_transform(pred_median[chunk_id], pred_mean, pred_std))\n",
    "            original_series['SCALED_MIN'] = np.array(z_transform(pred_min[chunk_id], pred_mean, pred_std))\n",
    "            original_series['SCALED_MAX'] = np.array(z_transform(pred_max[chunk_id], pred_mean, pred_std))\n",
    "\n",
    "            pred_median_scaled[chunk_id] = filler.transform(TimeSeries.from_dataframe(\n",
    "                df=original_series,\n",
    "                time_col='CHARTTIME',\n",
    "                value_cols=['SCALED_MEDIAN'],\n",
    "                freq='H'))\n",
    "\n",
    "            pred_min_scaled[chunk_id] = filler.transform(TimeSeries.from_dataframe(\n",
    "                df=original_series,\n",
    "                time_col='CHARTTIME',\n",
    "                value_cols=['SCALED_MIN'],\n",
    "                freq='H'))\n",
    "\n",
    "            pred_max_scaled[chunk_id] = filler.transform(TimeSeries.from_dataframe(\n",
    "                df=original_series,\n",
    "                time_col='CHARTTIME',\n",
    "                value_cols=['SCALED_MAX'],\n",
    "                freq='H'))\n",
    "\n",
    "        # Export dicts containing chunk ID and its scaled TimeSeries\n",
    "        train_median_scaled_f = open(f'../../data/z_scaled/{parameter}_{window_idx}_train_median.pickle', 'wb')\n",
    "        pickle.dump(train_median_scaled, train_median_scaled_f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        train_median_scaled_f.close()\n",
    "\n",
    "        train_min_scaled_f = open(f'../../data/z_scaled/{parameter}_{window_idx}_train_min.pickle', 'wb')\n",
    "        pickle.dump(train_min_scaled, train_min_scaled_f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        train_min_scaled_f.close()\n",
    "\n",
    "        train_max_scaled_f = open(f'../../data/z_scaled/{parameter}_{window_idx}_train_max.pickle', 'wb')\n",
    "        pickle.dump(train_max_scaled, train_max_scaled_f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        train_max_scaled_f.close()\n",
    "\n",
    "        pred_median_scaled_f = open(f'../../data/z_scaled/{parameter}_{window_idx}_pred_median.pickle', 'wb')\n",
    "        pickle.dump(pred_median_scaled, pred_median_scaled_f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pred_median_scaled_f.close()\n",
    "\n",
    "        pred_min_scaled_f = open(f'../../data/z_scaled/{parameter}_{window_idx}_pred_min.pickle', 'wb')\n",
    "        pickle.dump(pred_min_scaled, pred_min_scaled_f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pred_min_scaled_f.close()\n",
    "\n",
    "        pred_max_scaled_f = open(f'../../data/z_scaled/{parameter}_{window_idx}_pred_max.pickle', 'wb')\n",
    "        pickle.dump(pred_max_scaled, pred_max_scaled_f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pred_max_scaled_f.close()\n",
    "\n",
    "# Export dicts containing means and standard deviations\n",
    "means_f = open(f'../../data/z_scaled/means_z_scaling.pickle', 'wb')\n",
    "pickle.dump(means, means_f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "means_f.close()\n",
    "\n",
    "stds_f = open(f'../../data/z_scaled/stds_z_scaling.pickle', 'wb')\n",
    "pickle.dump(stds, stds_f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "stds_f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
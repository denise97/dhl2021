{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Miscellaneous Analyses for Prediction with RNNModel by Darts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Which chunk IDs were predicted?\n",
    "\n",
    "This extraction is performed to compare if the same chunks are considered in the ARIMA(X) and the RNNModel approach. It assumes that the prediction series have the following naming convention: `pred_series_{parameter}_{n_chunks}_window{window_nr}.pickle`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle5 as pickle\n",
    "\n",
    "# Define variables to adjust\n",
    "n_chunks = 2000\n",
    "style = 'all'\n",
    "\n",
    "path = f'../../data/chunk_ids/{n_chunks}_chunks/{style}'\n",
    "chunk_ids = defaultdict(list)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, file)) and file.startswith('pred_series'):\n",
    "\n",
    "        # Load current prediction series\n",
    "        current_pred_series_f = open(f'{path}/{file}', 'rb')\n",
    "        current_pred_series = pickle.load(current_pred_series_f)\n",
    "        current_pred_series_f.close()\n",
    "\n",
    "        # Extract substrings\n",
    "        parameter = file.split('_')[2]\n",
    "        n_chunks = file.split('_')[3]\n",
    "\n",
    "        # Add partial list of chunk IDs to dict\n",
    "        current_chunk_ids = list(current_pred_series.keys())\n",
    "        if f'{parameter}_{n_chunks}' not in chunk_ids:\n",
    "            chunk_ids[f'{parameter}_{n_chunks}'] = list()\n",
    "        chunk_ids[f'{parameter}_{n_chunks}'] = chunk_ids[f'{parameter}_{n_chunks}'] + current_chunk_ids\n",
    "\n",
    "# Combine partial lists of windows to final list and save it\n",
    "for key in chunk_ids.keys():\n",
    "    current_chunk_ids_f = open(f'{path}/chunk_ids_{key}.pickle', 'wb')\n",
    "\n",
    "    current_chunk_ids = chunk_ids[key]\n",
    "    print(f'{key.split(\"_\")[0]} with {key.split(\"_\")[1]} chunks: {len(current_chunk_ids)} chunks for prediction')\n",
    "\n",
    "    pickle.dump(current_chunk_ids, current_chunk_ids_f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    current_chunk_ids_f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check if combined chunk IDs match expected ones\n",
    "for parameter in ['HR', 'BP', 'O2']:\n",
    "    # Extract list with chunk IDs from prediction\n",
    "    current_chunk_ids_f = open(f'{path}/chunk_ids_{parameter}_{n_chunks}.pickle', 'rb')\n",
    "    current_chunk_ids_pred = pickle.load(current_chunk_ids_f)\n",
    "    current_chunk_ids_f.close()\n",
    "\n",
    "    # Extract list with expected chunk IDs\n",
    "    current_chunk_ids_original = list()\n",
    "    resampled_chunks = pd.read_parquet(f'../../data/resampling/resample_output_{parameter}_first{n_chunks}.parquet',\n",
    "                                       engine='pyarrow')\n",
    "\n",
    "    for chunk_id in pd.unique(resampled_chunks.CHUNK_ID_FILLED_TH):\n",
    "        current_series = resampled_chunks[resampled_chunks['CHUNK_ID_FILLED_TH'] == chunk_id]\n",
    "\n",
    "        if len(current_series) > 12:\n",
    "            current_chunk_ids_original.append(chunk_id)\n",
    "\n",
    "    # Inform if chunk IDs from prediction don't match expected ones\n",
    "    if set(current_chunk_ids_pred) != set(current_chunk_ids_original):\n",
    "        print(f'There are different chunk IDs than expected for {parameter} with {n_chunks} chunks')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Which chunks are affected by the ValueError?\n",
    "\n",
    "There were ValueErrors for the execution of the O2 runs with 1,000 chunks and for all runs with 15,000 chunks which were thrown in the confusion matrix generation and which led to predictions full of NaNs. Its origin lay in our resampling of the chunks, in which individual (very few) data points were missing and thus were filled in by Darts with NaN values by default. The following code cell only includes the final extraction of chunk IDs were values were missing.\n",
    "\n",
    "Note: It does not matter which resampling method is investigated as they all are dealing with the same chunk IDs. We have randomly chosen the MEDIAN method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "import pandas as pd\n",
    "\n",
    "for n_chunks in [1000, 2000, 15000]:\n",
    "    for parameter in ['hr', 'bp', 'o2']:\n",
    "        resampled_chunks = pd.read_parquet(f'../../data/resampling/resample_output_{parameter}_first{n_chunks}.parquet',\n",
    "                                           engine='pyarrow')\n",
    "\n",
    "        # Extract relevant (= minimal length 13) chunks\n",
    "        relevant_series = dict()\n",
    "\n",
    "        for chunk_id in pd.unique(resampled_chunks.CHUNK_ID_FILLED_TH):\n",
    "            current_series = resampled_chunks[resampled_chunks['CHUNK_ID_FILLED_TH'] == chunk_id]\n",
    "\n",
    "            if len(current_series) > 12:\n",
    "                relevant_series[chunk_id] = TimeSeries.from_dataframe(\n",
    "                    df=current_series,\n",
    "                    time_col='CHARTTIME',\n",
    "                    value_cols=['VITAL_PARAMTER_VALUE_MEDIAN_RESAMPLING'],\n",
    "                    freq='H')\n",
    "\n",
    "        # Look for chunks with NaN values (missing values are filled by Darts per default)\n",
    "        chunk_ids_with_nan = list()\n",
    "\n",
    "        for chunk_id in relevant_series.keys():\n",
    "            chunk_as_df = relevant_series[chunk_id].pd_dataframe()\n",
    "            chunk_as_df.reset_index(level=0, inplace=True)\n",
    "            chunk_as_df.columns = ['Time', 'Value']\n",
    "\n",
    "            if chunk_as_df['Value'].isnull().values.any():\n",
    "                chunk_ids_with_nan.append(chunk_id)\n",
    "\n",
    "        print(f'Chunk IDs with missing values for {parameter.upper()} with {n_chunks} chunks: \\n{chunk_ids_with_nan}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How long does an ICU stay last?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plotdata = pd.DataFrame()\n",
    "for parameter in ['hr', 'bp', 'o2']:\n",
    "    # Read cleaned, resampled and chunked CHARTEVENTS\n",
    "    resampled_chartsevents = pd.read_parquet(f'../../data/resampling/resample_output_{parameter}_first2000.parquet',\n",
    "                                             engine='pyarrow')\n",
    "\n",
    "    # Collect durations (in h) of chunks\n",
    "    chunk_durations = list()\n",
    "    for chunk_id in pd.unique(resampled_chartsevents.CHUNK_ID_FILLED_TH):\n",
    "        current_chunk = resampled_chartsevents[resampled_chartsevents['CHUNK_ID_FILLED_TH'] == chunk_id]\n",
    "\n",
    "        if len(current_chunk) > 12:\n",
    "            current_starttime = current_chunk['CHARTTIME'].min()\n",
    "            current_endtime = current_chunk['CHARTTIME'].max()\n",
    "            current_duration_in_s = (current_endtime - current_starttime).total_seconds()\n",
    "\n",
    "            # Convert duration to hours\n",
    "            chunk_durations.append(divmod(current_duration_in_s, 3600)[0])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    chunk_durations_param = pd.DataFrame({'PARAMETER': [parameter.upper()] * len(chunk_durations),\n",
    "                                          'DURATION_IN_H': chunk_durations})\n",
    "    plotdata = pd.concat([plotdata, chunk_durations_param], axis=0, ignore_index=True)\n",
    "\n",
    "# Visualize durations\n",
    "sns.set_style('whitegrid')\n",
    "sns.boxplot(data=plotdata,\n",
    "            x='DURATION_IN_H',\n",
    "            y='PARAMETER',\n",
    "            palette = sns.color_palette('colorblind'))\n",
    "plt.title('Chunk Durations')\n",
    "plt.xlabel('Duration (in hours)')\n",
    "plt.ylabel('Parameter')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How many and how often do ICU stays appear in ICU?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for i, parameter in enumerate(['hr', 'bp', 'o2']):\n",
    "    chunks = pd.read_parquet(f'../../data/resampling/resample_output_{parameter}_first2000.parquet',\n",
    "                             engine='pyarrow')\n",
    "\n",
    "    icustay_ids = list()\n",
    "    for chunk_id in pd.unique(chunks['CHUNK_ID_FILLED_TH']):\n",
    "        icustay_ids.append(chunk_id.split('_')[0])\n",
    "\n",
    "    # Show how many different ICU stays exist per parameter\n",
    "    print(parameter, len(set(icustay_ids)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from  matplotlib.ticker import FuncFormatter\n",
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "import seaborn as sns\n",
    "\n",
    "for i, parameter in enumerate(['hr', 'bp', 'o2']):\n",
    "    resampled_chartsevents = pd.read_parquet(f'../../data/resampling/resample_output_{parameter}_first2000.parquet',\n",
    "                                             engine='pyarrow')\n",
    "\n",
    "    # Collect non-unique ICU stay IDs\n",
    "    icustay_ids = list()\n",
    "    for chunk_id in pd.unique(resampled_chartsevents['CHUNK_ID_FILLED_TH']):\n",
    "        icustay_ids.append(chunk_id.split('_')[0])\n",
    "\n",
    "    print(f'{len(set(icustay_ids))} ICU stay IDs appear once in 2,000 {parameter.upper()} chunks')\n",
    "\n",
    "    unique_icustay_ids_f = open(f'../../data/icustay_ids/icustay_ids_{parameter.upper()}.pickle', 'wb')\n",
    "    pickle.dump(set(icustay_ids), unique_icustay_ids_f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    unique_icustay_ids_f.close()\n",
    "\n",
    "    # Count how often each ICU stay ID appear\n",
    "    icustay_id_counts = pd.DataFrame()\n",
    "    for icustay_id in set(icustay_ids):\n",
    "        icustay_id_counts = icustay_id_counts.append(\n",
    "            {'ICUSTAY_ID' : icustay_id, 'Count' : icustay_ids.count(icustay_id)},\n",
    "            ignore_index=True)\n",
    "\n",
    "    print(icustay_id_counts.Count.describe())\n",
    "\n",
    "    # Plot appearance count\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure(i)\n",
    "    ax = sns.countplot(data=icustay_id_counts,\n",
    "                       x='Count',\n",
    "                       color=sns.color_palette('colorblind')[0])\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: int(x)))\n",
    "    plt.title(f'Number of Appearances of ICU Stay IDs in 2,000 {parameter.upper()} Chunks')\n",
    "    plt.xlabel('Number of Appearances')\n",
    "    plt.ylabel('Count')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What are the patient characteristics?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "\n",
    "# Create plot data with patient information of chunks\n",
    "plotdata = pd.DataFrame(columns=['ICUSTAY_ID', 'PARAMETER', 'LEAVE_CHARTTIME', 'DATE_OF_LEAVE', 'SUBJECT_ID', 'GENDER',\n",
    "                                 'EXPIRE_FLAG', 'DOB', 'DOD', 'AGE'])\n",
    "\n",
    "# Add ICU stay IDs from our chunks\n",
    "for parameter in ['bp', 'hr', 'o2']:\n",
    "    # Read list with ICU stay IDs created above\n",
    "    icustay_ids_f = open(f'../../data/icustay_ids/icustay_ids_{parameter.upper()}.pickle', 'rb')\n",
    "    icustay_ids = pickle.load(icustay_ids_f)\n",
    "    icustay_ids_f.close()\n",
    "\n",
    "    plotdata = pd.concat([plotdata, pd.DataFrame(\n",
    "        {'PARAMETER' : [parameter.upper()] * len(icustay_ids),\n",
    "         'ICUSTAY_ID' : [int(float(i)) for i in icustay_ids]}\n",
    "    )], axis=0, ignore_index=True)\n",
    "\n",
    "# Add related subject IDs from CHARTEVENTS + chart times/ date at ICU leave\n",
    "chartevents = pd.read_parquet('../../data/chartevents_subset.parquet', engine='pyarrow')\n",
    "subject_ids, leave_charttimes = list(), list()\n",
    "\n",
    "for index, row in plotdata.iterrows():\n",
    "    subject_ids.append(chartevents[chartevents['ICUSTAY_ID'] == row['ICUSTAY_ID']]\n",
    "                       ['SUBJECT_ID'].tolist()[0]) # get single subject ID\n",
    "    leave_charttimes.append(chartevents[chartevents['ICUSTAY_ID'] == row['ICUSTAY_ID']].sort_values('CHARTTIME')\n",
    "                           ['CHARTTIME'].tolist()[-1]) # get last chart time\n",
    "plotdata['SUBJECT_ID'] = subject_ids\n",
    "plotdata['LEAVE_CHARTTIME'] = leave_charttimes\n",
    "plotdata['DATE_OF_LEAVE'] = pd.to_datetime(plotdata['LEAVE_CHARTTIME']).dt.date\n",
    "\n",
    "# Read patient data\n",
    "patients = pd.read_csv('../../data/mimic-iii-clinical-database-1.4/PATIENTS.csv',\n",
    "                       parse_dates=['DOB', 'DOD', 'DOD_HOSP', 'DOD_SSN'],\n",
    "                       dtype={\n",
    "                           'ROW_ID': 'float64', # int according to specification\n",
    "                           'SUBJECT_ID': 'float64', # int according to specification\n",
    "                           'GENDER': 'object', # varchar(5) according to specification\n",
    "                           'EXPIRE_FLAG': 'object' # varchar(5) according to specification\n",
    "                       })\n",
    "\n",
    "# Add patient columns (order is ensured by parameter and subjectID sorts)\n",
    "relevant_subject_ids = dict()\n",
    "genders, expire_flags, dobs, dods = list(), list(), list(), list()\n",
    "plotdata_cleaned = pd.DataFrame()\n",
    "\n",
    "for parameter in ['bp', 'hr', 'o2']:\n",
    "    plotdata_param = plotdata[plotdata['PARAMETER'] == parameter.upper()]\n",
    "\n",
    "    # Reduce patient data to plot data and vice versa\n",
    "    patients_param = patients[patients['SUBJECT_ID'].isin(plotdata_param['SUBJECT_ID'].tolist())]\n",
    "    plotdata_param = plotdata_param[plotdata_param['SUBJECT_ID'].isin(patients_param['SUBJECT_ID'].tolist())]\\\n",
    "        .drop_duplicates(subset='SUBJECT_ID', keep='last')\n",
    "    plotdata_cleaned = pd.concat([plotdata_cleaned, plotdata_param])\n",
    "\n",
    "    # Collect rows of cols per parameter\n",
    "    patients_param = patients_param.sort_values('SUBJECT_ID')\n",
    "    genders = genders + patients_param['GENDER'].tolist()\n",
    "    expire_flags = expire_flags + patients_param['EXPIRE_FLAG'].tolist()\n",
    "    dobs = dobs + patients_param['DOB'].tolist()\n",
    "    dods = dods + patients_param['DOD'].tolist()\n",
    "\n",
    "plotdata = plotdata_cleaned\n",
    "\n",
    "# Sort plotdata according to list filling above\n",
    "plotdata = plotdata.sort_values(['PARAMETER', 'SUBJECT_ID'])\n",
    "\n",
    "plotdata['GENDER'] = genders\n",
    "plotdata['EXPIRE_FLAG'] = expire_flags\n",
    "plotdata['DOB'] = dobs\n",
    "plotdata['DOD'] = dods\n",
    "\n",
    "# Add age column (either age at death or age at ICU leave)\n",
    "def calc_age(death_date, birth_date):\n",
    "    diff = datetime.datetime.strptime(death_date, '%Y-%m-%d') - datetime.datetime.strptime(birth_date, '%Y-%m-%d')\n",
    "    return int(float(diff.days) / 364.0)\n",
    "\n",
    "dead_patients = plotdata[plotdata['EXPIRE_FLAG'] == '1']\n",
    "dead_patients['AGE'] = [calc_age(death_date, birth_date) for death_date, birth_date in\n",
    "                        zip(dead_patients['DOD'].astype(str), dead_patients['DOB'].astype(str))]\n",
    "\n",
    "# Note: If the patient was alive at least 90 days post hospital discharge, DOD is null\n",
    "# Source: https://mit-lcp.github.io/mimic-schema-spy/tables/patients.html\n",
    "living_patients = plotdata[plotdata['EXPIRE_FLAG'] == '0']\n",
    "living_patients['AGE'] = [calc_age(death_date, birth_date) for death_date, birth_date in\n",
    "                          zip((living_patients['DATE_OF_LEAVE'] + datetime.timedelta(days=90)).astype(str),\n",
    "                              living_patients['DOB'].astype(str))]\n",
    "\n",
    "plotdata = pd.concat([dead_patients, living_patients], axis=0, ignore_index=True)\n",
    "\n",
    "# Remove absurd ages (e.g. created because of very early date of birth)\n",
    "plotdata = plotdata[plotdata['AGE'] < 150]\n",
    "\n",
    "plotdata['PARAMETER_LABEL'] = plotdata['PARAMETER']\\\n",
    "    .replace(['HR', 'BP', 'O2'], ['$HR$', '$NBPs$', '$S_pO_2$'], regex=True)\n",
    "\n",
    "plotdata.to_parquet(f'../../data/patient_info_of_2000chunks.parquet', engine='pyarrow')\n",
    "print(plotdata.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Visualize gender + dead/ alive distribution\n",
    "for col_name in ['Gender', 'Expire_Flag']:\n",
    "    plt.figure(figsize=(6, 4), dpi=72)\n",
    "    sns.countplot(\n",
    "        data=plotdata,\n",
    "        x=col_name.upper(),\n",
    "        hue='PARAMETER_LABEL',\n",
    "        palette=sns.color_palette('colorblind'))\n",
    "    plt.xlabel(col_name.replace('_', ' '))\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Parameter:', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.savefig(f'../../plots/patient_analysis/chunk_patients_{col_name.lower()}.pdf', dpi=300, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize ages (at ICU leave) of living patients\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=3,\n",
    "    ncols=1,\n",
    "    figsize=(6, 14),\n",
    "    sharex=True,\n",
    "    dpi=72\n",
    "    )\n",
    "#fig.suptitle('Ages of Patients (black bars for dead patients)', fontweight='bold', fontsize=22, y=1)\n",
    "\n",
    "for i, parameter in enumerate(['hr', 'bp', 'o2']):\n",
    "    g1 = sns.histplot(ax=ax[i],\n",
    "                      data=plotdata, # blue are all patients\n",
    "                      x='AGE',\n",
    "                      kde=True,\n",
    "                      palette=sns.color_palette('colorblind'),\n",
    "                      bins=50)\n",
    "    g2 = sns.histplot(ax=ax[i],\n",
    "                      data=plotdata[plotdata['EXPIRE_FLAG'] == '0'], # black are dead patients\n",
    "                      x='AGE',\n",
    "                      kde=True,\n",
    "                      color='black',\n",
    "                      bins=50)\n",
    "    ax[i].set_title(plotdata[plotdata['PARAMETER'] == parameter.upper()].iloc[0]['PARAMETER_LABEL'], fontweight='bold', fontsize=14)\n",
    "\n",
    "    if i != 2:\n",
    "        g1.set(xlabel=None)\n",
    "        g2.set(xlabel=None)\n",
    "    else:\n",
    "        plt.xlabel('Age')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(f'../../plots/patient_analysis/chunk_patients_ages.pdf', dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Are the chunks non-stationary?\n",
    "\n",
    "It is non-stationary if the properties of the time-series do depend on the time at which the series is observed (aka if they have a trend or seasonality). This can be checked by splitting the chunk data into partitions, and compare the means and variances of the partitions. If the differences are statistically significant, the time-series is likely non-stationary.\n",
    "\n",
    "**Source:** Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). \"Time series analysis: Forecasting and control (5th ed)\". Hoboken, New Jersey: John Wiley & Sons.\n",
    "\n",
    "**Result:** As assumed, the chunks are rather stationary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for parameter in ['hr', 'bp', 'o2']:\n",
    "    print(f'\\nPARAMETER: {parameter.upper()}')\n",
    "    chunks = pd.read_parquet(f'../../data/resampling/resample_output_{parameter}_first2000.parquet', engine='pyarrow')\n",
    "\n",
    "    n_chunks = len(pd.unique(chunks['CHUNK_ID_FILLED_TH']))\n",
    "    chunk_ids_first_partition = pd.unique(chunks['CHUNK_ID_FILLED_TH'])[:int(n_chunks/2)]\n",
    "    chunk_ids_second_partition = pd.unique(chunks['CHUNK_ID_FILLED_TH'])[int(n_chunks/2):]\n",
    "\n",
    "    first_partition = chunks[chunks['CHUNK_ID_FILLED_TH'].isin(chunk_ids_first_partition)]\n",
    "    second_partition = chunks[chunks['CHUNK_ID_FILLED_TH'].isin(chunk_ids_second_partition)]\n",
    "\n",
    "    print(f'Mean of First Partition: {first_partition[\"VITAL_PARAMTER_VALUE_MEDIAN_RESAMPLING\"].mean()}')\n",
    "    print(f'Mean of Second Partition: {second_partition[\"VITAL_PARAMTER_VALUE_MEDIAN_RESAMPLING\"].mean()}')\n",
    "\n",
    "    print(f'Variance of First Partition: {first_partition.var()[\"VITAL_PARAMTER_VALUE_MEDIAN_RESAMPLING\"]}')\n",
    "    print(f'Variance of Second Partition: {second_partition.var()[\"VITAL_PARAMTER_VALUE_MEDIAN_RESAMPLING\"]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Collection of Miscellaneous Scripts Needed for Prediction with RNNModels"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract Chunk IDs for Prediction with 20% of Chunks\n",
    "\n",
    "This extraction is needed for the comparison with the ARIMA(X) approach when only 20% of the chunks are predicted. It assumes that the prediction series have the following naming convention: `pred_series_{parameter}_{n_chunks}.pickle`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle5 as pickle\n",
    "\n",
    "path = '../../data/chunk_ids/20_percent'\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, file)) and file.startswith('pred_series'):\n",
    "        # Load current prediction series\n",
    "        current_pred_series_f = open(f'{path}/{file}', 'rb')\n",
    "        current_pred_series = pickle.load(current_pred_series_f)\n",
    "        current_pred_series_f.close()\n",
    "\n",
    "        # Extract substrings and current chunk ID list\n",
    "        parameter = file.split('_')[2]\n",
    "        n_chunks = file.split('_')[3].split('.')[0]\n",
    "        current_chunk_ids = list(current_pred_series.keys())\n",
    "\n",
    "        print(f'{parameter} with {n_chunks} chunks: {len(current_chunk_ids)} chunks for prediction')\n",
    "\n",
    "        # Save current chunk ID list\n",
    "        current_chunk_ids_f = open(f'{path}/chunk_ids_{parameter}_{n_chunks}.pickle', 'wb')\n",
    "        pickle.dump(current_chunk_ids, current_chunk_ids_f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        current_chunk_ids_f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract Chunk IDs for Prediction with All Chunks and Verify Generated Chunk IDs\n",
    "\n",
    "This extraction is performed to compare if the same chunks are considered in the ARIMA(X) and the RNNModel approach when all chunks are predicted (aka five times different 20% are predicted with the RNNModel approach)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle5 as pickle\n",
    "\n",
    "path = '../../data/chunk_ids/all'\n",
    "chunk_ids = defaultdict(list)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, file)) and file.startswith('pred_series'):\n",
    "\n",
    "        # Load current prediction series\n",
    "        current_pred_series_f = open(f'{path}/{file}', 'rb')\n",
    "        current_pred_series = pickle.load(current_pred_series_f)\n",
    "        current_pred_series_f.close()\n",
    "\n",
    "        # Extract substrings\n",
    "        parameter = file.split('_')[2]\n",
    "        n_chunks = file.split('_')[3]\n",
    "        window = file.split('_')[4].split('.')[0]\n",
    "\n",
    "        # Add partial list of chunk IDs to dict\n",
    "        current_chunk_ids = list(current_pred_series.keys())\n",
    "        if f'{parameter}_{n_chunks}' not in chunk_ids:\n",
    "            chunk_ids[f'{parameter}_{n_chunks}'] = list()\n",
    "        chunk_ids[f'{parameter}_{n_chunks}'] = chunk_ids[f'{parameter}_{n_chunks}'] + current_chunk_ids\n",
    "\n",
    "# Combine partial lists of windows to final list and save it\n",
    "for parameter in ['HR', 'BP', 'O2']:\n",
    "    for n_chunks in [1000]:\n",
    "        current_chunk_ids_f = open(f'{path}/chunk_ids_{parameter}_{n_chunks}.pickle', 'wb')\n",
    "        pickle.dump(chunk_ids[f'{parameter}_{n_chunks}'], current_chunk_ids_f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        current_chunk_ids_f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check if combined chunk IDs match expected ones\n",
    "for parameter in ['HR', 'BP', 'O2']:\n",
    "    for n_chunks in [1000]:\n",
    "\n",
    "        # Extract list with chunk IDs from prediction\n",
    "        current_chunk_ids_f = open(f'{path}/chunk_ids_{parameter}_{n_chunks}.pickle', 'rb')\n",
    "        current_chunk_ids_pred = pickle.load(current_chunk_ids_f)\n",
    "        current_chunk_ids_f.close()\n",
    "\n",
    "        # Extract list with expected chunk IDs\n",
    "        current_chunk_ids_original = list()\n",
    "        resampled_chunks = pd.read_parquet(f'../../data/resampling/resample_output_{parameter}_first{n_chunks}.parquet',\n",
    "                                           engine='pyarrow')\n",
    "\n",
    "        for chunk_id in pd.unique(resampled_chunks.CHUNK_ID_FILLED_TH):\n",
    "            current_series = resampled_chunks[resampled_chunks['CHUNK_ID_FILLED_TH'] == chunk_id]\n",
    "\n",
    "            if len(current_series) > 12:\n",
    "                current_chunk_ids_original.append(chunk_id)\n",
    "\n",
    "        # Inform if chunk IDs from prediction don't match expected ones\n",
    "        if set(current_chunk_ids_pred) != set(current_chunk_ids_original):\n",
    "            print(f'There are different chunk IDs than expected for {parameter} with {n_chunks} chunks')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyze ValueError\n",
    "\n",
    "There were ValueErrors for the execution of the O2 runs with 1,000 chunks and for all runs with 15,000 chunks which were thrown in the confusion matrix generation and which led to predictions full of NaNs. Its origin lay in our resampling of the chunks, in which individual (very few) data points were missing and thus were filled in by Darts with NaN values by default. The following code cell only includes the final extraction of chunk IDs were values were missing.\n",
    "\n",
    "Note: It does not matter which resampling method is investigated as they all are dealing with the same chunk IDs. We have randomly chosen the MEDIAN method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk IDs with missing values for HR with 1000 chunks: \n",
      "[] \n",
      "\n",
      "Chunk IDs with missing values for BP with 1000 chunks: \n",
      "[] \n",
      "\n",
      "Chunk IDs with missing values for O2 with 1000 chunks: \n",
      "['200238.0_220277.0_2117-04-22 21:31:00'] \n",
      "\n",
      "Chunk IDs with missing values for HR with 15000 chunks: \n",
      "['203781.0_220045.0_2195-07-29 08:00:00', '214944.0_220045.0_2115-10-28 15:16:00', '217172.0_220045.0_2121-06-25 17:39:00', '218982.0_220045.0_2162-10-16 07:01:00', '224573.0_220045.0_2108-12-25 19:04:00'] \n",
      "\n",
      "Chunk IDs with missing values for BP with 15000 chunks: \n",
      "['200944.0_220179.0_2110-08-28 23:59:00', '203781.0_220179.0_2195-07-30 19:14:00'] \n",
      "\n",
      "Chunk IDs with missing values for O2 with 15000 chunks: \n",
      "['200238.0_220277.0_2117-04-22 21:31:00', '201821.0_220277.0_2193-01-14 15:59:00', '203781.0_220277.0_2195-07-29 08:00:00', '212983.0_220277.0_2146-03-31 12:06:00', '213474.0_220277.0_2168-01-31 15:59:00'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from darts import TimeSeries\n",
    "import pandas as pd\n",
    "\n",
    "for n_chunks in [1000, 15000]:\n",
    "    for parameter in ['hr', 'bp', 'o2']:\n",
    "\n",
    "        resampled_chunks = pd.read_parquet(f'../../data/resampling/resample_output_{parameter}_first{n_chunks}.parquet',\n",
    "                                           engine='pyarrow')\n",
    "\n",
    "        # Extract relevant (= minimal length 13) chunks\n",
    "        relevant_series = dict()\n",
    "\n",
    "        for chunk_id in pd.unique(resampled_chunks.CHUNK_ID_FILLED_TH):\n",
    "            current_series = resampled_chunks[resampled_chunks['CHUNK_ID_FILLED_TH'] == chunk_id]\n",
    "\n",
    "            if len(current_series) > 12:\n",
    "                relevant_series[chunk_id] = TimeSeries.from_dataframe(\n",
    "                    df=current_series,\n",
    "                    time_col='CHARTTIME',\n",
    "                    value_cols=['VITAL_PARAMTER_VALUE_MEDIAN_RESAMPLING'],\n",
    "                    freq='H')\n",
    "\n",
    "        # Look for chunks with NaN values (missing values are filled by Darts per default)\n",
    "        chunk_ids_with_nan = list()\n",
    "\n",
    "        for chunk_id in relevant_series.keys():\n",
    "            chunk_as_df = relevant_series[chunk_id].pd_dataframe()\n",
    "            chunk_as_df.reset_index(level=0, inplace=True)\n",
    "            chunk_as_df.columns = ['Time', 'Value']\n",
    "\n",
    "            if chunk_as_df['Value'].isnull().values.any():\n",
    "                chunk_ids_with_nan.append(chunk_id)\n",
    "\n",
    "        print(f'Chunk IDs with missing values for {parameter.upper()} with {n_chunks} chunks: \\n{chunk_ids_with_nan}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
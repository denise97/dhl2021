{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis of Results Produced by `mimic_prediction_rnn/prediction_1000_chunks.py`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variables to Exchange"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_type = 'LSTM'\n",
    "parameter = 'hr'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract Relevant Chunk IDs and Its Predicted Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract predicted series\n",
    "to_pred_series_f = open(f'../../data/darts/1000_chunks/{model_type}/{parameter}/02_non-scaled_pred_series.pickle', 'rb')\n",
    "to_pred_series = pickle.load(to_pred_series_f)\n",
    "to_pred_series_f.close()\n",
    "\n",
    "print(f'#Chunks to predict: {len(list(to_pred_series.keys()))}')\n",
    "\n",
    "# Collect pickle file names with final prediction per chunk\n",
    "path_to_predicted_pickle_files = f'../../data/darts/1000_chunks/{model_type}/{parameter}'\n",
    "prediction_filenames = list()\n",
    "\n",
    "# Extract relevant pickle files\n",
    "for file in os.listdir(path_to_predicted_pickle_files):\n",
    "    if os.path.isfile(os.path.join(path_to_predicted_pickle_files, file)) and \\\n",
    "            file.startswith('05_non-scaled_prediction') and file.endswith('final.pickle'):\n",
    "        prediction_filenames.append(file)\n",
    "\n",
    "# Create dict {chunkID : prediction_timeseries}\n",
    "predicted_series = dict()\n",
    "\n",
    "for file in prediction_filenames:\n",
    "    current_pred_series_f = open(f'{path_to_predicted_pickle_files}/{file}', 'rb')\n",
    "    current_pred_series = pickle.load(current_pred_series_f)\n",
    "    current_pred_series_f.close()\n",
    "\n",
    "    # Extract chunk ID from filename\n",
    "    current_chunk = '_'.join(file[len('05_non-scaled_prediction_'):].split(\"_\", 3)[:3]).replace('%3A', ':')\n",
    "\n",
    "    predicted_series[current_chunk] = current_pred_series\n",
    "\n",
    "# Shortly check if extracted chunk ID are correct\n",
    "if set(to_pred_series.keys()) != set(predicted_series.keys()):\n",
    "    print('There is a mismatch between the expected and extracted chunk IDs !!!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add Original Alarm Triggering Booleans"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract original series\n",
    "original_resampled = pd.read_parquet(f'../../data/resampling/resample_output_{parameter}_first1000.parquet', engine='pyarrow')\n",
    "\n",
    "# TODO: resolve warning\n",
    "\n",
    "# Convert original series into dict for confusion matrix comparisons, aka {chunkID : info_columns}\n",
    "chunks = dict()\n",
    "\n",
    "for chunk_id in to_pred_series.keys():\n",
    "    # Filter for current chunk and sort by time\n",
    "    current_sorted_chunk = original_resampled[original_resampled['CHUNK_ID_FILLED_TH'] == chunk_id].sort_values('CHARTTIME')\n",
    "\n",
    "    # Remove 12 non-predicted values\n",
    "    current_sorted_chunk = current_sorted_chunk[-len(predicted_series[chunk_id]):].reset_index()\n",
    "\n",
    "    # Convert info into dict\n",
    "    chunks[chunk_id] = current_sorted_chunk[['CHARTTIME', 'VITAL_PARAMTER_VALUE_MEDIAN_RESAMPLING',\n",
    "                                             'THRESHOLD_VALUE_HIGH', 'THRESHOLD_VALUE_LOW']]\n",
    "\n",
    "    # Add boolean indicating if high alarms were triggered\n",
    "    chunks[chunk_id]['HIGH_ALARM_TRIGGERED'] = chunks[chunk_id].apply(\n",
    "        lambda row: row.VITAL_PARAMTER_VALUE_MEDIAN_RESAMPLING > row.THRESHOLD_VALUE_HIGH, axis=1)\n",
    "\n",
    "    # Add boolean indicating if low alarms were triggered\n",
    "    chunks[chunk_id]['LOW_ALARM_TRIGGERED'] = chunks[chunk_id].apply(\n",
    "        lambda row: row.VITAL_PARAMTER_VALUE_MEDIAN_RESAMPLING < row.THRESHOLD_VALUE_LOW, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add Predicted Values and Alarm Triggering Booleans"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: resolve warning\n",
    "\n",
    "for chunk_id in chunks.keys():\n",
    "    # Add predicted vital parameter value\n",
    "    chunks[chunk_id]['VITAL_PARAMTER_VALUE_PREDICTED'] = predicted_series[chunk_id].Value\n",
    "\n",
    "    # Add boolean indicating if predicted value exceeds high alarm\n",
    "    chunks[chunk_id]['HIGH_ALARM_TRIGGERED_PREDICTED'] = chunks[chunk_id].apply(\n",
    "        lambda row: row.VITAL_PARAMTER_VALUE_PREDICTED > row.THRESHOLD_VALUE_HIGH, axis=1)\n",
    "\n",
    "    # Add boolean indicating if predicted value falls below high alarm\n",
    "    chunks[chunk_id]['LOW_ALARM_TRIGGERED_PREDICTED'] = chunks[chunk_id].apply(\n",
    "        lambda row: row.VITAL_PARAMTER_VALUE_PREDICTED < row.THRESHOLD_VALUE_LOW, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fill Confusion Matrices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "confusion_matrix_high = pd.DataFrame(columns=['CHUNK_ID', 'TP', 'FN', 'FP', 'TN'])\n",
    "confusion_matrix_low = pd.DataFrame(columns=['CHUNK_ID', 'TP', 'FN', 'FP', 'TN'])\n",
    "\n",
    "for chunk_id in chunks.keys():\n",
    "\n",
    "    # Get indices where booleans are false/ true\n",
    "    high_triggered = set(chunks[chunk_id].index[chunks[chunk_id]['HIGH_ALARM_TRIGGERED']])\n",
    "    high_triggered_pred = set(chunks[chunk_id].index[chunks[chunk_id]['HIGH_ALARM_TRIGGERED_PREDICTED']])\n",
    "    high_not_triggered = set(chunks[chunk_id].index[~chunks[chunk_id]['HIGH_ALARM_TRIGGERED']])\n",
    "    high_not_triggered_pred = set(chunks[chunk_id].index[~chunks[chunk_id]['HIGH_ALARM_TRIGGERED_PREDICTED']])\n",
    "\n",
    "    # Fill confusion matrix for low threshold analysis\n",
    "    confusion_matrix_high = confusion_matrix_high.append(\n",
    "        {\n",
    "            'CHUNK_ID': chunk_id,\n",
    "            # Following 4 numbers look at how many indices are shared\n",
    "            'TP': len(high_triggered.intersection(high_triggered_pred)),\n",
    "            'FN': len(high_triggered.intersection(high_not_triggered_pred)),\n",
    "            'FP': len(high_not_triggered.intersection(high_triggered_pred)),\n",
    "            'TN': len(high_not_triggered.intersection(high_not_triggered_pred))\n",
    "        },\n",
    "        ignore_index=True)\n",
    "\n",
    "    # Add accuracy value (later to avoid storing TP etc. two times)\n",
    "    confusion_matrix_high['ACCURACY'] = confusion_matrix_high.apply(\n",
    "        lambda row: ((row.TP + row.TN) / (row.TP + row.FN + row.FP + row.TN)), axis=1)\n",
    "\n",
    "    # Get indices where booleans are false/ true\n",
    "    low_triggered = set(chunks[chunk_id].index[chunks[chunk_id]['LOW_ALARM_TRIGGERED']])\n",
    "    low_triggered_pred = set(chunks[chunk_id].index[chunks[chunk_id]['LOW_ALARM_TRIGGERED_PREDICTED']])\n",
    "    low_not_triggered = set(chunks[chunk_id].index[~chunks[chunk_id]['LOW_ALARM_TRIGGERED']])\n",
    "    low_not_triggered_pred = set(chunks[chunk_id].index[~chunks[chunk_id]['LOW_ALARM_TRIGGERED_PREDICTED']])\n",
    "\n",
    "    # Fill confusion matrix for low threshold analysis\n",
    "    confusion_matrix_low = confusion_matrix_low.append(\n",
    "        {\n",
    "            'CHUNK_ID': chunk_id,\n",
    "            # Following 4 numbers look at how many indices are shared\n",
    "            'TP': len(low_triggered.intersection(low_triggered_pred)),\n",
    "            'FN': len(low_triggered.intersection(low_not_triggered_pred)),\n",
    "            'FP': len(low_not_triggered.intersection(low_triggered_pred)),\n",
    "            'TN': len(low_not_triggered.intersection(low_not_triggered_pred))\n",
    "        },\n",
    "        ignore_index=True)\n",
    "\n",
    "    # Add accuracy value (later to avoid storing TP etc. two times)\n",
    "    confusion_matrix_low['ACCURACY'] = confusion_matrix_low.apply(\n",
    "        lambda row: ((row.TP + row.TN) / (row.TP + row.FN + row.FP + row.TN)), axis=1)\n",
    "\n",
    "#print(confusion_matrix_low[confusion_matrix_low['CHUNK_ID'] == '200033.0_220045.0_2198-08-07 19:53:00'])\n",
    "print(f'Confusion Matrix (HIGH): \\n{confusion_matrix_high}\\n')\n",
    "print(f'Confusion Matrix (LOW): \\n{confusion_matrix_low}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation Between Chunk Length and Chunk Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Maybe include chunk length into chunks dict (instead of creating 3 lists)\n",
    "\n",
    "# Fill lists for plotting\n",
    "chunk_lengths = list()\n",
    "chunk_accuracies_high = list()\n",
    "chunk_accuracies_low = list()\n",
    "\n",
    "for chunk_id in chunks:\n",
    "    chunk_lengths.append(len(chunks[chunk_id]))\n",
    "    chunk_accuracies_high.append(confusion_matrix_high[confusion_matrix_high['CHUNK_ID'] == chunk_id].ACCURACY.to_list()[0])\n",
    "    chunk_accuracies_low.append(confusion_matrix_low[confusion_matrix_low['CHUNK_ID'] == chunk_id].ACCURACY.to_list()[0])\n",
    "\n",
    "# Define background color, subplots and suptitle\n",
    "sns.set_style('whitegrid')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "fig.suptitle('Correlation of Chunk Length and Chunk Accuracy', fontsize=14)\n",
    "\n",
    "# Add left plot (high threshold analysis)\n",
    "ax1.plot(chunk_lengths, chunk_accuracies_high, 'o', color=sns.color_palette('colorblind')[0])\n",
    "ax1.set_title('Accuracy Regarding High Thresholds', fontsize=10)\n",
    "ax1.set_xlabel('Chunk Length', fontsize=8)\n",
    "ax1.set_ylabel('Chunk Accuracy', fontsize=8)\n",
    "ax1.set_ylim(bottom=0, top=1.1)\n",
    "\n",
    "# Add right plot (low threshold analysis)\n",
    "ax2.plot(chunk_lengths, chunk_accuracies_low, 'o', color=sns.color_palette('colorblind')[1])\n",
    "ax2.set_title('Accuracy Regarding Low Thresholds', fontsize=10)\n",
    "ax2.set_xlabel('Chunk Length', fontsize=8)\n",
    "ax2.set_ylabel('Chunk Accuracy', fontsize=8)\n",
    "ax2.set_ylim(bottom=0, top=1.1)\n",
    "\n",
    "# Improve layout and save figure\n",
    "fig.tight_layout()\n",
    "#fig.show()\n",
    "fig.savefig(f'../../plots/darts/1000_chunks/correlation_chunk_length_and_accuracy_{model_type}_{parameter}.png', dpi=1200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
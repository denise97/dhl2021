{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd0c24247fa39158f46a54dbb99bb8811b81cd84bf3c9aa6e8294d53a41a5837da9",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Perform Chunking\n",
    "Based on the visual analysis, we derived two possible chunking options:\n",
    "* Chunk after 60 min difference to previous timestamp\n",
    "* Chunk after 120 min difference to previous timestamp\n",
    "\n",
    "After the discussion with the teaching team, we decided to **chunk after 65 min difference to the previous timestamp** with the possibility to adapt that in future"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Add Timestamp of previous measurement and difference between timestamps to dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Read chartevents_subset from parquet file to pandas data frame\n",
    "chartevents_subset = pd.read_parquet('./data/chartevents_clean.parquet', engine='pyarrow')\n",
    "unique_icu_stays = pd.read_parquet('./data/unique_icustays_in_chartevents_subset.parquet', engine='pyarrow')\n",
    "#unique_icu_stays = chartevents_subset['ICUSTAY_ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select ICU_stay ids for analysis\n",
    "icustayid_filter = unique_icu_stays.ICUSTAY_ID\n",
    "\n",
    "# Filter by ICU_stay\n",
    "chunk_analysis_data = chartevents_subset[chartevents_subset.ICUSTAY_ID.isin(icustayid_filter)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling Rate Analysis is only being conducted on the values, not thresholds\n",
    "# Filter for item ids that refer to value\n",
    "itemids_for_values_filter = [220045, 220179, 220277]\n",
    "chunk_analysis_data = chunk_analysis_data[chunk_analysis_data.ITEMID.isin(itemids_for_values_filter)].copy()\n",
    "len(chunk_analysis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea: Keep chunk_analysis_data as is, only add a new column that holds the previous timestamp, the difference can then be performed outside the loop\n",
    "# Prerequisite: Sorted Data by ICUSTAY_ID,ITEMID,CHARTTIME\n",
    "chunk_analysis_data['CHARTTIME_PREV'] = chunk_analysis_data.groupby(['ICUSTAY_ID','ITEMID'])['CHARTTIME'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate difference between timestamps\n",
    "chunk_analysis_data['DIF_CHARTTIME_PREV'] = chunk_analysis_data['CHARTTIME']-chunk_analysis_data['CHARTTIME_PREV']\n",
    "chunk_analysis_data['DIF_CHARTTIME_PREV_S'] = chunk_analysis_data['DIF_CHARTTIME_PREV'].dt.total_seconds()\n",
    "chunk_analysis_data['DIF_CHARTTIME_PREV_MIN'] = divmod(chunk_analysis_data['DIF_CHARTTIME_PREV_S'], 60)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_analysis_data.head()"
   ]
  },
  {
   "source": [
    "## Apply Chunking Rule\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_dif = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce data to relevant columns to make validation easier\n",
    "#chunk_analysis_data = chunk_analysis_data[['ICUSTAY_ID','ITEMID','CHARTTIME','VALUENUM','VALUEUOM','CHARTTIME_PREV','DIF_CHARTTIME_PREV_MIN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all rows where dif to prev measurement is >chunking dif\n",
    "chunk_data = chunk_analysis_data[chunk_analysis_data[\"DIF_CHARTTIME_PREV_MIN\"] > chunking_dif]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a unique chunking ID to these rows\n",
    "chunk_data[\"CHUNK_ID\"] = chunk_data.ICUSTAY_ID.map(str) + \"_\" + chunk_data.ITEMID.map(str) + \"_\" + chunk_data.CHARTTIME.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check uniqueness - can only be violated if multiple measurements for that itemid/icustayid occured at the same charttime\n",
    "print(len(chunk_data[\"CHUNK_ID\"].value_counts()))\n",
    "print(len(chunk_data))\n",
    "# uiqueness for this data set is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep chunkid and index\n",
    "chunk_data_subset = chunk_data[\"CHUNK_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge back to all rows via index\n",
    "#no we have a data set that has a chunk_id at the beginning of each measurement that was conducted later than the chunking rule allows\n",
    "chunk_data_merged = pd.merge(chunk_analysis_data, chunk_data_subset,  how='left', left_index=True, right_index=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change sorting structure -  turn ITEMID and CHARTTIME around\n",
    "chunk_data_merged = chunk_data_merged.sort_values(by=['ICUSTAY_ID', 'ITEMID','CHARTTIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Chunk ID to first measurement of   ICUSTAY_ID/TEMID in case it does not already exist\n",
    "# Calculate min timestamp\n",
    "chunk_data_min = chunk_data_merged.groupby(['ICUSTAY_ID','ITEMID'])['CHARTTIME'].min()\n",
    "chunk_data_min_df = chunk_data_min.to_frame()\n",
    "chunk_data_min_df.reset_index(inplace=True)\n",
    "\n",
    "# for each first charttime (by ICUSTAYID/ITEEMID) create a chunk ID\n",
    "chunk_data_min_df[\"CHUNK_ID_MIN\"] = chunk_data_min_df.ICUSTAY_ID.map(str) + \"_\" + chunk_data_min_df.ITEMID.map(str) + \"_\" + chunk_data_min_df.CHARTTIME.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge that back so we have a chunk id for each first Measurement (by ICUSTAYID/TEMID)\n",
    "chunk_data_merged_2 = pd.merge(chunk_data_merged, chunk_data_min_df,  how='left', on=['ICUSTAY_ID','ITEMID','CHARTTIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# if chunkIdMin not Nan,write chunk_id_min in chunk_id\n",
    "# #no we have a data set that has a chunk_id at the beginning of each measurement that was conducted later than the chunking rule allows as well as an initial chunk id\n",
    "chunk_data_merged_2['CHUNK_ID'] = np.where(chunk_data_merged_2['CHUNK_ID_MIN'].notnull(), chunk_data_merged_2['CHUNK_ID_MIN'], chunk_data_merged_2['CHUNK_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_data_merged_2 = chunk_data_merged_2.drop(columns='CHUNK_ID_MIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill all cells with previous chunk id, until new chunk idea occurs\n",
    "#pre-requisite: data is sorted by ICUSTAY_ID & ITEMID\n",
    "chunk_data_merged_2['CHUNK_ID_FILLED'] = chunk_data_merged_2['CHUNK_ID'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that are obsolete now - only kept for validation purpose in previous steps\n",
    "#chunk_data_merged_2 = chunk_data_merged_2.drop(columns='CHUNK_ID')\n",
    "#chunk_data_merged_2.rename(columns={\"CHUNK_ID_FILLED\":\"CHUNK_ID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_data_merged_2 = chunk_data_merged_2.drop(columns='DIF_CHARTTIME_PREV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_data_merged_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as parquet file\n",
    "pd.DataFrame(chunk_data_merged_2).to_parquet('./data/chartevents_clean_values_with_chunkid_' + str(chunking_dif) + '.parquet', engine='pyarrow')"
   ]
  },
  {
   "source": [
    "## Bring Threshold Values back in"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Read chartevents_subset from parquet file to pandas data frame\n",
    "chartevents_subset = pd.read_parquet('./data/chartevents_clean.parquet', engine='pyarrow')\n",
    "chunk_data = pd.read_parquet('./data/chartevents_clean_values_with_chunkid_65.parquet', engine='pyarrow')\n",
    "unique_icu_stays = pd.read_parquet('./data/unique_icustays_in_chartevents_subset.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select ICU_stay ids for analysis\n",
    "icustayid_filter = unique_icu_stays.ICUSTAY_ID\n",
    "\n",
    "# Filter by ICU_stay\n",
    "chartevents_subset = chartevents_subset[chartevents_subset.ICUSTAY_ID.isin(icustayid_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match columns for later union\n",
    "import numpy as np\n",
    "chunk_data = chunk_data.drop(columns=['CHARTTIME_PREV','DIF_CHARTTIME_PREV_S','DIF_CHARTTIME_PREV_MIN','CHUNK_ID'])\n",
    "chartevents_subset.insert(loc=len(chartevents_subset.columns), column='CHUNK_ID_FILLED', value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_subset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#respective ITEMIDs\n",
    "itemids_for_thresholds_HR = [220046, 220047]\n",
    "itemids_for_thresholds_NBP = [223751, 223752]\n",
    "itemids_for_thresholds_O2 = [223769, 223770]\n",
    "itemids_for_value_HR = [220045]\n",
    "itemids_for_value_NBP = [220179]\n",
    "itemids_for_value_O2 = [220277]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold data\n",
    "threshold_data_HR = chartevents_subset[chartevents_subset.ITEMID.isin(itemids_for_thresholds_HR)].copy()\n",
    "threshold_data_NBP = chartevents_subset[chartevents_subset.ITEMID.isin(itemids_for_thresholds_NBP)].copy()\n",
    "threshold_data_O2 = chartevents_subset[chartevents_subset.ITEMID.isin(itemids_for_thresholds_O2)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value data\n",
    "value_chunk_data_HR = chunk_data[chunk_data.ITEMID.isin(itemids_for_value_HR)].copy()\n",
    "value_chunk_data_NBP = chunk_data[chunk_data.ITEMID.isin(itemids_for_value_NBP)].copy()\n",
    "value_chunk_data_O2 = chunk_data[chunk_data.ITEMID.isin(itemids_for_value_O2)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#union threshold and value\n",
    "threshold_and_value_HR = threshold_data_HR.append(value_chunk_data_HR)\n",
    "threshold_and_value_NBP = threshold_data_NBP.append(value_chunk_data_NBP)\n",
    "threshold_and_value_O2 = threshold_data_O2.append(value_chunk_data_O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort bei icusstay & charttime\n",
    "threshold_and_value_HR = threshold_and_value_HR.sort_values(by=['ICUSTAY_ID','CHARTTIME'])\n",
    "threshold_and_value_NBP = threshold_and_value_NBP.sort_values(by=['ICUSTAY_ID','CHARTTIME'])\n",
    "threshold_and_value_O2 = threshold_and_value_O2.sort_values(by=['ICUSTAY_ID','CHARTTIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can use ffill to fill the correspoding chunk IDs in\n",
    "# Problem: Cases where the threshold was first set before a value appeared - need to make sure to not write chunk id of different icustay in that\n",
    "#Solution: group by ICUSTAYID, first forward fill to fill threshold chunks where value was there before threshold, then backward fill to fill cases where threshold existed before value\n",
    "#For HR\n",
    "threshold_and_value_HR[\"CHUNK_ID_FILLED_TH\"] = threshold_and_value_HR.groupby('ICUSTAY_ID')['CHUNK_ID_FILLED'].transform(lambda x: x.ffill())\n",
    "threshold_and_value_HR[\"CHUNK_ID_FILLED_TH\"] = threshold_and_value_HR.groupby('ICUSTAY_ID')['CHUNK_ID_FILLED_TH'].transform(lambda x: x.bfill())\n",
    "\n",
    "#For NBP\n",
    "threshold_and_value_NBP[\"CHUNK_ID_FILLED_TH\"] = threshold_and_value_NBP.groupby('ICUSTAY_ID')['CHUNK_ID_FILLED'].transform(lambda x: x.ffill())\n",
    "threshold_and_value_NBP[\"CHUNK_ID_FILLED_TH\"] = threshold_and_value_NBP.groupby('ICUSTAY_ID')['CHUNK_ID_FILLED_TH'].transform(lambda x: x.bfill())\n",
    "\n",
    "#For O2\n",
    "threshold_and_value_O2[\"CHUNK_ID_FILLED_TH\"] = threshold_and_value_O2.groupby('ICUSTAY_ID')['CHUNK_ID_FILLED'].transform(lambda x: x.ffill())\n",
    "threshold_and_value_O2[\"CHUNK_ID_FILLED_TH\"] = threshold_and_value_O2.groupby('ICUSTAY_ID')['CHUNK_ID_FILLED_TH'].transform(lambda x: x.bfill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Validation\n",
    "threshold_and_value_HR.isna().sum()\n",
    "threshold_and_value_HR[threshold_and_value_HR['CHUNK_ID_FILLED_TH'].isna()]\n",
    "#we have some ICUSTAY_IDs where only thresholds are set but we have no values -> no chunkIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold_and_value_all = pd.concat([threshold_and_value_HR, threshold_and_value_NBP, threshold_and_value_O2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort the rows \n",
    "threshold_and_value_all = threshold_and_value_all.sort_values(by=['ICUSTAY_ID', 'CHARTTIME','ITEMID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(threshold_and_value_all).to_parquet('./data/chartevents_clean_values_and_thresholds_with_chunkid_' + str(chunking_dif) + '.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}